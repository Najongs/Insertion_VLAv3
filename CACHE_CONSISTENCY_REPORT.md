# VL Cache ì¼ê´€ì„± ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“‹ ë¶„ì„ ëŒ€ìƒ
- `TRAIN_FlowMatching.py`
- `TRAIN_Regression.py`
- `TOTAL_TRAIN.sh`
- `Make_VL_cache.py`
- `unified_dataset.py`

---

## âŒ ë¬¸ì œì  1: ìºì‹œ ê²½ë¡œ ë¶ˆì¼ì¹˜

### í˜„ì¬ ìƒíƒœ

| íŒŒì¼ | ê¸°ë³¸ ìºì‹œ ê²½ë¡œ |
|------|---------------|
| **TRAIN_FlowMatching.py** | `/home/najo/NAS/VLA/dataset/cache/qwen_vl_features` |
| **TRAIN_Regression.py** | `/home/najo/NAS/VLA/dataset/cache/qwen_vl_features` |
| **TOTAL_TRAIN.sh** | `CACHE_ROOT="/home/najo/NAS/VLA/dataset/cache"` |
| **unified_dataset.py** | `/home/najo/NAS/VLA/dataset/cache/qwen_vl_features` |

### ë¬¸ì œ
- `TOTAL_TRAIN.sh`ì˜ `CACHE_ROOT`ëŠ” `/home/najo/NAS/VLA/dataset/cache`ë¡œ ì„¤ì •
- ì‹¤ì œ ìºì‹œ ì €ì¥ ê²½ë¡œëŠ” `/home/najo/NAS/VLA/dataset/cache/qwen_vl_features`
- **`TOTAL_TRAIN.sh`ì— VL ìºì‹œ ìƒì„± ëª…ë ¹ì´ ì£¼ì„ ì²˜ë¦¬ë˜ì–´ ìˆìŒ!**

### ì˜í–¥
- ì£¼ì„ í•´ì œ ì‹œ ê²½ë¡œ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ìºì‹œë¥¼ ì°¾ì§€ ëª»í•  ìˆ˜ ìˆìŒ

---

## âŒ ë¬¸ì œì  2: TOTAL_TRAIN.shì˜ VL ìºì‹œ ìƒì„± ì„¹ì…˜ ì£¼ì„ ì²˜ë¦¬

### í˜„ì¬ ìƒíƒœ (Line 173-186)
```bash
# echo ""
# echo "=============== 0. VL CACHE BUILDING ==============="
# echo "Building VL feature cache for faster training..."
# torchrun --nproc_per_node=$NUM_GPUS TRAIN_FlowMatching.py \
#     --mode cache \
#     --dataset_paths "${DATASET_PATHS[@]}" \
#     --batch_size $MAIN_BATCH_SIZE \
#     --num_workers 8 \
#     --image_resize_height $IMG_HEIGHT \
#     --image_resize_width $IMG_WIDTH \
#     --cache_loader_only \
#     --cache_root $QWEN_CACHE_ROOT
```

### ë¬¸ì œ
1. **ë³€ìˆ˜ ë¯¸ì •ì˜**: `$MAIN_BATCH_SIZE`, `$IMG_HEIGHT`, `$IMG_WIDTH`, `$QWEN_CACHE_ROOT` ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•ŠìŒ
2. **ì‹¤í–‰ ë¶ˆê°€**: ì£¼ì„ í•´ì œí•´ë„ ë³€ìˆ˜ê°€ ì—†ì–´ ì‹¤í–‰ ì‹¤íŒ¨

---

## âŒ ë¬¸ì œì  3: FlowMatching vs Regressionì˜ ìºì‹œ ë¹Œë“œ ë°©ì‹ ì°¨ì´

### TRAIN_FlowMatching.py (Line 706-727)
```python
# QwenVLAUnified ëª¨ë¸ì„ ì§ì ‘ ìƒì„±
model = QwenVLAUnified(
    model_type='flow_matching',
    vl_model_name=vl_model_name,
    sensor_enabled=False,
    external_cache_root=args.cache_root,
)
model = model.to(device)
model.cache_dir = cache_dir  # ìˆ˜ë™ìœ¼ë¡œ cache_dir í• ë‹¹

build_vl_cache_distributed_optimized(
    model=model,
    dataset=train_loader.dataset,
    device=device,
    batch_size=args.batch_size,
    num_workers=args.num_workers,
)
```

### TRAIN_Regression.py (Line 928-997)
```python
# Processorì™€ VL ëª¨ë¸ì„ ìˆ˜ë™ìœ¼ë¡œ ë¡œë“œ
processor = AutoProcessor.from_pretrained(vl_model_name)
vl_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(...)

# DummyVLA ë˜í¼ ìƒì„±
class DummyVLA:
    def __init__(self, vl_model, processor, cache_dir: Path):
        self.vl_model = vl_model
        self.processor = processor
        self.cache_dir = cache_dir
        self._cache_path = QwenVLAUnified._cache_path.__get__(self)
        ...

dummy_model = DummyVLA(vl_model, processor, cache_dir)
build_vl_cache_distributed_optimized(
    dummy_model,
    full_dataset,
    device=device,
    batch_size=args.batch_size,
    num_workers=args.num_workers,
    prefetch_factor=4,
)
```

### ë¬¸ì œ
- **ë‘ ìŠ¤í¬ë¦½íŠ¸ê°€ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ìºì‹œ ë¹Œë“œ**
- FlowMatching: ì •ì‹ QwenVLAUnified ëª¨ë¸ ì‚¬ìš©
- Regression: DummyVLA ë˜í¼ ì‚¬ìš© (QwenVLAUnified ë©”ì„œë“œ ì°¸ì¡°)
- **ì¼ê´€ì„± ë¶€ì¡±**: DummyVLAê°€ QwenVLAUnifiedì˜ ë‚´ë¶€ ë©”ì„œë“œë¥¼ ì§ì ‘ ì°¸ì¡°í•˜ëŠ” ê²ƒì€ ì·¨ì•½í•¨

---

## âŒ ë¬¸ì œì  4: Make_VL_cache.pyì˜ ëª¨ë¸ ìš”êµ¬ì‚¬í•­

### Make_VL_cache.py (Line 45-47)
```python
model ìš”êµ¬ì‚¬í•­:
  - model.vl_model, model.processor í•„ìš”
  - (ì„ íƒ) model.cache_dir ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ cache_dir_fallback ì‚¬ìš©
```

### ì‹¤ì œ ì‚¬ìš©
- **FlowMatching**: âœ… model.vl_model, model.processor, model.cache_dir ëª¨ë‘ ì¡´ì¬
- **Regression**: âš ï¸ DummyVLAë¡œ ê°„ì ‘ ì œê³µ (model.vl_model, model.processor, model.cache_dir)

### ë¬¸ì œ
- DummyVLAëŠ” QwenVLAUnifiedì˜ private ë©”ì„œë“œë¥¼ ì§ì ‘ ì°¸ì¡°
- í–¥í›„ QwenVLAUnified ë¦¬íŒ©í† ë§ ì‹œ DummyVLAê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ

---

## âœ… ì˜¬ë°”ë¥¸ ì‘ë™ í™•ì¸

### Make_VL_cache.pyì˜ ìºì‹œ ì €ì¥ ë¡œì§ (Line 209-214)
```python
for j, item in enumerate(sub_items):
    pooled_single = pooled_batch[j:j+1]
    cache_mgr.save_cache(
        dataset_name=item["dataset_name"],
        vlm_idx=item["vlm_idx"],
        prompt_hash=item["prompt_hash"],
        vl_features=pooled_single
    )
```

### VLACacheManager.save_cache() (vla_cache_manager.py:90-106)
```python
def save_cache(self, dataset_name: str, vlm_idx: int, prompt_hash: str, vl_features: torch.Tensor):
    cache_path = self.get_cache_path(dataset_name, vlm_idx, prompt_hash)
    # cache_path = {cache_dir}/{prompt_hash}/{dataset_name}_vlm{vlm_idx}.pt
    self._atomic_save(vl_features.detach().to("cpu", dtype=torch.float16), cache_path)
    self._enforce_cache_limit()
```

### ê²½ë¡œ ìƒì„± ë¡œì§ í™•ì¸
- `cache_dir` â†’ `/home/najo/NAS/VLA/dataset/cache/qwen_vl_features`
- `prompt_hash` â†’ `"abc12345"` (ì˜ˆì‹œ)
- `dataset_name` â†’ `"data_collection_20251108_054442"`
- `vlm_idx` â†’ `0`

**ìµœì¢… ìºì‹œ íŒŒì¼ ê²½ë¡œ**:
```
/home/najo/NAS/VLA/dataset/cache/qwen_vl_features/abc12345/data_collection_20251108_054442_vlm0.pt
```

âœ… **ê²½ë¡œ ìƒì„± ë¡œì§ì€ ì˜¬ë°”ë¦„**

---

## ğŸ“Š ê¶Œì¥ ì‚¬í•­

### 1. TOTAL_TRAIN.sh ìˆ˜ì • í•„ìš”

#### ë³€ìˆ˜ ì •ì˜ ì¶”ê°€
```bash
# VL Cache ìƒì„±ì„ ìœ„í•œ ë³€ìˆ˜ ì •ì˜
MAIN_BATCH_SIZE=8
IMG_HEIGHT=360
IMG_WIDTH=640
QWEN_CACHE_ROOT="/home/najo/NAS/VLA/dataset/cache/qwen_vl_features"
```

#### VL ìºì‹œ ìƒì„± ì„¹ì…˜ í™œì„±í™” (ì£¼ì„ ì œê±°)
```bash
echo ""
echo "=============== 0. VL CACHE BUILDING ==============="
echo "Building VL feature cache for faster training..."
torchrun --nproc_per_node=$NUM_GPUS TRAIN_FlowMatching.py \
    --mode cache \
    --dataset_paths "${DATASET_PATHS[@]}" \
    --batch_size $MAIN_BATCH_SIZE \
    --num_workers 8 \
    --image_resize_height $IMG_HEIGHT \
    --image_resize_width $IMG_WIDTH \
    --cache_root $QWEN_CACHE_ROOT
echo "=============== VL CACHE BUILDING COMPLETE ==============="
echo ""
```

**ì£¼ì˜**: `--cache_loader_only` í”Œë˜ê·¸ ì œê±° (TRAIN_FlowMatching.pyê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬)

---

### 2. TRAIN_Regression.pyì˜ DummyVLA ì œê±° (ì„ íƒ ì‚¬í•­)

FlowMatchingê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í†µì¼:

```python
if args.mode == 'cache':
    # FlowMatchingê³¼ ë™ì¼í•œ ë°©ì‹
    train_loader, _ = build_dataloaders(
        args, rank, world_size,
        use_cache=True,
        cache_build_only=True,
    )

    model = QwenVLAUnified(
        model_type='regression',
        vl_model_name=vl_model_name,
        sensor_enabled=False,
        external_cache_root=args.cache_root,
        ...
    )
    model = model.to(device)
    model.cache_dir = Path(args.cache_root)

    build_vl_cache_distributed_optimized(
        model=model,
        dataset=train_loader.dataset,
        device=device,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
    )
```

---

### 3. ìºì‹œ ê²½ë¡œ ì¼ê´€ì„± í™•ì¸

ëª¨ë“  íŒŒì¼ì—ì„œ ë™ì¼í•œ ê²½ë¡œ ì‚¬ìš©:
```
/home/najo/NAS/VLA/dataset/cache/qwen_vl_features
```

---

## ğŸ¯ ìš”ì•½

| í•­ëª© | ìƒíƒœ | ë¹„ê³  |
|------|------|------|
| ìºì‹œ ì €ì¥ ë¡œì§ | âœ… ì •ìƒ | VLACacheManagerê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™ |
| FlowMatching ìºì‹œ ë¹Œë“œ | âœ… ì •ìƒ | QwenVLAUnified ì‚¬ìš© |
| Regression ìºì‹œ ë¹Œë“œ | âš ï¸ ì‘ë™í•˜ë‚˜ ë¹„ê¶Œì¥ | DummyVLA ë˜í¼ ì‚¬ìš© (ì·¨ì•½) |
| TOTAL_TRAIN.sh VL ìºì‹œ ì„¹ì…˜ | âŒ ì‹¤í–‰ ë¶ˆê°€ | ë³€ìˆ˜ ë¯¸ì •ì˜ + ì£¼ì„ ì²˜ë¦¬ |
| ê²½ë¡œ ì¼ê´€ì„± | âš ï¸ ì£¼ì˜ í•„ìš” | CACHE_ROOT vs QWEN_CACHE_ROOT |

---

## ğŸš€ ì¦‰ì‹œ ì¡°ì¹˜ ì‚¬í•­

1. **TOTAL_TRAIN.sh ìˆ˜ì •**: ë³€ìˆ˜ ì •ì˜ ë° VL ìºì‹œ ì„¹ì…˜ í™œì„±í™”
2. **ê²½ë¡œ í†µì¼**: `QWEN_CACHE_ROOT="/home/najo/NAS/VLA/dataset/cache/qwen_vl_features"` ëª…ì‹œì  ì •ì˜
3. **í…ŒìŠ¤íŠ¸**: FlowMatching ìºì‹œ ë¹Œë“œ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë™ì‘ í™•ì¸

---

ìƒì„±ì¼: 2025-01-11
