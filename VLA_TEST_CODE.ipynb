{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a5fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibom-a6000/miniconda3/envs/qwen_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.action_decoder import FlowMatchingActionExpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4e418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FlowMatchingActionExpert V2 (Cross-Attention + ModulatedDecoder) 초기화 완료\n",
      "   4개의 ModulatedDecoderLayer 사용\n",
      "=== Dummy Input Shapes ===\n",
      "actions          : (2, 8, 7)   (B,H,A)\n",
      "context_features : (2, 16, 2048)   (B,Sv,D_img)\n",
      "guidance_vector  : (2, 2048)   (B,D_text)\n",
      "sensor_features  : (2, 2048)   (B,D_sensor)\n",
      "\n",
      "actions_n (normalized) : (2, 8, 7)\n",
      "x_t (bridge state)     : (2, 8, 7)    (B,H,A)\n",
      "u_t (target velocity)  : (2, 8, 7)    (B,H,A)\n",
      "t_scalar               : (2,)    (B,) scalar in [0,1]\n",
      "\n",
      "======== [DEBUG FLOW FORWARD] ========\n",
      "x_t               : (2, 8, 7)   (B,H,A)\n",
      "t                 : (2,)     (B,)  scalar in [0,1]\n",
      "context_features  : (2, 16, 2048)   (B,Sv,D_img)\n",
      "guidance_vector   : (2, 2048)    (B,D_text)\n",
      "sensor_features   : (2, 2048)    (B,D_sensor)\n",
      "--------------------------------------\n",
      "time_raw (sinusoidal) : (2, 256)   (B,time_embed_dim=256)\n",
      "t_embed (after MLP+LN): (2, 1024)   (B,hidden_dim=1024)\n",
      "guidance_embed        : (2, 1024)   (B,hidden_dim)\n",
      "cond_embed (guide+time): (2, 1024)   (B,hidden_dim)\n",
      "--------------------------------------\n",
      "tgt (after action_embed): (2, 8, 1024)   (B,H,hidden_dim)\n",
      "tgt (after tgt_pos)      : (2, 8, 1024)   (B,H,hidden_dim)\n",
      "vision_mem (proj)       : (2, 16, 1024)   (B,Sv,hidden_dim)\n",
      "vision_mem (+type 0)    : (2, 16, 1024)\n",
      "sensor_tok (after proj): (2, 1, 1024)\n",
      "sensor_tok (+type 1)   : (2, 1, 1024)\n",
      "vision_mem (+mem_pos)  : (2, 16, 1024)\n",
      "sensor_tok (+mem_pos)  : (2, 1, 1024)\n",
      "memory (vision+sensor) : (2, 17, 1024)   (B,S,hidden_dim)\n",
      "--------------------------------------\n",
      "conditioned_tgt        : (2, 8, 1024)\n",
      "tgt_mask              : (8, 8)   (H,H) causal\n",
      "\n",
      "--- ModulatedDecoderLayer 0 ---\n",
      " input x     : (2, 8, 1024)\n",
      " output x    : (2, 8, 1024)\n",
      "\n",
      "--- ModulatedDecoderLayer 1 ---\n",
      " input x     : (2, 8, 1024)\n",
      " output x    : (2, 8, 1024)\n",
      "\n",
      "--- ModulatedDecoderLayer 2 ---\n",
      " input x     : (2, 8, 1024)\n",
      " output x    : (2, 8, 1024)\n",
      "\n",
      "--- ModulatedDecoderLayer 3 ---\n",
      " input x     : (2, 8, 1024)\n",
      " output x    : (2, 8, 1024)\n",
      "\n",
      "Decoder output         : (2, 8, 1024)\n",
      "velocity (final output): (2, 8, 7)   (B,H,A)\n",
      "=========================================\n",
      "\n",
      "v_pred shape           : (2, 8, 7)\n",
      "Smooth L1 loss (scalar): 0.4724693298339844\n",
      "\n",
      "=== Sampling ===\n",
      "sampled_actions : (2, 8, 7)    (B,H,A)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# 0. 설정: 더미 차원 / 배치 크기 (QwenVLAUnified V2 기준)\n",
    "# ============================================================\n",
    "BATCH_SIZE = 2\n",
    "HORIZON    = 8       # action horizon (QwenVLAUnified.horizon)\n",
    "ACTION_DIM = 7       # (dx,dy,dz,dRx,dRy,dRz,gripper)\n",
    "\n",
    "SV = 16              # vision token length (더미, 실제 VLM 토큰 수와 무관)\n",
    "# SS는 실제 파이프라인에서는 쓰지 않고, 센서+로봇 상태는 한 벡터로 합쳐서 들어감\n",
    "\n",
    "# Qwen2.5-VL-3B hidden size ~ 2048 가정 (코드 기본값과 맞춤)\n",
    "IMAGE_FEAT_DIM   = 2048   # vl_hidden_size → image_feature_dim\n",
    "TEXT_GUIDE_DIM   = 2048   # vl_hidden_size → text_guidance_dim\n",
    "\n",
    "# 센서 + 로봇 상태 통합 차원 (sensor_output_dim(1024) + robot_state_output_dim(1024))\n",
    "SENSOR_FEAT_DIM  = 2048   # combined_sensor_dim\n",
    "\n",
    "HIDDEN_DIM       = 1024   # QwenVLAUnified(hidden_dim)\n",
    "\n",
    "device = \"cpu\"  # 필요하면 \"cuda\"로 변경\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 모델 인스턴스 생성 (FlowMatchingActionExpert V2)\n",
    "#    └ QwenVLAUnified에서 사용하는 설정과 일치시킴\n",
    "# ============================================================\n",
    "flow_model = FlowMatchingActionExpert(\n",
    "    image_feature_dim = IMAGE_FEAT_DIM,\n",
    "    text_guidance_dim = TEXT_GUIDE_DIM,\n",
    "    sensor_dim        = SENSOR_FEAT_DIM,\n",
    "    action_dim        = ACTION_DIM,\n",
    "    horizon           = HORIZON,\n",
    "    hidden_dim        = HIDDEN_DIM,\n",
    "    nhead             = 8,\n",
    "    num_decoder_layers= 4,    # Unified에서 기본 4 layer\n",
    "    time_embed_dim    = 256,\n",
    "    dropout           = 0.1,\n",
    "    sigma_min         = 1e-4,\n",
    ").to(device)\n",
    "\n",
    "# 센서 토큰을 메모리에 포함 (Unified V2 기본값 True)\n",
    "flow_model.use_sensor_tokens = True\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. 더미 입력 생성\n",
    "#    - actions: GT action 시퀀스 (Flow loss용) → (B,H,7)\n",
    "#    - context_features: VLM image tokens → (B,Sv,2048)\n",
    "#    - guidance_vector: text guidance → (B,2048)\n",
    "#    - sensor_features: sensor+robot 통합 벡터 → (B,2048)\n",
    "# ============================================================\n",
    "torch.manual_seed(0)\n",
    "\n",
    "actions = torch.randn(BATCH_SIZE, HORIZON, ACTION_DIM, device=device)\n",
    "context_features = torch.randn(BATCH_SIZE, SV, IMAGE_FEAT_DIM, device=device)\n",
    "guidance_vector  = torch.randn(BATCH_SIZE, TEXT_GUIDE_DIM, device=device)\n",
    "\n",
    "# 실제 Unified 파이프라인과 동일하게 2D 벡터 (B, SENSOR_FEAT_DIM)\n",
    "sensor_features  = torch.randn(BATCH_SIZE, SENSOR_FEAT_DIM, device=device)\n",
    "\n",
    "print(\"=== Dummy Input Shapes ===\")\n",
    "print(f\"actions          : {tuple(actions.shape)}   (B,H,A)\")\n",
    "print(f\"context_features : {tuple(context_features.shape)}   (B,Sv,D_img)\")\n",
    "print(f\"guidance_vector  : {tuple(guidance_vector.shape)}   (B,D_text)\")\n",
    "print(f\"sensor_features  : {tuple(sensor_features.shape)}   (B,D_sensor)\")\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. FlowMatchingActionExpert 내부 shape 디버깅용 forward\n",
    "# ============================================================\n",
    "def debug_flow_forward(\n",
    "    model: FlowMatchingActionExpert,\n",
    "    x_t: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    "    context_features: torch.Tensor,\n",
    "    guidance_vector: torch.Tensor,\n",
    "    sensor_features: torch.Tensor,\n",
    "):\n",
    "    \"\"\"\n",
    "    FlowMatchingActionExpert.forward와 거의 동일한 로직을 따르되\n",
    "    중간 shape들을 전부 출력하는 디버그용 함수.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"======== [DEBUG FLOW FORWARD] ========\")\n",
    "    print(f\"x_t               : {tuple(x_t.shape)}   (B,H,A)\")\n",
    "    print(f\"t                 : {tuple(t.shape)}     (B,)  scalar in [0,1]\")\n",
    "    print(f\"context_features  : {tuple(context_features.shape)}   (B,Sv,D_img)\")\n",
    "    print(f\"guidance_vector   : {tuple(guidance_vector.shape)}    (B,D_text)\")\n",
    "    print(f\"sensor_features   : {tuple(sensor_features.shape)}    (B,D_sensor)\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    B, H, _ = x_t.shape\n",
    "\n",
    "    # 1) 시간 + 텍스트 가이던스 임베딩\n",
    "    time_raw = model.sinusoidal_time_embedding(t)        # (B, time_embed_dim)\n",
    "    print(f\"time_raw (sinusoidal) : {tuple(time_raw.shape)}   (B,time_embed_dim={model.time_embed_dim})\")\n",
    "\n",
    "    t_embed = model.time_mlp(time_raw)                  # (B, hidden_dim)\n",
    "    t_embed = model.time_norm(t_embed)\n",
    "    print(f\"t_embed (after MLP+LN): {tuple(t_embed.shape)}   (B,hidden_dim={model.hidden_dim})\")\n",
    "\n",
    "    guidance_embed = model.text_guidance_proj(guidance_vector)  # (B,hidden_dim)\n",
    "    guidance_embed = model.guidance_norm(guidance_embed)\n",
    "    print(f\"guidance_embed        : {tuple(guidance_embed.shape)}   (B,hidden_dim)\")\n",
    "\n",
    "    cond_embed = guidance_embed + t_embed               # (B,hidden_dim)\n",
    "    print(f\"cond_embed (guide+time): {tuple(cond_embed.shape)}   (B,hidden_dim)\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    # 2) 디코더 입력 토큰(tgt) + 포지셔널\n",
    "    tgt = model.action_embed(x_t)                       # (B,H,hidden_dim)\n",
    "    print(f\"tgt (after action_embed): {tuple(tgt.shape)}   (B,H,hidden_dim)\")\n",
    "\n",
    "    tgt = tgt + model.tgt_pos[:, :H]                    # (1,H,D) broadcast\n",
    "    print(f\"tgt (after tgt_pos)      : {tuple(tgt.shape)}   (B,H,hidden_dim)\")\n",
    "\n",
    "    # 3) 메모리(비전 + 센서) 구성\n",
    "    assert context_features.dim() == 3, \"context_features는 (B,Sv,D_img) 형태여야 함\"\n",
    "    vision_mem = model.context_proj(context_features)   # (B,Sv,hidden_dim)\n",
    "    print(f\"vision_mem (proj)       : {tuple(vision_mem.shape)}   (B,Sv,hidden_dim)\")\n",
    "\n",
    "    # 타입 임베딩: 0 = vision\n",
    "    vision_mem = vision_mem + model.token_type_embed.weight[0].view(1,1,-1)\n",
    "    print(f\"vision_mem (+type 0)    : {tuple(vision_mem.shape)}\")\n",
    "\n",
    "    if model.use_sensor_tokens:\n",
    "        assert sensor_features is not None, \"use_sensor_tokens=True이면 sensor_features 필요\"\n",
    "\n",
    "        # Unified 파이프라인처럼 2D 벡터 (B,D) → (B,1,D) 토큰으로 확장\n",
    "        if sensor_features.dim() == 2:\n",
    "            sensor_tok = model.sensor_proj(sensor_features).unsqueeze(1)  # (B,1,D)\n",
    "        else:\n",
    "            sensor_tok = model.sensor_proj(sensor_features)               # (B,Ss,D)\n",
    "        print(f\"sensor_tok (after proj): {tuple(sensor_tok.shape)}\")\n",
    "\n",
    "        # 타입 임베딩: 1 = sensor\n",
    "        sensor_tok = sensor_tok + model.token_type_embed.weight[1].view(1,1,-1)\n",
    "        print(f\"sensor_tok (+type 1)   : {tuple(sensor_tok.shape)}\")\n",
    "\n",
    "        Sv = vision_mem.size(1)\n",
    "        Ss = sensor_tok.size(1)\n",
    "        # 포지셔널 임베딩\n",
    "        vision_mem = vision_mem + model.mem_pos[:, :Sv]\n",
    "        if Ss <= model.mem_pos.size(1):\n",
    "            sensor_tok = sensor_tok + model.mem_pos[:, :Ss]\n",
    "        print(f\"vision_mem (+mem_pos)  : {tuple(vision_mem.shape)}\")\n",
    "        print(f\"sensor_tok (+mem_pos)  : {tuple(sensor_tok.shape)}\")\n",
    "\n",
    "        memory = torch.cat([vision_mem, sensor_tok], dim=1)  # (B,Sv+Ss,D)\n",
    "    else:\n",
    "        Sv = vision_mem.size(1)\n",
    "        vision_mem = vision_mem + model.mem_pos[:, :Sv]\n",
    "        memory = vision_mem\n",
    "\n",
    "    print(f\"memory (vision+sensor) : {tuple(memory.shape)}   (B,S,hidden_dim)\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    # 4) cond를 tgt에 주입\n",
    "    conditioned_tgt = tgt + cond_embed.unsqueeze(1)     # (B,H,D)\n",
    "    print(f\"conditioned_tgt        : {tuple(conditioned_tgt.shape)}\")\n",
    "\n",
    "    # 5) causal mask 생성\n",
    "    tgt_mask = None\n",
    "    if model.causal_self_attn:\n",
    "        H_ = conditioned_tgt.size(1)\n",
    "        tgt_mask = torch.triu(torch.ones(H_, H_, device=conditioned_tgt.device), diagonal=1).bool()\n",
    "        print(f\"tgt_mask              : {tuple(tgt_mask.shape)}   (H,H) causal\")\n",
    "\n",
    "    # 6) ModulatedDecoderLayer 스택 적용\n",
    "    x = conditioned_tgt\n",
    "    for layer_idx, layer in enumerate(model.mod_layers):\n",
    "        print(f\"\\n--- ModulatedDecoderLayer {layer_idx} ---\")\n",
    "        print(f\" input x     : {tuple(x.shape)}\")\n",
    "        x = layer(x, memory, cond_embed, tgt_mask=tgt_mask, memory_key_padding_mask=None)\n",
    "        print(f\" output x    : {tuple(x.shape)}\")\n",
    "\n",
    "    decoder_output = x\n",
    "    print(\"\\nDecoder output         :\", tuple(decoder_output.shape))\n",
    "\n",
    "    # 7) 출력 헤드 (velocity)\n",
    "    velocity = model.output_head(decoder_output)\n",
    "    print(\"velocity (final output):\", tuple(velocity.shape), \"  (B,H,A)\")\n",
    "    print(\"=========================================\\n\")\n",
    "\n",
    "    return velocity\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. OT-CFM까지 포함한 전체 forward + loss 디버깅\n",
    "# ============================================================\n",
    "with torch.no_grad():\n",
    "    # (1) 스케일 정규화 → flow 브리지 생성\n",
    "    actions_n = actions / flow_model.action_scale  # (B,H,A)\n",
    "    print(\"actions_n (normalized) :\", tuple(actions_n.shape))\n",
    "\n",
    "    x_t, u_t, t_scalar = flow_model.flow.compute_flow_and_target(actions_n)\n",
    "    print(\"x_t (bridge state)     :\", tuple(x_t.shape), \"   (B,H,A)\")\n",
    "    print(\"u_t (target velocity)  :\", tuple(u_t.shape), \"   (B,H,A)\")\n",
    "    print(\"t_scalar               :\", tuple(t_scalar.shape), \"   (B,) scalar in [0,1]\")\n",
    "    print()\n",
    "\n",
    "    # (2) 디버그 forward (내부 shape 전부 출력)\n",
    "    v_pred = debug_flow_forward(\n",
    "        flow_model,\n",
    "        x_t,\n",
    "        t_scalar,\n",
    "        context_features,\n",
    "        guidance_vector,\n",
    "        sensor_features,\n",
    "    )\n",
    "\n",
    "    # (3) 실제 loss 계산과 동일한 형태로 loss 확인\n",
    "    lam = (1.0 - t_scalar).clamp_(min=0.0)\n",
    "    lam = lam.view(lam.size(0), -1).mean(dim=1)      # (B,)\n",
    "    lam = lam.view(-1, 1, 1).clamp(min=flow_model.min_lambda)\n",
    "\n",
    "    loss_per_elem = torch.nn.functional.smooth_l1_loss(v_pred, u_t, reduction='none')\n",
    "    weighted = lam * loss_per_elem\n",
    "    loss = weighted.mean(dim=(1,2)).mean()\n",
    "\n",
    "    print(\"v_pred shape           :\", tuple(v_pred.shape))\n",
    "    print(\"Smooth L1 loss (scalar):\", float(loss))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. 샘플링 경로(dim 확인)\n",
    "# ============================================================\n",
    "with torch.no_grad():\n",
    "    sampled_actions = flow_model.sample(\n",
    "        context_features=context_features,\n",
    "        guidance_vector=guidance_vector,\n",
    "        sensor_features=sensor_features,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_steps=6,\n",
    "        method='rk4',\n",
    "    )\n",
    "    print(\"\\n=== Sampling ===\")\n",
    "    print(\"sampled_actions :\", tuple(sampled_actions.shape), \"   (B,H,A)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffe36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
