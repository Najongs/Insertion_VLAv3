"""
Cache VLM Features for CLIP Pre-training - DDP Version

This script pre-computes and caches features from the VLM for the samples
that will be used in `TRAIN_SensorImage_CLIP.py`.

This version uses torch.distributed (DDP) to parallelize the workload across
multiple GPUs, loading one VLM per GPU to avoid memory issues.

Usage:
    torchrun --nproc_per_node=4 cache_clip_vlm_features.py \
        --new_dataset_paths "/path/to/dataset1" "/path/to/dataset2" \
        --cache_root "/path/to/cache" \
        --vlm_model "Qwen/Qwen2.5-VL-7B-Instruct"
"""

# =============================================================================
# âš ï¸ CRITICAL: CLIP VLM ìºì‹œ êµ¬ì¡° ë° prompt_hash ë§¤ì¹­ (Updated 2025-01-13)
# =============================================================================
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” CLIP í•™ìŠµìš© VLM ìºì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
# ìºì‹œ êµ¬ì¡°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì´í•´í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.
#
# 1. CLIP VLM ìºì‹œ ê²½ë¡œ êµ¬ì¡° (2025-01-13 ì—…ë°ì´íŠ¸):
#    {cache_root}/clip_vlm_features/{prompt_hash}/{episode_name}_vlm{idx}.pt
#
#    - prompt_hash: CLIP_PROMPT_TEXTë¥¼ task_nameìœ¼ë¡œ í¬ë§·í•œ í›„ MD5 í•´ì‹œí™”í•œ ê°’ (ì²« 8ì)
#    - CLIP_PROMPT_TEXTëŠ” í…œí”Œë¦¿ ë¬¸ìì—´ì´ë©° {task_name} í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ í¬í•¨
#    - ê° íƒœìŠ¤í¬(Red_point, White_point, etc.)ê°€ ê³ ìœ í•œ prompt_hashë¥¼ ê°€ì§
#    - ê° íƒœìŠ¤í¬ì˜ ìºì‹œê°€ ë³„ë„ì˜ prompt_hash ë””ë ‰í† ë¦¬ì— ì €ì¥ë¨
#
# 2. 2025-01-13 ì£¼ìš” ë³€ê²½ì‚¬í•­:
#    âœ¨ CLIPë„ ì´ì œ task-specific prompts ì‚¬ìš©:
#       - CLIP_PROMPT_TEXT í…œí”Œë¦¿: "...target point is {task_name}..."
#       - Red_point  â†’ "...target point is Red_point..."  â†’ hash_red
#       - White_point â†’ "...target point is White_point..." â†’ hash_white
#       - ìºì‹œ êµ¬ì¡°: /cache/clip_vlm_features/hash_red/, /cache/clip_vlm_features/hash_white/
#
#    ğŸ–¼ï¸ Single-view support:
#       - VLMì— View5 ì¹´ë©”ë¼ ë·°ì˜ ì´ë¯¸ì§€ë§Œ ì „ë‹¬
#       - Content structure: [{"type": "image", "image": img}, {"type": "text", "text": prompt}]
#
# 3. Flow Matching VL ìºì‹œì™€ì˜ ì°¨ì´ì :
#
#    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#    â”‚                 â”‚  CLIP VLM ìºì‹œ           â”‚  Flow Matching VL ìºì‹œ      â”‚
#    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#    â”‚ Prompt ì†ŒìŠ¤     â”‚ CLIP_PROMPT_TEXT templateâ”‚ íƒœìŠ¤í¬ë³„ instruction        â”‚
#    â”‚ í•´ì‹œ ì•Œê³ ë¦¬ì¦˜   â”‚ MD5                      â”‚ SHA256                      â”‚
#    â”‚ task_name í¬í•¨  â”‚ O (íƒœìŠ¤í¬ë³„ë¡œ ë‹¤ë¦„)      â”‚ O (íƒœìŠ¤í¬ë³„ë¡œ ë‹¤ë¦„)         â”‚
#    â”‚ ìºì‹œ ë””ë ‰í† ë¦¬   â”‚ clip_vlm_features/       â”‚ qwen_vl_features/           â”‚
#    â”‚ prompt_hash ìˆ˜  â”‚ Nê°œ (íƒœìŠ¤í¬ë‹¹ 1ê°œ)       â”‚ Nê°œ (íƒœìŠ¤í¬ë‹¹ 1ê°œ)          â”‚
#    â”‚ ì´ë¯¸ì§€ ì…ë ¥     â”‚ Single view (View5)      â”‚ Single view                 â”‚
#    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# 4. âš ï¸ ìºì‹œ ë¬´íš¨í™” ì£¼ì˜ì‚¬í•­:
#    - CLIP_PROMPT_TEXT í…œí”Œë¦¿ì„ ë³€ê²½í•˜ë©´ ëª¨ë“  prompt_hashê°€ ë°”ë€Œì–´ ê¸°ì¡´ ìºì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
#    - CLIP_PROMPT_TEXT ë³€ê²½ ì‹œ ë°˜ë“œì‹œ ëª¨ë“  íƒœìŠ¤í¬ì˜ ìºì‹œë¥¼ ì¬ìƒì„±í•´ì•¼ í•¨
#    - í•™ìŠµ ì‹œ TRAIN_SensorImage_CLIP.pyì˜ CLIP_PROMPT_TEXTì™€ ì™„ì „íˆ ì¼ì¹˜í•´ì•¼ í•¨
#
# 5. ìºì‹œ ìƒì„±ê³¼ í•™ìŠµ ê°„ ì¼ê´€ì„±:
#    âœ… ë°˜ë“œì‹œ í™•ì¸í•´ì•¼ í•  ì‚¬í•­:
#       - ë™ì¼í•œ CLIP_PROMPT_TEXT í…œí”Œë¦¿ ì‚¬ìš© (TRAIN_SensorImage_CLIP.pyì—ì„œ import)
#       - ë™ì¼í•œ cache_root ê²½ë¡œ
#       - ë™ì¼í•œ VLM ëª¨ë¸ (Qwen2.5-VL-3B-Instruct ë“±)
#       - ë™ì¼í•œ hand-eye view ì„¤ì • (View5 only)
# =============================================================================

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import argparse
import json
from pathlib import Path
import torch
import torch.distributed as dist
from torch.utils.data import Dataset, DataLoader, Subset, DistributedSampler
from tqdm import tqdm
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from PIL import Image
import time

# Add project root to import custom modules
import sys
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from vla_datasets.unified_dataset import create_unified_dataloader
from TRAIN_SensorImage_CLIP import (
    SensorImageCLIPDataset,
    CLIP_PROMPT_TEXT,
    get_clip_prompt_hash,
    get_formatted_clip_prompt,
    extract_task_name_from_episode_path,
)
from qwen_vl_utils import process_vision_info
from vla_cache_manager import VLACacheManager


def disable_generation_temperature(vlm_model):
    """
    Keep the temperature attribute but neutralize it so the model
    does not try to use it (while avoiding HF warnings).
    """
    gen_cfg = getattr(vlm_model, "generation_config", None)
    if gen_cfg is None:
        return

    extra_params = getattr(gen_cfg, "_extra_generation_params", None)
    if isinstance(extra_params, dict):
        extra_params.pop("temperature", None)

    try:
        setattr(gen_cfg, "temperature", None)
    except AttributeError:
        gen_cfg.__dict__["temperature"] = None


def setup_distributed():
    """Initialize distributed training environment."""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        print('Not using distributed mode')
        return 0, 1, 0, False

    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend='nccl')

    return rank, world_size, local_rank, True


def cleanup_distributed():
    """Cleanup distributed training environment."""
    if dist.is_initialized():
        dist.destroy_process_group()


def generate_text_response(vlm_model, vlm_processor, messages, max_new_tokens):
    # messages: [{"role":"user","content":[{"type":"image","image": raw_image}, {"type":"text","text": prompt}]}]
    text_input = vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)

    model_inputs = vlm_processor(
        text=[text_input],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    ).to(device=vlm_model.device, dtype=vlm_model.dtype)

    input_lens = [len(ids) for ids in model_inputs.input_ids]
    with torch.no_grad():
        gen_ids = vlm_model.generate(
            **model_inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            num_beams=1,
        )

    trimmed = [ids[il:] for ids, il in zip(gen_ids, input_lens)]
    resp = vlm_processor.batch_decode(trimmed, skip_special_tokens=True)[0]
    # Qwen í…œí”Œë¦¿ êµ¬ê°„ ì œê±°
    if "<|im_start|>assistant" in resp:
        resp = resp.split("<|im_start|>assistant", 1)[-1]
    if "<|im_end|>" in resp:
        resp = resp.split("<|im_end|>", 1)[0]
    return resp.strip()



def cache_worker(rank, world_size, local_rank, args, clip_dataset):
    """
    The worker function for each DDP process.
    Loads a model onto its assigned GPU and processes a subset of the data.
    """
    device = torch.device(f"cuda:{local_rank}")
    is_main_process = rank == 0

    # Each worker gets its own cache manager instance
    cache_manager = VLACacheManager(cache_dir=str(Path(args.cache_root) / "clip_vlm_features"))

    # Build episode_id -> task_name mapping from unified_dataset
    episode_to_task = {}
    for sub_dataset in clip_dataset.unified_dataset.datasets:
        episode_id = sub_dataset.data_dir.name
        task_name = extract_task_name_from_episode_path(sub_dataset.data_dir)
        episode_to_task[episode_id] = task_name

    if is_main_process:
        print(f"[Rank {rank}] Built task name mapping for {len(episode_to_task)} episodes")
        unique_tasks = set(episode_to_task.values())
        print(f"[Rank {rank}] Found {len(unique_tasks)} unique tasks: {sorted(unique_tasks)}")

    # 1. Load VLM (one per process)
    if is_main_process:
        print(f"[Rank {rank}] Loading VLM on GPU {local_rank}...")

    vlm_processor = AutoProcessor.from_pretrained(
        args.vlm_model,
        trust_remote_code=True,
        local_files_only=True  # Use local cache only, avoid network verification
    )
    vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        args.vlm_model,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map={"": device},  # Load entire model on this specific GPU
        attn_implementation="flash_attention_2",
        local_files_only=True  # Use local cache only, avoid network verification
    )
    vlm_model.eval()
    disable_generation_temperature(vlm_model)

    if is_main_process:
        print(f"[Rank {rank}] VLM loaded on {device}.")

    # 2. Create DistributedSampler for this worker
    sampler = DistributedSampler(
        clip_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=False,
        drop_last=False
    )

    def collate_fn_cache(batch):
        """Collate function that keeps samples as list for batch processing"""
        return batch

    dataloader = DataLoader(
        clip_dataset,
        batch_size=args.batch_size,  # Process in batches
        sampler=sampler,
        num_workers=args.num_workers,
        collate_fn=collate_fn_cache
    )

    # 3. Iterate and cache features
    if is_main_process:
        pbar = tqdm(total=len(dataloader), desc=f"Rank {rank} (GPU {local_rank})")

    for batch in dataloader:
        # Process each sample in the batch
        for sample in batch:
            images = sample["hand_eye_image"]  # Now a list of images (View5, View4, etc.)
            episode_id = sample["episode_id"]
            vlm_idx = sample["vlm_idx"]

            if vlm_idx is None:
                continue

            # Get task-specific prompt and hash
            task_name = episode_to_task.get(episode_id, "Unknown")
            formatted_prompt = get_formatted_clip_prompt(task_name)
            task_prompt_hash = get_clip_prompt_hash(task_name)

            if cache_manager.cache_exists(dataset_name=episode_id, vlm_idx=vlm_idx, prompt_hash=task_prompt_hash):
                continue

            # Generate text, image embeds, and text embeds
            try:
                # Use only the first image (View5)
                image = images[0] if isinstance(images, list) and len(images) > 0 else images
                messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": formatted_prompt}]}]
                text_response = generate_text_response(
                    vlm_model, vlm_processor, messages, args.max_new_tokens
                )

                with torch.no_grad():
                    # 1. ì´ë¯¸ì§€ ì „ìš© ì¶”ë¡  (ìˆœìˆ˜ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ - ëª¨ë“  ì´ë¯¸ì§€ í† í°)
                    image_only_messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": ""}]}]
                    image_text_with_placeholders = vlm_processor.apply_chat_template(
                        image_only_messages, tokenize=False, add_generation_prompt=False
                    )
                    image_only_vision_input, _ = process_vision_info(image_only_messages)
                    image_inputs = vlm_processor(
                        text=[image_text_with_placeholders],
                        images=image_only_vision_input, padding=True, return_tensors="pt"
                    ).to(device=vlm_model.device, dtype=vlm_model.dtype)

                    image_outputs = vlm_model(**image_inputs, output_hidden_states=True, return_dict=True)
                    image_hidden_state = image_outputs.hidden_states[-1]

                    # ì´ë¯¸ì§€ í† í°ë§Œ ì¶”ì¶œ (í† í° ID 151655 for Qwen2.5-VL image_pad)
                    image_token_mask = (image_inputs['input_ids'] == 151655)
                    image_indices = torch.where(image_token_mask.squeeze(0))[0]
                    image_features = image_hidden_state[:, image_indices, :]

                    # 2. í…ìŠ¤íŠ¸ ì „ìš© ì¶”ë¡  (ê°€ì´ë˜ìŠ¤ ë²¡í„° ì¶”ì¶œ - í‰ê·  í’€ë§)
                    text_inputs = vlm_processor(
                        text=[text_response], images=None, padding=True, return_tensors="pt"
                    ).to(device=vlm_model.device, dtype=vlm_model.dtype)

                    text_outputs = vlm_model(**text_inputs, output_hidden_states=True, return_dict=True)
                    text_hidden_state = text_outputs.hidden_states[-1]
                    guidance_vector = text_hidden_state.mean(dim=1)

                # 3. ìºì‹œ ì €ì¥ (íŠœí”Œ í˜•ì‹ìœ¼ë¡œ, task-specific prompt hash ì‚¬ìš©)
                features_to_cache = (
                    image_features.detach().to("cpu", dtype=torch.float16),
                    guidance_vector.detach().to("cpu", dtype=torch.float16)
                )

                cache_manager.save_cache_tuple(
                    dataset_name=episode_id, vlm_idx=vlm_idx, prompt_hash=task_prompt_hash, features_tuple=features_to_cache
                )

            except Exception as e:
                if is_main_process:
                    print(f"[Rank {rank}] Error processing {episode_id}_vlm{vlm_idx}: {e}")

        if is_main_process:
            pbar.update(1)

    if is_main_process:
        pbar.close()


def main():
    parser = argparse.ArgumentParser(description="Cache VLM features for CLIP pre-training (DDP Version).")
    parser.add_argument('--new_dataset_paths', type=str, nargs='*',
                       default=["/home/najo/NAS/VLA/dataset/New_dataset", "/home/najo/NAS/VLA/dataset/New_dataset2"])
    parser.add_argument('--old_dataset_patterns', type=str, nargs='*', default=[])
    parser.add_argument('--batch_size', type=int, default=4,
                       help='Batch size for dataloader (higher = faster but more memory).')
    parser.add_argument('--num_workers', type=int, default=2,
                       help='Number of dataloader workers per GPU process.')
    parser.add_argument('--vlm_model', type=str, default="Qwen/Qwen2.5-VL-3B-Instruct",
                       help='VLM model for encoding.')
    parser.add_argument('--cache_root', type=str, default="/home/najo/NAS/VLA/dataset/cache",
                       help='Root directory for all caches.')
    parser.add_argument('--max_new_tokens', type=int, default=256)
    parser.add_argument('--skip_dataset_stats', action='store_true',
                       help='Skip dataset statistics collection for faster startup')

    args = parser.parse_args()

    # Setup distributed environment
    rank, world_size, local_rank, is_distributed = setup_distributed()

    if rank == 0:
        print(f"ğŸš€ Starting CLIP VLM feature caching with {world_size} GPUs")
        print(f"ğŸ“‚ Dataset paths: {args.new_dataset_paths}")
        print(f"ğŸ’¾ Cache root: {args.cache_root}")
        print(f"ğŸ¤– VLM model: {args.vlm_model}")
        print(f"ğŸ“¦ Batch size: {args.batch_size} (per GPU)")
        print(f"ğŸ‘· Workers: {args.num_workers} (per GPU)")
        print()

    # Create dataset (only on rank 0, then broadcast)
    if rank == 0:
        print("ğŸ“Š Creating dataset to identify all valid samples for caching...")
        unified_dataset = create_unified_dataloader(
            new_dataset_paths=args.new_dataset_paths,
            old_dataset_patterns=args.old_dataset_patterns,
            return_dataset=True,
            use_cache=False,
            skip_dataset_stats=args.skip_dataset_stats,
        )

        # This dataset filters for the last 20% of samples etc.
        clip_dataset = SensorImageCLIPDataset(
            unified_dataset,
            vlm_annotations={},
            use_augmentation=False,
            mode="cache_build"
        )
        print(f"âœ… Found {len(clip_dataset)} total valid samples to process.")
        print()
    else:
        # Other ranks: wait for rank 0 to create dataset
        unified_dataset = create_unified_dataloader(
            new_dataset_paths=args.new_dataset_paths,
            old_dataset_patterns=args.old_dataset_patterns,
            return_dataset=True,
            use_cache=False,
            skip_dataset_stats=args.skip_dataset_stats,
        )
        clip_dataset = SensorImageCLIPDataset(
            unified_dataset,
            vlm_annotations={},
            use_augmentation=False,
            mode="cache_build"
        )

    # Synchronize all processes
    if is_distributed:
        dist.barrier()

    # Run caching worker
    cache_worker(rank, world_size, local_rank, args, clip_dataset)

    # Synchronize before final stats
    if is_distributed:
        dist.barrier()

    # Print final stats (only rank 0)
    if rank == 0:
        print("\nâœ… VLM feature caching complete.")
        cache_manager = VLACacheManager(cache_dir=str(Path(args.cache_root) / "clip_vlm_features"))
        stats = cache_manager.get_cache_stats()
        print("ğŸ“Š Cache statistics:")
        for key, value in stats.items():
            print(f"   {key}: {value}")

    # Cleanup
    cleanup_distributed()


if __name__ == "__main__":
    main()
