"""
Cache VLM Features for CLIP Pre-training - DDP Version

This script pre-computes and caches features from the VLM for the samples
that will be used in `TRAIN_SensorImage_CLIP.py`.

This version uses torch.distributed (DDP) to parallelize the workload across
multiple GPUs, loading one VLM per GPU to avoid memory issues.

Usage:
    torchrun --nproc_per_node=4 cache_clip_vlm_features.py \
        --new_dataset_paths "/path/to/dataset1" "/path/to/dataset2" \
        --cache_root "/path/to/cache" \
        --vlm_model "Qwen/Qwen2.5-VL-7B-Instruct"
"""

# =============================================================================
# âš ï¸ CRITICAL: CLIP VLM ìºì‹œ êµ¬ì¡° ë° prompt_hash ë§¤ì¹­ (2025-01-12)
# =============================================================================
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” CLIP í•™ìŠµìš© VLM ìºì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
# ìºì‹œ êµ¬ì¡°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì´í•´í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.
#
# 1. CLIP VLM ìºì‹œ ê²½ë¡œ êµ¬ì¡°:
#    {cache_root}/clip_vlm_features/{prompt_hash}/{episode_name}_vlm{idx}.pt
#
#    - prompt_hash: CLIP_PROMPT_TEXT (TRAIN_SensorImage_CLIP.pyì—ì„œ import)ë¥¼
#                   MD5 í•´ì‹œí™”í•œ ê°’ (ì²« 8ì)
#    - CLIP_PROMPT_TEXTëŠ” ê³ ì •ëœ í…ìŠ¤íŠ¸ (íƒœìŠ¤í¬ ì´ë¦„ í¬í•¨ X)
#    - ëª¨ë“  íƒœìŠ¤í¬(Red/Blue/Green/White/Yellow point)ê°€ ë™ì¼í•œ prompt ì‚¬ìš©
#    - ë”°ë¼ì„œ ëª¨ë“  íƒœìŠ¤í¬ì˜ ìºì‹œê°€ í•˜ë‚˜ì˜ prompt_hash ë””ë ‰í† ë¦¬ì— ì €ì¥ë¨
#
# 2. Flow Matching VL ìºì‹œì™€ì˜ í•µì‹¬ ì°¨ì´ì :
#
#    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#    â”‚                 â”‚  CLIP VLM ìºì‹œ           â”‚  Flow Matching VL ìºì‹œ      â”‚
#    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#    â”‚ Prompt ì†ŒìŠ¤     â”‚ ê³ ì •ëœ CLIP_PROMPT_TEXT  â”‚ íƒœìŠ¤í¬ë³„ instruction        â”‚
#    â”‚ í•´ì‹œ ì•Œê³ ë¦¬ì¦˜   â”‚ MD5                      â”‚ SHA256                      â”‚
#    â”‚ task_name í¬í•¨  â”‚ X (ëª¨ë“  íƒœìŠ¤í¬ ë™ì¼)     â”‚ O (íƒœìŠ¤í¬ë³„ë¡œ ë‹¤ë¦„)         â”‚
#    â”‚ ìºì‹œ ë””ë ‰í† ë¦¬   â”‚ clip_vlm_features/       â”‚ qwen_vl_features/           â”‚
#    â”‚ prompt_hash ìˆ˜  â”‚ 1ê°œ (ëª¨ë“  íƒœìŠ¤í¬ ê³µìœ )   â”‚ 5ê°œ (íƒœìŠ¤í¬ë‹¹ 1ê°œ)          â”‚
#    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
#    ì˜ˆì‹œ:
#      - CLIP: ëª¨ë“  íƒœìŠ¤í¬ â†’ ë™ì¼ prompt â†’ 1ê°œ hash (ì˜ˆ: a1b2c3d4)
#              ìºì‹œ: /cache/clip_vlm_features/a1b2c3d4/
#
#      - Flow Matching:
#              Red_point   â†’ "...target is the Red point..."   â†’ hash1
#              Blue_point  â†’ "...target is the Blue point..."  â†’ hash2
#              ìºì‹œ: /cache/qwen_vl_features/hash1/, /cache/qwen_vl_features/hash2/, ...
#
# 3. âš ï¸ ìºì‹œ ë¬´íš¨í™” ì£¼ì˜ì‚¬í•­:
#    - CLIP_PROMPT_TEXTë¥¼ ë³€ê²½í•˜ë©´ prompt_hashê°€ ë°”ë€Œì–´ ê¸°ì¡´ ìºì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
#    - CLIP_PROMPT_TEXT ë³€ê²½ ì‹œ ë°˜ë“œì‹œ ìºì‹œë¥¼ ì¬ìƒì„±í•´ì•¼ í•¨
#    - í•™ìŠµ ì‹œ TRAIN_SensorImage_CLIP.pyì˜ CLIP_PROMPT_TEXTì™€ ì™„ì „íˆ ì¼ì¹˜í•´ì•¼ í•¨
#
# 4. ìºì‹œ ìƒì„±ê³¼ í•™ìŠµ ê°„ ì¼ê´€ì„±:
#    âœ… ë°˜ë“œì‹œ í™•ì¸í•´ì•¼ í•  ì‚¬í•­:
#       - ë™ì¼í•œ CLIP_PROMPT_TEXT ì‚¬ìš© (TRAIN_SensorImage_CLIP.pyì—ì„œ import)
#       - ë™ì¼í•œ cache_root ê²½ë¡œ
#       - ë™ì¼í•œ VLM ëª¨ë¸ (Qwen2.5-VL-3B-Instruct ë“±)
#
# 5. 2025-01-12 ìºì‹œ ë¬¸ì œ í•´ê²° êµí›ˆ:
#    - Flow Matching VL ìºì‹œê°€ íƒœìŠ¤í¬ë³„ë¡œ ë¶„ë¦¬ëœ ì´ìœ ë¥¼ ì´í•´
#    - CLIPì€ ëª¨ë“  íƒœìŠ¤í¬ì— ë™ì¼í•œ promptë¥¼ ì‚¬ìš©í•˜ì—¬ í†µí•© ìºì‹œ ìƒì„±
#    - ì´ ì°¨ì´ì ì„ ì¸ì§€í•˜ì§€ ëª»í•˜ë©´ ìºì‹œë¥¼ ì°¾ì§€ ëª»í•˜ëŠ” ë¬¸ì œ ë°œìƒ ê°€ëŠ¥
# =============================================================================

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import argparse
import json
from pathlib import Path
import torch
import torch.distributed as dist
from torch.utils.data import Dataset, DataLoader, Subset, DistributedSampler
from tqdm import tqdm
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from PIL import Image
import time

# Add project root to import custom modules
import sys
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from vla_datasets.unified_dataset import create_unified_dataloader
from TRAIN_SensorImage_CLIP import (
    SensorImageCLIPDataset,
    CLIP_PROMPT_TEXT,
    get_clip_prompt_hash,
)
from qwen_vl_utils import process_vision_info
from vla_cache_manager import VLACacheManager


def disable_generation_temperature(vlm_model):
    """
    Keep the temperature attribute but neutralize it so the model
    does not try to use it (while avoiding HF warnings).
    """
    gen_cfg = getattr(vlm_model, "generation_config", None)
    if gen_cfg is None:
        return

    extra_params = getattr(gen_cfg, "_extra_generation_params", None)
    if isinstance(extra_params, dict):
        extra_params.pop("temperature", None)

    try:
        setattr(gen_cfg, "temperature", None)
    except AttributeError:
        gen_cfg.__dict__["temperature"] = None


def setup_distributed():
    """Initialize distributed training environment."""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        print('Not using distributed mode')
        return 0, 1, 0, False

    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend='nccl')

    return rank, world_size, local_rank, True


def cleanup_distributed():
    """Cleanup distributed training environment."""
    if dist.is_initialized():
        dist.destroy_process_group()


def generate_text_response(
    vlm_model,
    vlm_processor,
    generation_text_input,
    vision_input,
    max_new_tokens,
):
    """Generate the VLM's textual response for the given prompt/image pair."""
    model_inputs = vlm_processor(
        text=[generation_text_input],
        images=[vision_input],
        padding=True,
        return_tensors="pt",
    ).to(device=vlm_model.device, dtype=vlm_model.dtype)

    input_lengths = [len(ids) for ids in model_inputs.input_ids]
    with torch.no_grad():
        generated_ids = vlm_model.generate(
            **model_inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            num_beams=1,
        )
    trimmed = [ids[len:] for ids, len in zip(generated_ids, input_lengths)]
    response = vlm_processor.batch_decode(trimmed, skip_special_tokens=True)[0]
    if "<|im_start|>assistant" in response:
        response = response.split("<|im_start|>assistant", 1)[-1]
    if "<|im_end|>" in response:
        response = response.split("<|im_end|>", 1)[0]
    return response.strip()


def cache_worker(rank, world_size, local_rank, args, clip_dataset):
    """
    The worker function for each DDP process.
    Loads a model onto its assigned GPU and processes a subset of the data.
    """
    device = torch.device(f"cuda:{local_rank}")
    is_main_process = rank == 0

    # Each worker gets its own cache manager instance
    cache_manager = VLACacheManager(cache_dir=str(Path(args.cache_root) / "clip_vlm_features"))
    prompt_hash = get_clip_prompt_hash()

    # 1. Load VLM (one per process)
    if is_main_process:
        print(f"[Rank {rank}] Loading VLM on GPU {local_rank}...")

    vlm_processor = AutoProcessor.from_pretrained(args.vlm_model, trust_remote_code=True)
    vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        args.vlm_model,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map={"": device},  # Load entire model on this specific GPU
        attn_implementation="flash_attention_2"
    )
    vlm_model.eval()
    disable_generation_temperature(vlm_model)

    if is_main_process:
        print(f"[Rank {rank}] VLM loaded on {device}.")

    # 2. Create DistributedSampler for this worker
    sampler = DistributedSampler(
        clip_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=False,
        drop_last=False
    )

    def collate_fn_cache(batch):
        """Collate function that keeps samples as list for batch processing"""
        return batch

    dataloader = DataLoader(
        clip_dataset,
        batch_size=args.batch_size,  # Process in batches
        sampler=sampler,
        num_workers=args.num_workers,
        collate_fn=collate_fn_cache
    )

    # 3. Iterate and cache features
    if is_main_process:
        pbar = tqdm(total=len(dataloader), desc=f"Rank {rank} (GPU {local_rank})")

    for batch in dataloader:
        # Process each sample in the batch
        for sample in batch:
            image = sample["hand_eye_image"]
            episode_id = sample["episode_id"]
            vlm_idx = sample["vlm_idx"]

            if vlm_idx is None:
                continue

            if cache_manager.cache_exists(dataset_name=episode_id, vlm_idx=vlm_idx, prompt_hash=prompt_hash):
                continue

            # Generate text, image embeds, and text embeds
            try:
                messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": CLIP_PROMPT_TEXT}]}]
                generation_text_input = vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                vision_input, _ = process_vision_info(messages)

                text_response = generate_text_response(
                    vlm_model, vlm_processor, generation_text_input, vision_input, args.max_new_tokens
                )

                with torch.no_grad():
                    # 1. ì´ë¯¸ì§€ ì „ìš© ì¶”ë¡  (ìˆœìˆ˜ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ - ëª¨ë“  ì´ë¯¸ì§€ í† í°)
                    image_only_messages = [{"role": "user", "content": [{"type": "image", "image": vision_input}, {"type": "text", "text": ""}]}]
                    image_text_with_placeholders = vlm_processor.apply_chat_template(
                        image_only_messages, tokenize=False, add_generation_prompt=False
                    )
                    image_inputs = vlm_processor(
                        text=[image_text_with_placeholders],
                        images=[vision_input], padding=True, return_tensors="pt"
                    ).to(device=vlm_model.device, dtype=vlm_model.dtype)

                    image_outputs = vlm_model(**image_inputs, output_hidden_states=True, return_dict=True)
                    image_hidden_state = image_outputs.hidden_states[-1]

                    # ì´ë¯¸ì§€ í† í°ë§Œ ì¶”ì¶œ (í† í° ID 151857)
                    image_token_mask = (image_inputs['input_ids'] == 151857)
                    image_indices = torch.where(image_token_mask.squeeze(0))[0]
                    image_features = image_hidden_state[:, image_indices, :]

                    # 2. í…ìŠ¤íŠ¸ ì „ìš© ì¶”ë¡  (ê°€ì´ë˜ìŠ¤ ë²¡í„° ì¶”ì¶œ - í‰ê·  í’€ë§)
                    text_inputs = vlm_processor(
                        text=[text_response], images=None, padding=True, return_tensors="pt"
                    ).to(device=vlm_model.device, dtype=vlm_model.dtype)

                    text_outputs = vlm_model(**text_inputs, output_hidden_states=True, return_dict=True)
                    text_hidden_state = text_outputs.hidden_states[-1]
                    guidance_vector = text_hidden_state.mean(dim=1)

                # 3. ìºì‹œ ì €ì¥ (íŠœí”Œ í˜•ì‹ìœ¼ë¡œ)
                features_to_cache = (
                    image_features.detach().to("cpu", dtype=torch.float16),
                    guidance_vector.detach().to("cpu", dtype=torch.float16)
                )

                cache_manager.save_cache_tuple(
                    dataset_name=episode_id, vlm_idx=vlm_idx, prompt_hash=prompt_hash, features_tuple=features_to_cache
                )

            except Exception as e:
                if is_main_process:
                    print(f"[Rank {rank}] Error processing {episode_id}_vlm{vlm_idx}: {e}")

        if is_main_process:
            pbar.update(1)

    if is_main_process:
        pbar.close()


def main():
    parser = argparse.ArgumentParser(description="Cache VLM features for CLIP pre-training (DDP Version).")
    parser.add_argument('--new_dataset_paths', type=str, nargs='*',
                       default=["/home/najo/NAS/VLA/dataset/New_dataset", "/home/najo/NAS/VLA/dataset/New_dataset2"])
    parser.add_argument('--old_dataset_patterns', type=str, nargs='*', default=[])
    parser.add_argument('--batch_size', type=int, default=4,
                       help='Batch size for dataloader (higher = faster but more memory).')
    parser.add_argument('--num_workers', type=int, default=2,
                       help='Number of dataloader workers per GPU process.')
    parser.add_argument('--vlm_model', type=str, default="Qwen/Qwen2.5-VL-3B-Instruct",
                       help='VLM model for encoding.')
    parser.add_argument('--cache_root', type=str, default="/home/najo/NAS/VLA/dataset/cache",
                       help='Root directory for all caches.')
    parser.add_argument('--max_new_tokens', type=int, default=256)

    args = parser.parse_args()

    # Setup distributed environment
    rank, world_size, local_rank, is_distributed = setup_distributed()

    if rank == 0:
        print(f"ğŸš€ Starting CLIP VLM feature caching with {world_size} GPUs")
        print(f"ğŸ“‚ Dataset paths: {args.new_dataset_paths}")
        print(f"ğŸ’¾ Cache root: {args.cache_root}")
        print(f"ğŸ¤– VLM model: {args.vlm_model}")
        print(f"ğŸ“¦ Batch size: {args.batch_size} (per GPU)")
        print(f"ğŸ‘· Workers: {args.num_workers} (per GPU)")
        print()

    # Create dataset (only on rank 0, then broadcast)
    if rank == 0:
        print("ğŸ“Š Creating dataset to identify all valid samples for caching...")
        unified_dataset = create_unified_dataloader(
            new_dataset_paths=args.new_dataset_paths,
            old_dataset_patterns=args.old_dataset_patterns,
            return_dataset=True,
            use_cache=False,
        )

        # This dataset filters for the last 20% of samples etc.
        clip_dataset = SensorImageCLIPDataset(
            unified_dataset,
            vlm_annotations={},
            use_augmentation=False,
            mode="cache_build"
        )
        print(f"âœ… Found {len(clip_dataset)} total valid samples to process.")
        print()
    else:
        # Other ranks: wait for rank 0 to create dataset
        unified_dataset = create_unified_dataloader(
            new_dataset_paths=args.new_dataset_paths,
            old_dataset_patterns=args.old_dataset_patterns,
            return_dataset=True,
            use_cache=False,
        )
        clip_dataset = SensorImageCLIPDataset(
            unified_dataset,
            vlm_annotations={},
            use_augmentation=False,
            mode="cache_build"
        )

    # Synchronize all processes
    if is_distributed:
        dist.barrier()

    # Run caching worker
    cache_worker(rank, world_size, local_rank, args, clip_dataset)

    # Synchronize before final stats
    if is_distributed:
        dist.barrier()

    # Print final stats (only rank 0)
    if rank == 0:
        print("\nâœ… VLM feature caching complete.")
        cache_manager = VLACacheManager(cache_dir=str(Path(args.cache_root) / "clip_vlm_features"))
        stats = cache_manager.get_cache_stats()
        print("ğŸ“Š Cache statistics:")
        for key, value in stats.items():
            print(f"   {key}: {value}")

    # Cleanup
    cleanup_distributed()


if __name__ == "__main__":
    main()
