{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FlowMatchingActionExpert Cross-Attention Heatmap\n",
        "\n",
        "이 노트북은 `FlowMatchingActionExpert` / `ModulatedDecoderLayer`에서 추출한 cross-attention 가중치를 시각화합니다.\n",
        "- Vision 토큰과 Sensor(+Robot) 토큰을 하나의 메모리로 구성\n",
        "- 마지막 디코더 층의 cross-attention weight를 저장 후 heatmap으로 표현\n",
        "- Vision 대비 Sensor 중요도 비율을 곡선으로 확인\n",
        "\n",
        "더미 텐서를 사용하지만, 실제 피쳐로 교체하면 그대로 활용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from models.action_decoder import FlowMatchingActionExpert\n",
        "\n",
        "# CUDA가 있으면 사용\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ FlowMatchingActionExpert V2 (Cross-Attention + ModulatedDecoder) 초기화 완료\n",
            "   4개의 ModulatedDecoderLayer 사용\n",
            "{'context_features': (1, 16, 2048), 'sensor_features': (1, 8, 2048), 'guidance_vector': (1, 2048), 'actions': (1, 8, 7)}\n"
          ]
        }
      ],
      "source": [
        "# FlowMatchingActionExpert 초기화 (필요 시 config에 맞춰 수정)\n",
        "flow_model = FlowMatchingActionExpert(\n",
        "    image_feature_dim=2048,\n",
        "    text_guidance_dim=2048,\n",
        "    sensor_dim=2048,\n",
        "    action_dim=7,\n",
        "    horizon=8,\n",
        "    hidden_dim=512,\n",
        "    nhead=8,\n",
        "    num_decoder_layers=4,\n",
        "    dropout=0.1,\n",
        ")\n",
        "flow_model = flow_model.to(device).eval()\n",
        "\n",
        "# 더미 입력 (실제 실험에서는 아래 텐서를 실제 feature로 교체)\n",
        "B = 1\n",
        "Sv, Ss = 16, 8  # vision / sensor token 길이\n",
        "D_img = flow_model.context_proj.in_features\n",
        "D_sensor = flow_model.sensor_proj.in_features\n",
        "guidance_dim = flow_model.text_guidance_proj.in_features\n",
        "H, action_dim = flow_model.horizon, flow_model.action_dim\n",
        "\n",
        "context_features = torch.randn(B, Sv, D_img, device=device)\n",
        "sensor_features = torch.randn(B, Ss, D_sensor, device=device)\n",
        "guidance_vector = torch.randn(B, guidance_dim, device=device)\n",
        "actions = torch.randn(B, H, action_dim, device=device)\n",
        "print({\n",
        "    \"context_features\": tuple(context_features.shape),\n",
        "    \"sensor_features\": tuple(sensor_features.shape),\n",
        "    \"guidance_vector\": tuple(guidance_vector.shape),\n",
        "    \"actions\": tuple(actions.shape),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "attn_mean shape: (8, 24), Sv=16, Ss=8\n"
          ]
        }
      ],
      "source": [
        "# Flow Matching 단계: normalize -> flow -> forward\n",
        "with torch.no_grad():\n",
        "    actions_n = actions / flow_model.action_scale\n",
        "    x_t, u_t, t_scalar = flow_model.flow.compute_flow_and_target(actions_n)\n",
        "    _ = flow_model.forward(\n",
        "        x_t,\n",
        "        t_scalar,\n",
        "        context_features,\n",
        "        guidance_vector,\n",
        "        sensor_features=sensor_features,\n",
        "    )\n",
        "\n",
        "attn = flow_model.mod_layers[-1].last_cross_attn_weights\n",
        "if attn is None:\n",
        "    raise RuntimeError(\"Cross-attention weights were not captured.\")\n",
        "\n",
        "attn = attn.detach().cpu()\n",
        "attn_mean = attn.mean(dim=1)[0]  # (H, S)\n",
        "Sv = context_features.shape[1]\n",
        "Ss = sensor_features.shape[1]\n",
        "print(f\"attn_mean shape: {tuple(attn_mean.shape)}, Sv={Sv}, Ss={Ss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Heatmap + Vision/Sensor 비율 시각화\n",
        "S = attn_mean.shape[1]\n",
        "vision_importance = attn_mean[:, :Sv].sum(dim=-1)\n",
        "sensor_importance = attn_mean[:, Sv:].sum(dim=-1)\n",
        "ratio = vision_importance / (vision_importance + sensor_importance + 1e-8)\n",
        "steps = torch.arange(attn_mean.shape[0])\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8), constrained_layout=True)\n",
        "\n",
        "im = axes[0].imshow(attn_mean.numpy(), aspect=\"auto\", cmap=\"viridis\")\n",
        "axes[0].axvline(x=Sv - 0.5, color=\"red\", linestyle=\"--\", label=\"Vision/Sensor boundary\")\n",
        "axes[0].set_title(\"Cross-attention heatmap (mean over heads)\")\n",
        "axes[0].set_ylabel(\"Action timestep\")\n",
        "axes[0].set_xlabel(\"Token index\")\n",
        "axes[0].legend(loc=\"upper right\")\n",
        "fig.colorbar(im, ax=axes[0], fraction=0.025)\n",
        "\n",
        "axes[1].plot(steps.numpy(), ratio.numpy(), label=\"Vision ratio\", color=\"tab:blue\")\n",
        "axes[1].plot(steps.numpy(), (1 - ratio).numpy(), label=\"Sensor ratio\", color=\"tab:orange\")\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].set_title(\"Vision vs Sensor attention contribution\")\n",
        "axes[1].set_xlabel(\"Action timestep\")\n",
        "axes[1].set_ylabel(\"Importance ratio\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Episode-Level Cross-Attention Timeline (real data)\n",
        "\n",
        "모든 피치 필드 대신 층스토링 구동을 사용하며, 지정한 에피소드를 순차적으로 순회해 모든 cross-attention 가중치를 수집합니다.  \n",
        "- VL 캐시가 있다면 바로 불러오고, `use_cache=False`로 설정하면 노트북이 자동으로 Qwen 인코더를 호출해 라이브로 이미지 토큰을 만듭니다.  \n",
        "- `MAX_STEPS`, `TARGET_STEP_IDX` 등은 조사 범위와 시각화 대상을 조절합니다.  \n",
        "- `UnifiedVLADataset`은 순서를 유지하므로 결과 그래프의 x축을 원본 시간 축으로 해석하면 됩니다.  \n",
        "- 멀티뷰 중 일부만 쓰려면 `views_to_use` 옵션을 지정해 빈 이미지 리스트가 나오지 않도록 해 주세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode root: /home/najo/NAS/VLA/dataset/New_dataset4/Eye_trocar/data_collection_20251115_003810\n",
            "Checkpoint: checkpoints/flow_matching_best.pt\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from vla_datasets.unified_dataset import UnifiedVLADataset\n",
        "from models.unified_model import QwenVLAUnified\n",
        "\n",
        "EPISODE_PATH = Path(\"/home/najo/NAS/VLA/dataset/New_dataset4/Eye_trocar/data_collection_20251115_003810\")\n",
        "CACHE_ROOT = Path(\"/home/najo/NAS/VLA/dataset/cache/qwen_vl_features\")\n",
        "CHECKPOINT_PATH = Path(\"checkpoints/flow_matching_best.pt\")\n",
        "\n",
        "SENSOR_WINDOW = 65\n",
        "ROBOT_WINDOW = 100\n",
        "VLM_REUSE_COUNT = 3\n",
        "ACTION_EXPERT_HZ = 10\n",
        "MAX_STEPS = None\n",
        "TARGET_STEP_IDX = 120\n",
        "\n",
        "assert EPISODE_PATH.exists(), f\"{EPISODE_PATH} not found\"\n",
        "assert CACHE_ROOT.exists(), f\"{CACHE_ROOT} not found\"\n",
        "assert CHECKPOINT_PATH.exists(), f\"{CHECKPOINT_PATH} not found\"\n",
        "\n",
        "print(f\"Episode root: {EPISODE_PATH}\")\n",
        "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples in episode: 292\n",
            "🚀 QwenVLA 통합 모델 V2 (Cross-Attention) 로딩 중\n",
            "   모델 타입: FLOW_MATCHING\n",
            "   센서 활성화: True\n",
            "   로봇 상태 활성화: True\n",
            "   Flow 스텝: 10, 솔버: euler\n",
            "   이미지 리사이즈: 640x360\n",
            "🧠 flash_attention_2 어텐션과 torch.bfloat16 데이터 타입으로 Qwen-VL 로드 시도 중...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe19dbbd317d4fc397c539fa213171ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6068876090744b3d923723ee7e90a0f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0715561999484c57ae52a8d75173d93b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Qwen-VL 모델 로드 성공: flash_attention_2 어텐션 (torch.bfloat16)\n",
            "   VL 모델 hidden_size: 2048\n",
            "🧊 VL 모델 매개변수 동결 중...\n",
            "✅ VL 모델 동결 완료.\n",
            "   센서 인코더 타입: Force-Aware\n",
            "   📊 Conv backbone: 1025ch → ... → 1024ch (hidden_dim=128)\n",
            "   🔧 Mode: Conv(1024) → Projection(256) → Transformer(256)\n",
            "   💡 Lightweight mode\n",
            "   💡 Lightweight mode: Conv(1024) → Projection(256) → Transformer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/najo/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ FlowMatchingActionExpert V2 (Cross-Attention + ModulatedDecoder) 초기화 완료\n",
            "   4개의 ModulatedDecoderLayer 사용\n",
            "✅ 모델 초기화 완료!\n",
            "Loaded trained FlowMatchingActionExpert for episode sweep.\n"
          ]
        }
      ],
      "source": [
        "episode_dataset = UnifiedVLADataset(\n",
        "    data_dir=str(EPISODE_PATH),\n",
        "    format=\"auto\",\n",
        "    horizon=flow_model.horizon if 'flow_model' in globals() else 8,\n",
        "    vlm_reuse_count=VLM_REUSE_COUNT,\n",
        "    sensor_window_size=SENSOR_WINDOW,\n",
        "    robot_window_size=ROBOT_WINDOW,\n",
        "    action_expert_hz=ACTION_EXPERT_HZ,\n",
        "    cache_root=str(CACHE_ROOT),\n",
        "    use_cache=False,\n",
        "    disable_sensor=False,\n",
        "    disable_robot_state=False,\n",
        ")\n",
        "print(f\"Total samples in episode: {len(episode_dataset)}\")\n",
        "\n",
        "flow_hidden_dim = flow_model.hidden_dim if 'flow_model' in globals() else 1024\n",
        "flow_horizon = flow_model.horizon if 'flow_model' in globals() else 8\n",
        "flow_action_dim = flow_model.action_dim if 'flow_model' in globals() else 7\n",
        "\n",
        "vl_model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "trained_model = QwenVLAUnified(\n",
        "    model_type='flow_matching', vl_model_name=vl_model_name, action_dim=7, horizon=8,\n",
        "    hidden_dim=1024, sensor_enabled=True,\n",
        "    sensor_encoder_type='force_aware', # Match pre-training architecture\n",
        "    sensor_input_channels=1026,\n",
        "    sensor_temporal_length=65,\n",
        "    sensor_hidden_dim=128,  # Conv backbone channels\n",
        "    sensor_output_dim=1024,\n",
        "    sensor_transformer_dim=256,  # Transformer projection\n",
        "    robot_state_enabled=True,\n",
        "    robot_state_output_dim=1024, # Match pre-training architecture\n",
        "    image_resize_height=360, image_resize_width=640,\n",
        "    device_map=None,\n",
        ")\n",
        "\n",
        "state = torch.load(str(CHECKPOINT_PATH), map_location='cpu')\n",
        "loaded = state.get(\"model_state_dict\", state)\n",
        "missing, unexpected = trained_model.load_state_dict(loaded, strict=False)\n",
        "if missing:\n",
        "    print(f\"Missing keys: {len(missing)} (first: {missing[:3]})\")\n",
        "if unexpected:\n",
        "    print(f\"Unexpected keys: {len(unexpected)} (first: {unexpected[:3]})\")\n",
        "\n",
        "trained_model.eval()\n",
        "flow_model_trained = trained_model.action_expert\n",
        "flow_model_trained.eval()\n",
        "print(\"Loaded trained FlowMatchingActionExpert for episode sweep.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   0%|          | 0/292 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   0%|          | 1/292 [00:00<03:43,  1.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 0] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   1%|          | 2/292 [00:01<03:39,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 1] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   1%|          | 3/292 [00:02<03:35,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 2] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   1%|▏         | 4/292 [00:02<03:33,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 3] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   2%|▏         | 5/292 [00:03<03:34,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 4] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   2%|▏         | 6/292 [00:04<03:33,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 5] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   2%|▏         | 7/292 [00:05<03:31,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 6] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   3%|▎         | 8/292 [00:05<03:30,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 7] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   3%|▎         | 9/292 [00:06<03:29,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 8] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   3%|▎         | 10/292 [00:07<03:29,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 9] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   4%|▍         | 11/292 [00:08<03:29,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 10] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   4%|▍         | 12/292 [00:08<03:29,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 11] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   4%|▍         | 13/292 [00:09<03:28,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 12] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   5%|▍         | 14/292 [00:10<03:27,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 13] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   5%|▌         | 15/292 [00:11<03:25,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 14] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   5%|▌         | 16/292 [00:11<03:26,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 15] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   6%|▌         | 17/292 [00:12<03:25,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 16] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   6%|▌         | 18/292 [00:13<03:24,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 17] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   7%|▋         | 19/292 [00:14<03:25,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 18] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   7%|▋         | 20/292 [00:14<03:23,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 19] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   7%|▋         | 21/292 [00:15<03:22,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 20] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   8%|▊         | 22/292 [00:16<03:21,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 21] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   8%|▊         | 23/292 [00:17<03:21,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 22] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   8%|▊         | 24/292 [00:17<03:20,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 23] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   9%|▊         | 25/292 [00:18<03:20,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 24] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   9%|▉         | 26/292 [00:19<03:18,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 25] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:   9%|▉         | 27/292 [00:20<03:18,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 26] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  10%|▉         | 28/292 [00:20<03:17,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 27] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  10%|▉         | 29/292 [00:21<03:16,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 28] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  10%|█         | 30/292 [00:22<03:15,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 29] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  11%|█         | 31/292 [00:23<03:15,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 30] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  11%|█         | 32/292 [00:23<03:14,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 31] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  11%|█▏        | 33/292 [00:24<03:13,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 32] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  12%|█▏        | 34/292 [00:25<03:12,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 33] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  12%|█▏        | 35/292 [00:26<03:11,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 34] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  12%|█▏        | 36/292 [00:26<03:10,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 35] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  13%|█▎        | 37/292 [00:27<03:09,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 36] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  13%|█▎        | 38/292 [00:28<03:09,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 37] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  13%|█▎        | 39/292 [00:29<03:08,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 38] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  14%|█▎        | 40/292 [00:29<03:07,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 39] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  14%|█▍        | 41/292 [00:30<03:06,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 40] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  14%|█▍        | 42/292 [00:31<03:06,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 41] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  15%|█▍        | 43/292 [00:32<03:05,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 42] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  15%|█▌        | 44/292 [00:32<03:05,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 43] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  15%|█▌        | 45/292 [00:33<03:05,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 44] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  16%|█▌        | 46/292 [00:34<03:04,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 45] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  16%|█▌        | 47/292 [00:35<03:03,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 46] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  16%|█▋        | 48/292 [00:35<03:02,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 47] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  17%|█▋        | 49/292 [00:36<03:01,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 48] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  17%|█▋        | 50/292 [00:37<03:01,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 49] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  17%|█▋        | 51/292 [00:38<03:00,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 50] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  18%|█▊        | 52/292 [00:38<02:59,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 51] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  18%|█▊        | 53/292 [00:39<02:58,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 52] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  18%|█▊        | 54/292 [00:40<02:58,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 53] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  19%|█▉        | 55/292 [00:41<02:57,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 54] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  19%|█▉        | 56/292 [00:41<02:56,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 55] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  20%|█▉        | 57/292 [00:42<02:55,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 56] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  20%|█▉        | 58/292 [00:43<02:54,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 57] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  20%|██        | 59/292 [00:44<02:54,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 58] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  21%|██        | 60/292 [00:44<02:53,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 59] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  21%|██        | 61/292 [00:45<02:52,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 60] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  21%|██        | 62/292 [00:46<02:52,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 61] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  22%|██▏       | 63/292 [00:47<02:50,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 62] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  22%|██▏       | 64/292 [00:47<02:51,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 63] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  22%|██▏       | 65/292 [00:48<02:49,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 64] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  23%|██▎       | 66/292 [00:49<02:48,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 65] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  23%|██▎       | 67/292 [00:50<02:47,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 66] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  23%|██▎       | 68/292 [00:50<02:46,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 67] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  24%|██▎       | 69/292 [00:51<02:46,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 68] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  24%|██▍       | 70/292 [00:52<02:46,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 69] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  24%|██▍       | 71/292 [00:53<02:45,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 70] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  25%|██▍       | 72/292 [00:53<02:44,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 71] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  25%|██▌       | 73/292 [00:54<02:47,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 72] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  25%|██▌       | 74/292 [00:55<02:46,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 73] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  26%|██▌       | 75/292 [00:56<02:45,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 74] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  26%|██▌       | 76/292 [00:56<02:45,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 75] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  26%|██▋       | 77/292 [00:57<02:44,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 76] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  27%|██▋       | 78/292 [00:58<02:43,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 77] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  27%|██▋       | 79/292 [00:59<02:42,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 78] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  27%|██▋       | 80/292 [00:59<02:40,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 79] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  28%|██▊       | 81/292 [01:00<02:39,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 80] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  28%|██▊       | 82/292 [01:01<02:38,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 81] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  28%|██▊       | 83/292 [01:02<02:37,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 82] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  29%|██▉       | 84/292 [01:02<02:36,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 83] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  29%|██▉       | 85/292 [01:03<02:36,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 84] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  29%|██▉       | 86/292 [01:04<02:35,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 85] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  30%|██▉       | 87/292 [01:05<02:34,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 86] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  30%|███       | 88/292 [01:05<02:32,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 87] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  30%|███       | 89/292 [01:06<02:31,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 88] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  31%|███       | 90/292 [01:07<02:31,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 89] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  31%|███       | 91/292 [01:08<02:31,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 90] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  32%|███▏      | 92/292 [01:08<02:30,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 91] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  32%|███▏      | 93/292 [01:09<02:40,  1.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 92] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  32%|███▏      | 94/292 [01:10<02:36,  1.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 93] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  33%|███▎      | 95/292 [01:11<02:32,  1.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 94] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  33%|███▎      | 96/292 [01:12<02:30,  1.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 95] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  33%|███▎      | 97/292 [01:12<02:30,  1.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 96] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  34%|███▎      | 98/292 [01:13<02:28,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 97] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  34%|███▍      | 99/292 [01:14<02:26,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 98] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  34%|███▍      | 100/292 [01:15<02:25,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 99] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  35%|███▍      | 101/292 [01:15<02:24,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 100] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  35%|███▍      | 102/292 [01:16<02:23,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 101] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  35%|███▌      | 103/292 [01:17<02:22,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 102] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  36%|███▌      | 104/292 [01:18<02:21,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 103] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  36%|███▌      | 105/292 [01:18<02:20,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 104] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  36%|███▋      | 106/292 [01:19<02:19,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 105] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  37%|███▋      | 107/292 [01:20<02:18,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 106] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  37%|███▋      | 108/292 [01:21<02:18,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 107] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  37%|███▋      | 109/292 [01:21<02:17,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 108] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  38%|███▊      | 110/292 [01:22<02:17,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 109] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  38%|███▊      | 111/292 [01:23<02:16,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 110] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  38%|███▊      | 112/292 [01:24<02:15,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 111] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  39%|███▊      | 113/292 [01:24<02:14,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 112] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  39%|███▉      | 114/292 [01:25<02:13,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 113] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  39%|███▉      | 115/292 [01:26<02:12,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 114] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  40%|███▉      | 116/292 [01:27<02:11,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 115] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  40%|████      | 117/292 [01:27<02:10,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 116] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  40%|████      | 118/292 [01:28<02:10,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 117] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  41%|████      | 119/292 [01:29<02:09,  1.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 118] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  41%|████      | 120/292 [01:30<02:08,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 119] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  41%|████▏     | 121/292 [01:30<02:07,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 120] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  42%|████▏     | 122/292 [01:31<02:06,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 121] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  42%|████▏     | 123/292 [01:32<02:06,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 122] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  42%|████▏     | 124/292 [01:33<02:05,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 123] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  43%|████▎     | 125/292 [01:33<02:04,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 124] Live VL encoder returned 0 vision tokens, skipping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing episode:  43%|████▎     | 125/292 [01:34<02:05,  1.33it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 36\u001b[0m     img_tokens, txt_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvl_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Live VL encoder returned 0 vision tokens, skipping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/NAS/VLA/Insertion_VLAv3/models/vl_encoder.py:113\u001b[0m, in \u001b[0;36mVisionLanguageEncoder.encode\u001b[0;34m(self, text_inputs, image_inputs, cache_keys, use_cache)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📝 VisionLanguageEncoder: Using sequential view encoding with caching.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequential_encoding_confirmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/NAS/VLA/Insertion_VLAv3/models/vl_encoder.py:234\u001b[0m, in \u001b[0;36mVisionLanguageEncoder._encode_sequential\u001b[0;34m(self, text_inputs, image_inputs, cache_keys, use_cache)\u001b[0m\n\u001b[1;32m    228\u001b[0m image_only_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    229\u001b[0m     k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m image_only_inputs_cpu\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    231\u001b[0m }\n\u001b[1;32m    232\u001b[0m image_only_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m image_only_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m--> 234\u001b[0m image_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimage_only_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m image_hidden_state \u001b[38;5;241m=\u001b[39m image_outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    237\u001b[0m image_token_mask \u001b[38;5;241m=\u001b[39m (image_only_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m151857\u001b[39m)\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/utils/generic.py:757\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    756\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 757\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    759\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1495\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1490\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n\u001b[1;32m   1491\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1492\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1493\u001b[0m )\n\u001b[0;32m-> 1495\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1514\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1282\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **kwargs)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input_embeddings()(input_ids)\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m     image_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(image_embeds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice, inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1284\u001b[0m     image_mask, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_placeholder_mask(\n\u001b[1;32m   1285\u001b[0m         input_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds, image_features\u001b[38;5;241m=\u001b[39mimage_embeds\n\u001b[1;32m   1286\u001b[0m     )\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1195\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.get_image_features\u001b[0;34m(self, pixel_values, image_grid_thw)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;124;03mEncodes images into continuous embeddings that can be forwarded to the language model.\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m        The temporal, height and width of feature shape of each image in LLM.\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1195\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m split_sizes \u001b[38;5;241m=\u001b[39m (image_grid_thw\u001b[38;5;241m.\u001b[39mprod(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mspatial_merge_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m   1197\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(image_embeds, split_sizes)\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:454\u001b[0m, in \u001b[0;36mQwen2_5_VisionTransformerPretrainedModel.forward\u001b[0;34m(self, hidden_states, grid_thw, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m         cu_seqlens_now \u001b[38;5;241m=\u001b[39m cu_window_seqlens\n\u001b[0;32m--> 454\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens_now\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerger(hidden_states)\n\u001b[1;32m    462\u001b[0m reverse_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(window_index)\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:281\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionBlock.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 281\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states))\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:222\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionAttention.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# Flash Attention 2: Use cu_seqlens for variable length attention\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     max_seqlen \u001b[38;5;241m=\u001b[39m (cu_seqlens[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m cu_seqlens[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m--> 222\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seq_lens_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seq_lens_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# Other implementations: Process each chunk separately\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m cu_seqlens[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m cu_seqlens[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/integrations/flash_attention.py:70\u001b[0m, in \u001b[0;36mflash_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, sliding_window, softcap, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented\u001b[39;00m\n\u001b[1;32m     68\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m is_causal \u001b[38;5;28;01mif\u001b[39;00m is_causal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m module\u001b[38;5;241m.\u001b[39mis_causal\n\u001b[0;32m---> 70\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43m_flash_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_top_left_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_use_top_left_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py:647\u001b[0m, in \u001b[0;36m_flash_attention_forward\u001b[0;34m(query_states, key_states, value_states, attention_mask, query_length, is_causal, dropout, position_ids, softmax_scale, sliding_window, use_top_left_mask, softcap, deterministic, cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k, target_dtype, attn_implementation, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(q\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m    645\u001b[0m     cu_seq_lens_k \u001b[38;5;241m=\u001b[39m cu_seq_lens_k\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m--> 647\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mflash_varlen_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seq_lens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seq_lens_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    658\u001b[0m     out \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:1066\u001b[0m, in \u001b[0;36mflash_attn_varlen_func\u001b[0;34m(q, k, v, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal, window_size, alibi_slopes, deterministic, return_attn_probs, block_table)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflash_attn_varlen_func\u001b[39m(\n\u001b[1;32m    996\u001b[0m     q,\n\u001b[1;32m    997\u001b[0m     k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     block_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1011\u001b[0m ):\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"dropout_p should be set to 0.0 during evaluation\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;124;03m    Supports multi-query and grouped-query attention (MQA/GQA) by passing in K, V with fewer heads\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;124;03m    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m            pattern (negative means that location was dropped, nonnegative means it was kept).\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlashAttnVarlenFunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attn_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:581\u001b[0m, in \u001b[0;36mFlashAttnVarlenFunc.forward\u001b[0;34m(ctx, q, k, v, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal, window_size, alibi_slopes, deterministic, return_softmax, block_table)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m softmax_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     softmax_scale \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m--> 581\u001b[0m out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state \u001b[38;5;241m=\u001b[39m \u001b[43m_flash_attn_varlen_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_softmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_softmax\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\n\u001b[1;32m    598\u001b[0m     q, k, v, out_padded, softmax_lse, cu_seqlens_q, cu_seqlens_k, rng_state\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    600\u001b[0m ctx\u001b[38;5;241m.\u001b[39mdropout_p \u001b[38;5;241m=\u001b[39m dropout_p\n",
            "File \u001b[0;32m~/.conda/envs/qwen_env/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py:86\u001b[0m, in \u001b[0;36m_flash_attn_varlen_forward\u001b[0;34m(q, k, v, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal, window_size, alibi_slopes, return_softmax, block_table)\u001b[0m\n\u001b[1;32m     84\u001b[0m maybe_contiguous \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mcontiguous() \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m     85\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m [maybe_contiguous(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (q, k, v)]\n\u001b[0;32m---> 86\u001b[0m out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state \u001b[38;5;241m=\u001b[39m \u001b[43mflash_attn_cuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvarlen_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_softmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# if out.isnan().any() or softmax_lse.isnan().any():\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#     breakpoint()\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "flow_device = next(flow_model_trained.parameters()).device\n",
        "flow_dtype = next(flow_model_trained.parameters()).dtype\n",
        "sensor_dtype = next(trained_model.sensor_encoder.parameters()).dtype if trained_model.sensor_encoder else flow_dtype\n",
        "robot_dtype = next(trained_model.robot_state_encoder.parameters()).dtype if trained_model.robot_state_encoder else flow_dtype\n",
        "\n",
        "max_steps = len(episode_dataset) if MAX_STEPS is None else min(len(episode_dataset), MAX_STEPS)\n",
        "\n",
        "captured_attn = []\n",
        "vision_ratio_series = []\n",
        "vision_lengths = []\n",
        "episode_indices = []\n",
        "timestamps = []\n",
        "\n",
        "for step in tqdm(range(max_steps), desc=\"Processing episode\"):\n",
        "    sample = episode_dataset[step]\n",
        "    vl_cache = sample.get(\"vl_cache\")\n",
        "    use_cached_tokens = (\n",
        "        isinstance(vl_cache, (tuple, list))\n",
        "        and len(vl_cache) == 2\n",
        "        and isinstance(vl_cache[0], torch.Tensor)\n",
        "        and vl_cache[0].shape[1] > 0\n",
        "    )\n",
        "\n",
        "    if use_cached_tokens:\n",
        "        vision_tokens, guidance_vec = vl_cache\n",
        "        context_features = vision_tokens.to(device=flow_device, dtype=flow_dtype, non_blocking=True)\n",
        "        guidance_vector = guidance_vec.to(device=flow_device, dtype=flow_dtype, non_blocking=True)\n",
        "    else:\n",
        "        image_paths = sample.get(\"images\", [\"View1\",\"View2\",\"View3\",\"View4\",\"View5\" ])\n",
        "        instruction = sample.get(\"instruction\", \"\")\n",
        "        cache_key = sample.get(\"cache_key\", f\"step_{step}\")\n",
        "        if not image_paths:\n",
        "            print(f\"[Step {step}] No images available for live VL encoding, skipping.\")\n",
        "            continue\n",
        "        with torch.no_grad():\n",
        "            img_tokens, txt_tokens = trained_model.vl_encoder.encode(\n",
        "                [instruction], [image_paths], [cache_key], use_cache=False\n",
        "            )\n",
        "        if img_tokens.shape[1] == 0:\n",
        "            print(f\"[Step {step}] Live VL encoder returned 0 vision tokens, skipping.\")\n",
        "            continue\n",
        "        context_features = img_tokens.to(device=flow_device, dtype=flow_dtype, non_blocking=True)\n",
        "        guidance_vector = txt_tokens.to(device=flow_device, dtype=flow_dtype, non_blocking=True)\n",
        "\n",
        "    sensor_data = sample[\"sensor_data\"].unsqueeze(0).to(device=flow_device, dtype=sensor_dtype, non_blocking=True)\n",
        "    robot_states = sample[\"robot_states\"].unsqueeze(0).to(device=flow_device, dtype=robot_dtype, non_blocking=True)\n",
        "    actions = sample[\"actions\"].unsqueeze(0).to(device=flow_device, dtype=flow_dtype, non_blocking=True)\n",
        "\n",
        "    sensor_chunks = []\n",
        "    if trained_model.sensor_encoder is not None:\n",
        "        sensor_chunks.append(trained_model.sensor_encoder(sensor_data))\n",
        "    if trained_model.robot_state_encoder is not None:\n",
        "        sensor_chunks.append(trained_model.robot_state_encoder(robot_states))\n",
        "    if not sensor_chunks:\n",
        "        print(f\"[Step {step}] Sensor/robot features missing, skipping.\")\n",
        "        continue\n",
        "    sensor_features = torch.cat(sensor_chunks, dim=-1).to(dtype=flow_dtype)\n",
        "\n",
        "    actions_n = actions / flow_model_trained.action_scale.to(device=flow_device, dtype=flow_dtype)\n",
        "    x_t, _, t_scalar = flow_model_trained.flow.compute_flow_and_target(actions_n)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = flow_model_trained(\n",
        "            x_t,\n",
        "            t_scalar,\n",
        "            context_features,\n",
        "            guidance_vector,\n",
        "            sensor_features=sensor_features,\n",
        "        )\n",
        "        attn = flow_model_trained.mod_layers[-1].last_cross_attn_weights\n",
        "    if attn is None:\n",
        "        raise RuntimeError(\"Cross-attention weights were not captured.\")\n",
        "    attn_mean = attn.detach().mean(dim=1).cpu()[0]\n",
        "    if captured_attn and attn_mean.shape[0] != captured_attn[0].shape[0]:\n",
        "        print(f\"[Step {step}] Horizon mismatch, skipping.\")\n",
        "        continue\n",
        "\n",
        "    Sv = context_features.shape[1]\n",
        "    vision_lengths.append(Sv)\n",
        "    captured_attn.append(attn_mean)\n",
        "    episode_indices.append(step)\n",
        "    timestamps.append(sample.get(\"timestamp\", float(step)))\n",
        "\n",
        "    vision_importance = attn_mean[:, :Sv].sum(dim=-1)\n",
        "    sensor_importance = attn_mean[:, Sv:].sum(dim=-1)\n",
        "    ratio = vision_importance / (vision_importance + sensor_importance + 1e-8)\n",
        "    vision_ratio_series.append(ratio.cpu())\n",
        "\n",
        "if not captured_attn:\n",
        "    raise RuntimeError(\"No valid steps processed. Check cache/checkpoint paths and VL encoding.\")\n",
        "print(f\"Processed {len(captured_attn)} / {max_steps} steps with valid attention.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "stack expects a non-empty TensorList",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ratio_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_ratio_series\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m time_axis \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(timestamps, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_axis\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
          ]
        }
      ],
      "source": [
        "ratio_matrix = torch.stack(vision_ratio_series, dim=1)\n",
        "time_axis = torch.tensor(timestamps, dtype=torch.float64)\n",
        "if time_axis.numel() > 0:\n",
        "    time_axis = time_axis - time_axis[0]\n",
        "\n",
        "processed = ratio_matrix.shape[1]\n",
        "target_pos = processed // 2\n",
        "if TARGET_STEP_IDX is not None and TARGET_STEP_IDX in episode_indices:\n",
        "    target_pos = episode_indices.index(TARGET_STEP_IDX)\n",
        "\n",
        "selected_heatmap = captured_attn[target_pos]\n",
        "Sv = vision_lengths[target_pos]\n",
        "Ss = selected_heatmap.shape[1] - Sv\n",
        "selected_ratio = ratio_matrix[:, target_pos]\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8), constrained_layout=True)\n",
        "im = axes[0].imshow(selected_heatmap.numpy(), aspect=\"auto\", cmap=\"viridis\")\n",
        "axes[0].axvline(x=Sv - 0.5, color=\"red\", linestyle=\"--\", label=\"Vision/Sensor boundary\")\n",
        "axes[0].set_title(f\"Step {episode_indices[target_pos]} cross-attention heatmap\")\n",
        "axes[0].set_ylabel(\"Action timestep\")\n",
        "axes[0].set_xlabel(\"Token index\")\n",
        "axes[0].legend(loc=\"upper right\")\n",
        "fig.colorbar(im, ax=axes[0], fraction=0.025)\n",
        "\n",
        "axes[1].plot(selected_ratio.numpy(), label=\"Vision ratio\", color=\"tab:blue\")\n",
        "axes[1].plot((1 - selected_ratio).numpy(), label=\"Sensor ratio\", color=\"tab:orange\")\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].set_title(\"Vision vs Sensor contribution for selected step\")\n",
        "axes[1].set_xlabel(\"Action timestep\")\n",
        "axes[1].set_ylabel(\"Importance ratio\")\n",
        "axes[1].legend()\n",
        "plt.show()\n",
        "\n",
        "x_ticks = np.linspace(0, processed - 1, min(6, processed)).astype(int)\n",
        "fig2, axes2 = plt.subplots(2, 1, figsize=(14, 8), constrained_layout=True)\n",
        "im2 = axes2[0].imshow(ratio_matrix.numpy(), aspect=\"auto\", cmap=\"magma\", vmin=0, vmax=1)\n",
        "axes2[0].set_title(\"Vision ratio across the episode (per action step)\")\n",
        "axes2[0].set_ylabel(\"Action timestep\")\n",
        "axes2[0].set_xlabel(\"Processed sample index\")\n",
        "axes2[0].set_xticks(x_ticks)\n",
        "axes2[0].set_xticklabels([episode_indices[i] for i in x_ticks])\n",
        "fig2.colorbar(im2, ax=axes2[0], fraction=0.025, pad=0.02)\n",
        "\n",
        "mean_ratio = ratio_matrix.mean(dim=0).numpy()\n",
        "axes2[1].plot(episode_indices, mean_ratio, label=\"Vision ratio\", color=\"tab:blue\")\n",
        "axes2[1].plot(episode_indices, 1 - mean_ratio, label=\"Sensor ratio\", color=\"tab:orange\")\n",
        "axes2[1].set_ylim(0, 1)\n",
        "axes2[1].set_xlabel(\"Episode step index\")\n",
        "axes2[1].set_ylabel(\"Mean importance ratio\")\n",
        "axes2[1].legend()\n",
        "\n",
        "if time_axis.numel() >= len(episode_indices):\n",
        "    tick_frac = np.linspace(0, len(episode_indices) - 1, min(6, len(episode_indices))).astype(int)\n",
        "    ax_time = axes2[1].twiny()\n",
        "    ax_time.set_xlim(axes2[1].get_xlim())\n",
        "    ax_time.set_xticks([episode_indices[i] for i in tick_frac])\n",
        "    ax_time.set_xticklabels([f\"{time_axis[i].item():.2f}s\" for i in tick_frac])\n",
        "    ax_time.set_xlabel(\"Elapsed time (s)\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490041d7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "qwen_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
