"""
VLA Encoder Modules for Robot State and Sensor Data.

This file defines various encoder modules used within the unified VLA model
to process robot proprioceptive states and diverse sensor data.

Components:
- Helper Functions/Classes:
    - `ResidualDownsample1d`: 1D Residual Downsampling block for time-series data.
    - `force_bn_fp32_`: Utility to cast BatchNorm layers to float32.
- Encoder Modules:
    - `RobotStateEncoder`: Processes sequences of robot joint angles and end-effector poses.
    - `SensorEncoder`: A temporal ConvFormer for general time-series sensor data.
    - `ForceAwareSensorEncoder`: Specializes in processing distance and force features separately.
"""

from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

# ==============================================================================
# 1. í—¬í¼ í•¨ìˆ˜ ë° í´ë˜ìŠ¤ (Helper Functions & Classes)
# ==============================================================================

class ResidualDownsample1d(nn.Module):
    """
    1D Residual Downsampling ë¸”ë¡. ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    BatchNorm ë ˆì´ì–´ëŠ” í•­ìƒ FP32ë¡œ ê³ ì •ë©ë‹ˆë‹¤.
    """
    def __init__(self, in_ch, out_ch, stride=2, dropout=0.1):
        super().__init__()

        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1   = nn.BatchNorm1d(out_ch)
        self.act1  = nn.GELU()
        self.do1   = nn.Dropout(dropout)

        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2   = nn.BatchNorm1d(out_ch)
        self.act2  = nn.GELU()

        self.skip  = nn.Identity() if (in_ch == out_ch and stride == 1) else \
                     nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False)

        # BatchNormì„ í•­ìƒ FP32ë¡œ ê³ ì •
        self.bn1.float()
        self.bn2.float()

    def forward(self, x):
        y = self.conv1(x)
        y = self.bn1(y)
        y = self.act1(y)
        y = self.do1(y)

        y = self.conv2(y)
        y = self.bn2(y)
        y = self.act2(y)

        s = self.skip(x)
        return y + s


def force_bn_fp32_(module: torch.nn.Module):
    """
    ì£¼ì–´ì§„ ëª¨ë“ˆ ë‚´ì˜ ëª¨ë“  BatchNorm ë ˆì´ì–´ì˜ ë§¤ê°œë³€ìˆ˜/ë²„í¼ë¥¼ float32ë¡œ ìºìŠ¤íŒ…í•©ë‹ˆë‹¤.
    í˜¼í•© ì •ë°€ë„(Mixed Precision) í›ˆë ¨ ì‹œ ì£¼ì˜ ì‚¬í•­.
    """
    for m in module.modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            m.float()  # ê°€ì¤‘ì¹˜/í¸í–¥ ë° ëŸ¬ë‹ í†µê³„ ëª¨ë‘ FP32ë¡œ

# ==============================================================================
# 2. ì¸ì½”ë” ëª¨ë“ˆ (Encoder Modules)
#    - ë¡œë´‡ ìƒíƒœ ì¸ì½”ë” (RobotStateEncoder)
#    - ì„¼ì„œ ì¸ì½”ë” (SensorEncoder) ë° Force-Aware ì„¼ì„œ ì¸ì½”ë”
# ==============================================================================

class RobotStateEncoder(nn.Module):
    """
    ë¡œë´‡ì˜ ê´€ì ˆ ê°ë„(joint angles)ì™€ ì—”ë“œ ì´í™í„° í¬ì¦ˆ(end-effector pose) ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ”
    Transformer ê¸°ë°˜ ì¸ì½”ë”ì…ë‹ˆë‹¤. Temporal Poolingê³¼ Projectionì„ í†µí•´ ê³ ì •ëœ í¬ê¸°ì˜
    ì¶œë ¥ íŠ¹ì§• ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def __init__(self,
                 input_dim: int = 12, # 6 ê´€ì ˆ ê°ë„ + 6 ì—”ë“œ ì´í™í„° í¬ì¦ˆ (x, y, z, roll, pitch, yaw)
                 model_dim: int = 256,
                 output_dim: int = 2048,
                 num_heads: int = 8,
                 num_layers: int = 4,
                 temporal_length: int = 60,
                 dropout: float = 0.1):
        super().__init__()
        self.input_dim = input_dim
        self.model_dim = model_dim
        self.output_dim = output_dim

        # 1. ì…ë ¥ íˆ¬ì˜ (Input Projection): ì›ì‹œ ë¡œë´‡ ìƒíƒœë¥¼ ëª¨ë¸ ì°¨ì›ìœ¼ë¡œ ë§¤í•‘
        self.input_proj = nn.Linear(input_dim, model_dim)

        # 2. ìœ„ì¹˜ ì¸ì½”ë”© (Positional Encoding): ì‹œê³„ì—´ ë°ì´í„°ì˜ ì‹œê°„ì  ìˆœì„œ ì •ë³´ ì œê³µ
        self.pos_encoder = nn.Parameter(torch.zeros(1, temporal_length, model_dim))

        # 3. Transformer ì¸ì½”ë”: ì‹œê°„ì  ì˜ì¡´ì„±ì„ í•™ìŠµ
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim,
            nhead=num_heads,
            dim_feedforward=model_dim * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True, # (Batch, Sequence, Feature) í˜•íƒœë¡œ ì…ë ¥ ì²˜ë¦¬
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # 4. Temporal Pooling ë° Projection Head: ì‹œê³„ì—´ íŠ¹ì§•ì„ ê³ ì • ê¸¸ì´ ë²¡í„°ë¡œ ì••ì¶•í•˜ê³  ìµœì¢… ì¶œë ¥ ì°¨ì›ìœ¼ë¡œ ë§¤í•‘
        self.temporal_pool = nn.AdaptiveAvgPool1d(1) # ì‹œê°„ ì¶• í‰ê·  í’€ë§
        self.projection = nn.Sequential(
            nn.Linear(model_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(output_dim, output_dim),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, src: torch.Tensor, return_sequence: bool = False) -> torch.Tensor:
        """
        ë¡œë´‡ ìƒíƒœ ì‹œí€€ìŠ¤ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.

        Args:
            src (torch.Tensor): ë¡œë´‡ ìƒíƒœ ì‹œí€€ìŠ¤, (B, T, D_in) í˜•íƒœ.
            return_sequence (bool): Trueì´ë©´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì „ì²´ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•˜ê³ ,
                                    False (ê¸°ë³¸ê°’)ì´ë©´ í’€ë§ ë° íˆ¬ì˜ëœ íŠ¹ì§• ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Returns:
            torch.Tensor: ì¸ì½”ë”©ëœ íŠ¹ì§•. 
                          return_sequenceê°€ Falseì´ë©´ (B, D_out) í˜•íƒœ,
                          Trueì´ë©´ (B, T, model_dim) í˜•íƒœì…ë‹ˆë‹¤.
        """
        # ì…ë ¥ íˆ¬ì˜ ë° ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        x = self.input_proj(src) # (B, T, model_dim)
        x = x + self.pos_encoder # ìœ„ì¹˜ ì¸ì½”ë”© ë”í•˜ê¸°
        x = self.dropout(x)

        # íŠ¸ëœìŠ¤í¬ë¨¸ í†µê³¼
        x = self.transformer_encoder(x) # (B, T, model_dim)

        # MAE ì‚¬ì „ í›ˆë ¨ê³¼ ê°™ì´ ì‹œí€€ìŠ¤ ìì²´ë¥¼ ë°˜í™˜í•´ì•¼ í•˜ëŠ” ê²½ìš°
        if return_sequence:
            return x

        # ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì„ ìœ„í•œ í’€ë§ ë° íˆ¬ì˜
        pooled_x = x.transpose(1, 2) # (B, model_dim, T) í˜•íƒœë¡œ ë³€ê²½í•˜ì—¬ 1D í’€ë§ ì¤€ë¹„
        pooled_x = self.temporal_pool(pooled_x).squeeze(-1) # (B, model_dim)
        output_features = self.projection(pooled_x) # (B, output_dim)

        return output_features

class SensorEncoder(nn.Module):
    """
    í–¥ìƒëœ ì„¼ì„œ ì¸ì½”ë” (Temporal ConvFormer).
    1D Convolutional ë°±ë³¸ê³¼ Transformer ì¸ì½”ë”ë¥¼ ê²°í•©í•˜ì—¬ ì‹œê³„ì—´ ì„¼ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        input_channels (int, optional): ì…ë ¥ ì„¼ì„œ ë°ì´í„°ì˜ ì±„ë„ ìˆ˜. Defaults to 1026.
        temporal_length (int, optional): ì…ë ¥ ì‹œê³„ì—´ ë°ì´í„°ì˜ ì˜ˆìƒ ê¸¸ì´. Defaults to 650.
        hidden_dim (int, optional): ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ì´ˆê¸° ì€ë‹‰ ì°¨ì›. Defaults to 512.
        output_dim (int, optional): ìµœì¢… ì¶œë ¥ íŠ¹ì§• ë²¡í„°ì˜ ì°¨ì›. Defaults to 3072.
        num_conv_layers (int, optional): ì»¨ë³¼ë£¨ì…˜ ë°±ë³¸ì˜ ë ˆì´ì–´ ìˆ˜. Defaults to 4.
        use_transformer (bool, optional): ì»¨ë³¼ë£¨ì…˜ í›„ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€. Defaults to True.
        num_transformer_layers (int, optional): íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë ˆì´ì–´ ìˆ˜. Defaults to 2.
        nhead (int, optional): íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì–´í…ì…˜ í—¤ë“œ ìˆ˜. Defaults to 8.
        dropout (float, optional): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨. Defaults to 0.1.
        gradient_checkpointing (bool, optional): ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™” ì—¬ë¶€. Defaults to False.
        interpolation_mode (str, optional): ì‹œê³„ì—´ ê¸¸ì´ ë¶ˆì¼ì¹˜ ì‹œ ë³´ê°„ ëª¨ë“œ ('linear', 'cubic', 'nearest'). Defaults to 'linear'.
    """
    def __init__(self,
                 input_channels=1026,
                 temporal_length=650,
                 hidden_dim=512,
                 output_dim=3072,
                 num_conv_layers=4,
                 use_transformer=True,
                 num_transformer_layers=2,
                 nhead=8,
                 dropout=0.1,
                 gradient_checkpointing=False,
                 interpolation_mode='linear'):
        super().__init__()
        self.input_channels = input_channels
        self.temporal_length = temporal_length
        self.output_dim = output_dim
        self.gradient_checkpointing = gradient_checkpointing
        self.interpolation_mode = interpolation_mode

        # ì”ì°¨ ë‹¤ìš´ìƒ˜í”Œ ë¸”ë¡ ìŠ¤íƒ (Residual Downsample Block Stack)
        # ì„¼ì„œ ë°ì´í„°ì˜ ì±„ë„ì„ í™•ì¥í•˜ê³  ì‹œê°„ ì°¨ì›ì„ ì ì§„ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
        chs = [input_channels]
        conv_blocks = []
        for i in range(num_conv_layers):
            out_ch = hidden_dim if i == 0 else hidden_dim * (2 ** i) # ì±„ë„ ì¦ê°€ëŠ” ì˜ˆì „ ì½”ë“œ ë°©ì‹ ìœ ì§€
            conv_blocks.append(ResidualDownsample1d(
                in_ch=chs[-1], out_ch=out_ch, stride=2, dropout=dropout
            ))
            chs.append(out_ch)
        self.conv_backbone = nn.ModuleList(conv_blocks)
        self.final_channels = chs[-1] # ìµœì¢… ì»¨ë³¼ë£¨ì…˜ ì¶œë ¥ ì±„ë„

        # ìµœì¢… ì‹œê°„ ê¸¸ì´ (ëŒ€ëµ ì ˆë°˜ì”© ì¤„ì–´ë“¦, ì •í™•í•œ ê³„ì‚°ì€ ì•„ë‹˜)
        # ì‹¤ì œ ê¸¸ì´ëŠ” F.interpolate í›„ ì»¨ë³¼ë£¨ì…˜ ì—°ì‚°ìœ¼ë¡œ ì¸í•´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ.
        self.final_temporal_length = temporal_length // (2 ** num_conv_layers) # ê·¼ì‚¬ê°’

        self.use_transformer = use_transformer
        if use_transformer:
            # ì»¨ë³¼ë£¨ì…˜ ë°±ë³¸ì˜ ì¶œë ¥ íŠ¹ì§•ì— ëŒ€í•´ Transformerë¥¼ ì ìš©í•˜ì—¬ ì¥ê±°ë¦¬ ì‹œê°„ì  ì˜ì¡´ì„± í•™ìŠµ
            enc_layer = nn.TransformerEncoderLayer(
                d_model=self.final_channels, nhead=nhead,
                dim_feedforward=self.final_channels * 4,
                dropout=dropout,
                batch_first=True,
                norm_first=True
            )
            self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_transformer_layers)

        # Temporal Pooling ë° Projection: ì²˜ë¦¬ëœ ì‹œê³„ì—´ íŠ¹ì§•ì„ ê³ ì • ê¸¸ì´ ë²¡í„°ë¡œ ë³€í™˜
        self.temporal_pool = nn.AdaptiveAvgPool1d(1) # ì‹œê°„ ì¶• í‰ê·  í’€ë§
        self.projection = nn.Sequential(
            nn.Linear(self.final_channels, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(output_dim, output_dim),
        )

    def forward(self, sensor_data: torch.Tensor) -> torch.Tensor:
        """
        ì„¼ì„œ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•˜ì—¬ íŠ¹ì§• ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            sensor_data (torch.Tensor): ì…ë ¥ ì„¼ì„œ ë°ì´í„°, (B, T, C) í˜•íƒœ.

        Returns:
            torch.Tensor: ì¸ì½”ë”©ëœ ì„¼ì„œ íŠ¹ì§•, (B, output_dim) í˜•íƒœ.
        """
        B, T, C = sensor_data.shape
        if C != self.input_channels:
            raise ValueError(f"ì˜ˆìƒë˜ëŠ” ì±„ë„ ìˆ˜ {self.input_channels}ì™€ ë‹¤ë¦…ë‹ˆë‹¤. í˜„ì¬: {C}")

        # ë¹„ë™ê¸° ê¸¸ì´ ë³´ì • (interpolation)
        # ì…ë ¥ ì‹œê³„ì—´ ê¸¸ì´ê°€ ëª¨ë¸ì´ ì˜ˆìƒí•˜ëŠ” ê¸¸ì´ì™€ ë‹¤ë¥´ë©´ ë³´ê°„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        if T != self.temporal_length:
            x = sensor_data.transpose(1, 2)  # (B, C, T) í˜•íƒœë¡œ ë³€ê²½
            # ì¸í„°í´ë ˆì´ì…˜ ëª¨ë“œ ì„ íƒ: ë„ˆë¬´ ì§§ì€ ì‹œí€€ìŠ¤ì—ëŠ” 'cubic' ëŒ€ì‹  'linear' ì‚¬ìš©
            mode = 'linear' if self.interpolation_mode == 'cubic' and T < 4 else self.interpolation_mode
            x = F.interpolate(x, size=self.temporal_length, mode=mode,
                              align_corners=False if mode in ('linear', 'cubic') else None)
        else:
            x = sensor_data.transpose(1, 2)  # (B, C, T) í˜•íƒœë¡œ ë³€ê²½

        # Conv ë°±ë³¸ í†µê³¼
        for block in self.conv_backbone:
            if self.gradient_checkpointing and self.training:
                # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
                x = torch.utils.checkpoint.checkpoint(block, x, use_reentrant=False)
            else:
                x = block(x)   # (B, ch, T')

        # Transformer í†µê³¼ (ì‚¬ìš© í™œì„±í™” ì‹œ)
        if self.use_transformer:
            x = x.transpose(1, 2)  # (B, T', ch) í˜•íƒœë¡œ ë³€ê²½í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ì— ì…ë ¥
            if self.gradient_checkpointing and self.training:
                x = torch.utils.checkpoint.checkpoint(self.transformer, x, use_reentrant=False)
            else:
                x = self.transformer(x)
            x = x.transpose(1, 2)  # ë‹¤ì‹œ (B, ch, T') í˜•íƒœë¡œ ë³€ê²½

        # ì‹œê³„ì—´ í’€ë§ ë° ìµœì¢… íˆ¬ì˜
        x = self.temporal_pool(x).squeeze(-1)        # (B, ch)
        sensor_features = self.projection(x)         # (B, output_dim)
        return sensor_features

class ForceAwareSensorEncoder(nn.Module):
    """
    'ê±°ë¦¬' (distance)ì™€ 'í˜' (force) íŠ¹ì§•ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬
    í˜ ë°ì´í„°ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì„¼ì„œ ì¸ì½”ë”ì…ë‹ˆë‹¤.

    ì•„í‚¤í…ì²˜:
    1.  ì£¼ìš” 'ê±°ë¦¬' íŠ¹ì§•(`dist_channels`)ì€ í‘œì¤€ `SensorEncoder`ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    2.  'í˜' íŠ¹ì§•(`force_channels`)ì€ ì „ìš© MLPë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    3.  ë‘ ì¸ì½”ë”ì˜ ì¶œë ¥ì´ ê²°í•©(concat)ë˜ì–´ ìµœì¢… ì¶œë ¥ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜ë©ë‹ˆë‹¤.

    Args:
        dist_channels (int, optional): ê±°ë¦¬ ì„¼ì„œ ì±„ë„ ìˆ˜. Defaults to 1025.
        force_channels (int, optional): í˜ ì„¼ì„œ ì±„ë„ ìˆ˜. Defaults to 1.
        temporal_length (int, optional): ì‹œê³„ì—´ ê¸¸ì´. Defaults to 65.
        dist_hidden_dim (int, optional): ê±°ë¦¬ ì¸ì½”ë”ì˜ ì€ë‹‰ ì°¨ì›. Defaults to 512.
        force_hidden_dim (int, optional): í˜ ì¸ì½”ë”ì˜ ì€ë‹‰ ì°¨ì›. Defaults to 128.
        output_dim (int, optional): ìµœì¢… ì¶œë ¥ íŠ¹ì§• ë²¡í„°ì˜ ì°¨ì›. Defaults to 3072.
        **kwargs: `SensorEncoder`ë¡œ ì „ë‹¬ë  ì¶”ê°€ ì¸ìë“¤.
    """
    def __init__(self,
                 dist_channels=1025,
                 force_channels=1,
                 temporal_length=65,
                 dist_hidden_dim=512,
                 force_hidden_dim=128,
                 output_dim=3072,
                 **kwargs):
        super().__init__()
        self.input_channels = dist_channels + force_channels
        self.force_channels = force_channels # forwardì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥

        print(f"ğŸš€ ForceAwareSensorEncoder ì´ˆê¸°í™” ì¤‘:")
        print(f"   - ê±°ë¦¬ íŠ¹ì§• (1-{dist_channels})ì€ ConvFormerë¡œ ì²˜ë¦¬.")
        print(f"   - í˜ íŠ¹ì§• ({dist_channels+1})ì€ ì „ìš© MLPë¡œ ì²˜ë¦¬.")

        # ì£¼ìš” 'ê±°ë¦¬' íŠ¹ì§•ì„ ìœ„í•œ ì¸ì½”ë”
        self.dist_encoder = SensorEncoder(
            input_channels=dist_channels,
            temporal_length=temporal_length,
            hidden_dim=dist_hidden_dim,
            output_dim=output_dim - force_hidden_dim, # ì¶œë ¥ ê³µê°„ì˜ ì¼ë¶€ë¥¼ í• ë‹¹
            **kwargs
        )
        # `dist_encoder` ë‚´ì˜ BatchNorm ë ˆì´ì–´ë¥¼ float32ë¡œ ê°•ì œ
        force_bn_fp32_(self.dist_encoder)

        # 'í˜' íŠ¹ì§•ì„ ìœ„í•œ ì‘ê³  ì „ìš© MLP
        self.force_encoder = nn.Sequential(
            nn.Linear(force_channels, force_hidden_dim // 2),
            nn.GELU(),
            nn.LayerNorm(force_hidden_dim // 2),
            nn.Linear(force_hidden_dim // 2, force_hidden_dim)
        )
        self.force_pool = nn.AdaptiveAvgPool1d(1) # ì‹œê°„ ì¶• í’€ë§

        print(f"   - ìµœì¢… ì¶œë ¥: {output_dim} ì°¨ì›ìœ¼ë¡œ ê²°í•© ë° íˆ¬ì˜.")


    def forward(self, sensor_data: torch.Tensor) -> torch.Tensor:
        """
        Force-Aware ë°©ì‹ìœ¼ë¡œ ì„¼ì„œ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.

        Args:
            sensor_data (torch.Tensor): ì…ë ¥ ì„¼ì„œ ë°ì´í„°, (B, T, C) í˜•íƒœ (ì—¬ê¸°ì„œ CëŠ” self.input_channels).

        Returns:
            torch.Tensor: ì¸ì½”ë”©ëœ ê²°í•© íŠ¹ì§•, (B, output_dim) í˜•íƒœ.
        """
        B, T, C = sensor_data.shape
        if C != self.input_channels:
            raise ValueError(f"ì˜ˆìƒë˜ëŠ” ì±„ë„ ìˆ˜ {self.input_channels}ì™€ ë‹¤ë¦…ë‹ˆë‹¤. í˜„ì¬: {C}")

        # ë°ì´í„°ë¥¼ ê±°ë¦¬ì™€ í˜ìœ¼ë¡œ ë¶„ë¦¬
        dist_data = sensor_data[..., :-self.force_channels]  # (B, T, dist_channels)
        force_data = sensor_data[..., -self.force_channels:] # (B, T, force_channels)

        # 1. ê±°ë¦¬ ë°ì´í„° ì²˜ë¦¬
        dist_features = self.dist_encoder(dist_data) # (B, output_dim - force_hidden_dim)

        # 2. í˜ ë°ì´í„° ì²˜ë¦¬ (MLP -> ì‹œê°„ í’€ë§)
        force_features_temporal = self.force_encoder(force_data) # (B, T, force_hidden_dim)
        force_features_pooled = self.force_pool(force_features_temporal.transpose(1, 2)).squeeze(-1) # (B, force_hidden_dim)

        # 3. ë‘ íŠ¹ì§• ê²°í•© ë° ë°˜í™˜
        combined_features = torch.cat([dist_features, force_features_pooled], dim=-1)

        return combined_features
