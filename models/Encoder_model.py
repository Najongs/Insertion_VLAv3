"""
VLA Encoder Modules for Robot State and Sensor Data.

This file defines various encoder modules used within the unified VLA model
to process robot proprioceptive states and diverse sensor data.

Components:
- Helper Functions/Classes:
    - `ResidualDownsample1d`: 1D Residual Downsampling block for time-series data.
    - `force_bn_fp32_`: Utility to cast BatchNorm layers to float32.
- Encoder Modules:
    - `RobotStateEncoder`: Processes sequences of robot joint angles and end-effector poses.
    - `SensorEncoder`: A temporal ConvFormer for general time-series sensor data.
    - `ForceAwareSensorEncoder`: Specializes in processing distance and force features separately.
"""

from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

# ==============================================================================
# 1. í—¬í¼ í•¨ìˆ˜ ë° í´ë˜ìŠ¤ (Helper Functions & Classes)
# ==============================================================================

class ResidualDownsample1d(nn.Module):
    """
    1D Residual Downsampling ë¸”ë¡. ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    BatchNorm ë ˆì´ì–´ëŠ” í•­ìƒ FP32ë¡œ ê³ ì •ë©ë‹ˆë‹¤.
    """
    def __init__(self, in_ch, out_ch, stride=2, dropout=0.1):
        super().__init__()

        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1   = nn.BatchNorm1d(out_ch)
        self.act1  = nn.GELU()
        self.do1   = nn.Dropout(dropout)

        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2   = nn.BatchNorm1d(out_ch)
        self.act2  = nn.GELU()

        self.skip  = nn.Identity() if (in_ch == out_ch and stride == 1) else \
                     nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False)

        # BatchNormì„ í•­ìƒ FP32ë¡œ ê³ ì •
        self.bn1.float()
        self.bn2.float()

    def forward(self, x):
        y = self.conv1(x)
        y = self.bn1(y)
        y = self.act1(y)
        y = self.do1(y)

        y = self.conv2(y)
        y = self.bn2(y)
        y = self.act2(y)

        s = self.skip(x)
        return y + s


def force_bn_fp32_(module: torch.nn.Module):
    """
    ì£¼ì–´ì§„ ëª¨ë“ˆ ë‚´ì˜ ëª¨ë“  BatchNorm ë ˆì´ì–´ì˜ ë§¤ê°œë³€ìˆ˜/ë²„í¼ë¥¼ float32ë¡œ ìºìŠ¤íŒ…í•©ë‹ˆë‹¤.
    í˜¼í•© ì •ë°€ë„(Mixed Precision) í›ˆë ¨ ì‹œ ì£¼ì˜ ì‚¬í•­.
    """
    for m in module.modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            m.float()  # ê°€ì¤‘ì¹˜/í¸í–¥ ë° ëŸ¬ë‹ í†µê³„ ëª¨ë‘ FP32ë¡œ

# ==============================================================================
# 2. ì¸ì½”ë” ëª¨ë“ˆ (Encoder Modules)
#    - ë¡œë´‡ ìƒíƒœ ì¸ì½”ë” (RobotStateEncoder)
#    - ì„¼ì„œ ì¸ì½”ë” (SensorEncoder) ë° Force-Aware ì„¼ì„œ ì¸ì½”ë”
# ==============================================================================

class RobotStateEncoder(nn.Module):
    """
    ë¡œë´‡ì˜ ê´€ì ˆ ê°ë„(joint angles)ì™€ ì—”ë“œ ì´í™í„° í¬ì¦ˆ(end-effector pose) ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ”
    Transformer ê¸°ë°˜ ì¸ì½”ë”ì…ë‹ˆë‹¤. Temporal Poolingê³¼ Projectionì„ í†µí•´ ê³ ì •ëœ í¬ê¸°ì˜
    ì¶œë ¥ íŠ¹ì§• ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def __init__(self,
                input_dim: int = 12, # 6 ê´€ì ˆ ê°ë„ + 6 ì—”ë“œ ì´í™í„° í¬ì¦ˆ (x, y, z, roll, pitch, yaw)
                model_dim: int = 256,
                output_dim: int = 512,
                num_heads: int = 8,
                num_layers: int = 4,
                temporal_length: int = 60,
                dropout: float = 0.1):
        super().__init__()
        self.input_dim = input_dim
        self.model_dim = model_dim
        self.output_dim = output_dim

        # 1. ì…ë ¥ íˆ¬ì˜ (Input Projection): ì›ì‹œ ë¡œë´‡ ìƒíƒœë¥¼ ëª¨ë¸ ì°¨ì›ìœ¼ë¡œ ë§¤í•‘
        self.input_proj = nn.Linear(input_dim, model_dim)

        # 2. ìœ„ì¹˜ ì¸ì½”ë”© (Positional Encoding): ì‹œê³„ì—´ ë°ì´í„°ì˜ ì‹œê°„ì  ìˆœì„œ ì •ë³´ ì œê³µ
        self.pos_encoder = nn.Parameter(torch.zeros(1, temporal_length, model_dim))

        # 3. Transformer ì¸ì½”ë”: ì‹œê°„ì  ì˜ì¡´ì„±ì„ í•™ìŠµ
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim,
            nhead=num_heads,
            dim_feedforward=model_dim * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True, # (Batch, Sequence, Feature) í˜•íƒœë¡œ ì…ë ¥ ì²˜ë¦¬
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # 4. Temporal Pooling ë° Projection Head: ì‹œê³„ì—´ íŠ¹ì§•ì„ ê³ ì • ê¸¸ì´ ë²¡í„°ë¡œ ì••ì¶•í•˜ê³  ìµœì¢… ì¶œë ¥ ì°¨ì›ìœ¼ë¡œ ë§¤í•‘
        self.temporal_pool = nn.AdaptiveAvgPool1d(1) # ì‹œê°„ ì¶• í‰ê·  í’€ë§
        self.projection = nn.Sequential(
            nn.Linear(model_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(output_dim, output_dim),
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, src: torch.Tensor, return_sequence: bool = False) -> torch.Tensor:
        """
        ë¡œë´‡ ìƒíƒœ ì‹œí€€ìŠ¤ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.

        Args:
            src (torch.Tensor): ë¡œë´‡ ìƒíƒœ ì‹œí€€ìŠ¤, (B, T, D_in) í˜•íƒœ.
            return_sequence (bool): Trueì´ë©´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì „ì²´ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•˜ê³ ,
                                    False (ê¸°ë³¸ê°’)ì´ë©´ í’€ë§ ë° íˆ¬ì˜ëœ íŠ¹ì§• ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Returns:
            torch.Tensor: ì¸ì½”ë”©ëœ íŠ¹ì§•. 
                          return_sequenceê°€ Falseì´ë©´ (B, D_out) í˜•íƒœ,
                          Trueì´ë©´ (B, T, model_dim) í˜•íƒœì…ë‹ˆë‹¤.
        """
        # ì…ë ¥ íˆ¬ì˜ ë° ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        x = self.input_proj(src) # (B, T, model_dim)
        x = x + self.pos_encoder # ìœ„ì¹˜ ì¸ì½”ë”© ë”í•˜ê¸°
        x = self.dropout(x)

        # íŠ¸ëœìŠ¤í¬ë¨¸ í†µê³¼
        x = self.transformer_encoder(x) # (B, T, model_dim)

        # MAE ì‚¬ì „ í›ˆë ¨ê³¼ ê°™ì´ ì‹œí€€ìŠ¤ ìì²´ë¥¼ ë°˜í™˜í•´ì•¼ í•˜ëŠ” ê²½ìš°
        if return_sequence:
            return x

        # ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì„ ìœ„í•œ í’€ë§ ë° íˆ¬ì˜
        pooled_x = x.transpose(1, 2) # (B, model_dim, T) í˜•íƒœë¡œ ë³€ê²½í•˜ì—¬ 1D í’€ë§ ì¤€ë¹„
        pooled_x = self.temporal_pool(pooled_x).squeeze(-1) # (B, model_dim)
        output_features = self.projection(pooled_x) # (B, output_dim)

        return output_features

class SensorEncoder(nn.Module):
    """
    í–¥ìƒëœ ì„¼ì„œ ì¸ì½”ë” (Temporal ConvFormer).
    1D Convolutional ë°±ë³¸ê³¼ Transformer ì¸ì½”ë”ë¥¼ ê²°í•©í•˜ì—¬ ì‹œê³„ì—´ ì„¼ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        input_channels (int, optional): ì…ë ¥ ì„¼ì„œ ë°ì´í„°ì˜ ì±„ë„ ìˆ˜. Defaults to 1026.
        temporal_length (int, optional): ì…ë ¥ ì‹œê³„ì—´ ë°ì´í„°ì˜ ì˜ˆìƒ ê¸¸ì´. Defaults to 650.
        hidden_dim (int, optional): ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ì´ˆê¸° ì€ë‹‰ ì°¨ì›. Defaults to 512.
        output_dim (int, optional): ìµœì¢… ì¶œë ¥ íŠ¹ì§• ë²¡í„°ì˜ ì°¨ì›. Defaults to 3072.
        num_conv_layers (int, optional): ì»¨ë³¼ë£¨ì…˜ ë°±ë³¸ì˜ ë ˆì´ì–´ ìˆ˜. Defaults to 4.
        use_transformer (bool, optional): ì»¨ë³¼ë£¨ì…˜ í›„ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€. Defaults to True.
        num_transformer_layers (int, optional): íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë ˆì´ì–´ ìˆ˜. Defaults to 2.
        nhead (int, optional): íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì–´í…ì…˜ í—¤ë“œ ìˆ˜. Defaults to 8.
        dropout (float, optional): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨. Defaults to 0.1.
        gradient_checkpointing (bool, optional): ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™” ì—¬ë¶€. Defaults to False.
        interpolation_mode (str, optional): ì‹œê³„ì—´ ê¸¸ì´ ë¶ˆì¼ì¹˜ ì‹œ ë³´ê°„ ëª¨ë“œ ('linear', 'cubic', 'nearest'). Defaults to 'linear'.
    """
    def __init__(self,
                input_channels=1026,
                temporal_length=650,
                hidden_dim=512,
                output_dim=1024,
                num_conv_layers=4,
                use_transformer=True,
                num_transformer_layers=2,
                nhead=8,
                dropout=0.1,
                gradient_checkpointing=False,
                interpolation_mode='linear',
                transformer_dim=None):  # ìƒˆ íŒŒë¼ë¯¸í„°: Transformer ì°¨ì› (Noneì´ë©´ final_channels ì‚¬ìš©)
        super().__init__()
        self.input_channels = input_channels
        self.temporal_length = temporal_length
        self.output_dim = output_dim
        self.gradient_checkpointing = gradient_checkpointing
        self.interpolation_mode = interpolation_mode
        self.transformer_dim = transformer_dim

        # ì”ì°¨ ë‹¤ìš´ìƒ˜í”Œ ë¸”ë¡ ìŠ¤íƒ (Residual Downsample Block Stack)
        # ì„¼ì„œ ë°ì´í„°ì˜ ì±„ë„ì„ í™•ì¥í•˜ê³  ì‹œê°„ ì°¨ì›ì„ ì ì§„ì ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
        chs = [input_channels]
        conv_blocks = []
        for i in range(num_conv_layers):
            out_ch = hidden_dim if i == 0 else hidden_dim * (2 ** i) # ì±„ë„ ì¦ê°€ëŠ” ì˜ˆì „ ì½”ë“œ ë°©ì‹ ìœ ì§€
            conv_blocks.append(ResidualDownsample1d(
                in_ch=chs[-1], out_ch=out_ch, stride=2, dropout=dropout
            ))
            chs.append(out_ch)
        self.conv_backbone = nn.ModuleList(conv_blocks)
        self.final_channels = chs[-1] # ìµœì¢… ì»¨ë³¼ë£¨ì…˜ ì¶œë ¥ ì±„ë„

        # ìµœì¢… ì‹œê°„ ê¸¸ì´ (ëŒ€ëµ ì ˆë°˜ì”© ì¤„ì–´ë“¦, ì •í™•í•œ ê³„ì‚°ì€ ì•„ë‹˜)
        # ì‹¤ì œ ê¸¸ì´ëŠ” F.interpolate í›„ ì»¨ë³¼ë£¨ì…˜ ì—°ì‚°ìœ¼ë¡œ ì¸í•´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ.
        self.final_temporal_length = temporal_length // (2 ** num_conv_layers) # ê·¼ì‚¬ê°’

        self.use_transformer = use_transformer
        if use_transformer:
            # Transformer ì°¨ì› ê²°ì •: transformer_dimì´ ì§€ì •ë˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ final_channels ì‚¬ìš©
            if transformer_dim is not None:
                self.actual_transformer_dim = transformer_dim
                # Conv ì¶œë ¥ì„ Transformer ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ëŠ” projection layer ì¶”ê°€
                self.conv_to_transformer_proj = nn.Linear(self.final_channels, transformer_dim)
                print(f"   ğŸ’¡ Lightweight mode: Conv({self.final_channels}) â†’ Projection({transformer_dim}) â†’ Transformer")
            else:
                self.actual_transformer_dim = self.final_channels
                self.conv_to_transformer_proj = None

            # ì»¨ë³¼ë£¨ì…˜ ë°±ë³¸ì˜ ì¶œë ¥ íŠ¹ì§•ì— ëŒ€í•´ Transformerë¥¼ ì ìš©í•˜ì—¬ ì¥ê±°ë¦¬ ì‹œê°„ì  ì˜ì¡´ì„± í•™ìŠµ
            enc_layer = nn.TransformerEncoderLayer(
                d_model=self.actual_transformer_dim, nhead=nhead,
                dim_feedforward=self.actual_transformer_dim * 4,
                dropout=dropout,
                batch_first=True,
                norm_first=True
            )
            self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_transformer_layers)

        # Temporal Pooling ë° Projection: ì²˜ë¦¬ëœ ì‹œê³„ì—´ íŠ¹ì§•ì„ ê³ ì • ê¸¸ì´ ë²¡í„°ë¡œ ë³€í™˜
        self.temporal_pool = nn.AdaptiveAvgPool1d(1) # ì‹œê°„ ì¶• í‰ê·  í’€ë§
        # Projection headëŠ” Transformerë¥¼ ì‚¬ìš©í•˜ë©´ actual_transformer_dimì—ì„œ, ì•„ë‹ˆë©´ final_channelsì—ì„œ ì‹œì‘
        proj_input_dim = self.actual_transformer_dim if use_transformer else self.final_channels
        self.projection = nn.Sequential(
            nn.Linear(proj_input_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(output_dim, output_dim),
        )

    def forward(self, sensor_data: torch.Tensor, return_sequence: bool = False):
        """
        Args:
            sensor_data: (B, T, C)
            return_sequence:
                - False(default): (B, output_dim)  # ì „ì—­ íŠ¹ì§•
                - True: (B, T', D_seq), (B, output_dim)  # ì‹œí€€ìŠ¤ íŠ¹ì§•, ì „ì—­ íŠ¹ì§•
        """
        B, T, C = sensor_data.shape
        if C != self.input_channels:
            raise ValueError(f"ì˜ˆìƒë˜ëŠ” ì±„ë„ ìˆ˜ {self.input_channels}ì™€ ë‹¤ë¦…ë‹ˆë‹¤. í˜„ì¬: {C}")

        # ê¸¸ì´ ë³´ì •
        if T != self.temporal_length:
            x = sensor_data.transpose(1, 2)  # (B,C,T)
            mode = 'linear' if self.interpolation_mode == 'cubic' and T < 4 else self.interpolation_mode
            x = F.interpolate(
                x, size=self.temporal_length, mode=mode,
                align_corners=False if mode in ('linear','cubic') else None
            )
        else:
            x = sensor_data.transpose(1, 2)  # (B,C,T)

        # Conv backbone
        for block in self.conv_backbone:
            if self.gradient_checkpointing and self.training:
                x = torch.utils.checkpoint.checkpoint(block, x, use_reentrant=False)
            else:
                x = block(x)  # (B, ch, T')

        # Transformer
        if self.use_transformer:
            x = x.transpose(1, 2)  # (B, T', ch)
            if self.conv_to_transformer_proj is not None:
                x = self.conv_to_transformer_proj(x)  # (B, T', actual_transformer_dim)
            if self.gradient_checkpointing and self.training:
                x = torch.utils.checkpoint.checkpoint(self.transformer, x, use_reentrant=False)
            else:
                x = self.transformer(x)               # (B, T', D_seq)
            seq_feat = x.transpose(1, 2)              # (B, D_seq, T')
        else:
            # Transformer ë¹„í™œì„± ì‹œ conv ì¶œë ¥ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            seq_feat = x                               # (B, ch, T')  â† ì—¬ê¸°ì„œ chê°€ D_seq ì—­í• 

        # ì „ì—­ í’€ë§ + íˆ¬ì˜
        pooled = self.temporal_pool(seq_feat).squeeze(-1)  # (B, D_seq)
        sensor_features = self.projection(pooled)          # (B, output_dim)

        if return_sequence:
            # (B, T', D_seq) í˜•íƒœë¡œ ë°˜í™˜í•˜ë„ë¡ transpose
            seq_feat_bt = seq_feat.transpose(1, 2)         # (B, T', D_seq)
            return seq_feat_bt, sensor_features
        return sensor_features

class ForceAwareSensorEncoder(nn.Module):
    """
    'ê±°ë¦¬' (distance)ì™€ 'í˜' (force) íŠ¹ì§•ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬
    í˜ ë°ì´í„°ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì„¼ì„œ ì¸ì½”ë”ì…ë‹ˆë‹¤.

    ì•„í‚¤í…ì²˜:
    1.  ì£¼ìš” 'ê±°ë¦¬' íŠ¹ì§•(`dist_channels`)ì€ í‘œì¤€ `SensorEncoder`ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    2.  'í˜' íŠ¹ì§•(`force_channels`)ì€ ì „ìš© MLPë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    3.  ë‘ ì¸ì½”ë”ì˜ ì¶œë ¥ì´ ê²°í•©(concat)ë˜ì–´ ìµœì¢… ì¶œë ¥ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜ë©ë‹ˆë‹¤.

    Args:
        dist_channels (int, optional): ê±°ë¦¬ ì„¼ì„œ ì±„ë„ ìˆ˜. Defaults to 1025.
        force_channels (int, optional): í˜ ì„¼ì„œ ì±„ë„ ìˆ˜. Defaults to 1.
        temporal_length (int, optional): ì‹œê³„ì—´ ê¸¸ì´. Defaults to 65.
        dist_hidden_dim (int, optional): ê±°ë¦¬ ì¸ì½”ë”ì˜ ì€ë‹‰ ì°¨ì›. Defaults to 512.
        force_hidden_dim (int, optional): í˜ ì¸ì½”ë”ì˜ ì€ë‹‰ ì°¨ì›. Defaults to 128.
        output_dim (int, optional): ìµœì¢… ì¶œë ¥ íŠ¹ì§• ë²¡í„°ì˜ ì°¨ì›. Defaults to 3072.
        **kwargs: `SensorEncoder`ë¡œ ì „ë‹¬ë  ì¶”ê°€ ì¸ìë“¤.
    """
    def __init__(self,
                dist_channels=1025,
                force_channels=1,
                temporal_length=65,
                dist_hidden_dim=512,
                force_hidden_dim=48,
                output_dim=1024,
                transformer_dim=None,
                **kwargs):
        super().__init__()
        self.input_channels = dist_channels + force_channels
        self.force_channels = force_channels

        # ê±°ë¦¬(branch)
        self.dist_encoder = SensorEncoder(
            input_channels=dist_channels,
            temporal_length=temporal_length,
            hidden_dim=dist_hidden_dim,
            output_dim=output_dim - force_hidden_dim,  # ì „ì—­ ì¶œë ¥ ì¼ë¶€ í• ë‹¹
            transformer_dim=transformer_dim,
            **kwargs
        )
        force_bn_fp32_(self.dist_encoder)

        # í˜(branch)
        self.force_encoder = nn.Sequential(
            nn.Linear(force_channels, force_hidden_dim // 2),
            nn.GELU(),
            nn.LayerNorm(force_hidden_dim // 2),
            nn.Linear(force_hidden_dim // 2, force_hidden_dim)
        )
        self.force_pool = nn.AdaptiveAvgPool1d(1)

        # ====== í•µì‹¬: ì‹œí€€ìŠ¤ ê²°í•© í›„ output_dimìœ¼ë¡œ ì •ë ¬ ======
        # dist ì‹œí€€ìŠ¤ ì°¨ì› ì¶”ë¡ : SensorEncoderì—ì„œ ë°˜í™˜ë˜ëŠ” seqì˜ feature dim
        # use_transformer=True â†’ self.dist_encoder.actual_transformer_dim
        # else                â†’ self.dist_encoder.final_channels
        if getattr(self.dist_encoder, "use_transformer", False):
            dist_seq_dim = self.dist_encoder.actual_transformer_dim
        else:
            dist_seq_dim = self.dist_encoder.final_channels

        self.seq_feature_dim = dist_seq_dim + force_hidden_dim
        self.seq_proj = nn.Linear(self.seq_feature_dim, output_dim)  # (B,T',seq_dim) â†’ (B,T',output_dim)

        self.output_dim = output_dim
        self.force_hidden_dim = force_hidden_dim

    def forward(self, sensor_data: torch.Tensor, return_sequence: bool = False):
        """
        Args:
            sensor_data: (B,T,C)
            return_sequence:
                - False: (B, output_dim)
                - True : (B, T', output_dim), (B, output_dim)
        """
        B, T, C = sensor_data.shape
        if C != self.input_channels:
            raise ValueError(f"ì˜ˆìƒë˜ëŠ” ì±„ë„ ìˆ˜ {self.input_channels}ì™€ ë‹¤ë¦…ë‹ˆë‹¤. í˜„ì¬: {C}")

        # ë¶„ë¦¬
        dist_data  = sensor_data[..., :-self.force_channels]   # (B,T,dist_channels)
        force_data = sensor_data[..., -self.force_channels:]   # (B,T,force_channels)

        if return_sequence:
            # 1) ê±°ë¦¬: ì‹œí€€ìŠ¤ + ì „ì—­
            dist_seq, dist_global = self.dist_encoder(dist_data, return_sequence=True)  # (B,T',D_dist_seq), (B, output_dim - force_hidden_dim)

            # 2) í˜: ì‹œê°„ì¶• MLP â†’ (B,T,force_hidden_dim) â†’ T'ë¡œ ë³´ê°„
            f_tmp = self.force_encoder(force_data)                   # (B,T,force_hidden_dim)
            f_tmp = f_tmp.transpose(1, 2)                            # (B,force_hidden_dim,T)
            # dist_seq ê¸¸ì´ì— ë§ì¶¤
            T_prime = dist_seq.shape[1]
            f_tmp = F.interpolate(f_tmp, size=T_prime, mode='linear', align_corners=False)  # (B,force_hidden_dim,T')
            force_seq = f_tmp.transpose(1, 2)                         # (B,T',force_hidden_dim)

            # 3) ì‹œí€€ìŠ¤ ê²°í•© â†’ output_dim ì •ë ¬
            seq_cat = torch.cat([dist_seq, force_seq], dim=-1)        # (B,T', D_dist_seq + force_hidden_dim)
            seq_out = self.seq_proj(seq_cat)                          # (B,T', output_dim)

            # 4) í˜ ì „ì—­í’€ë§
            force_pooled = self.force_pool(f_tmp).squeeze(-1)         # (B, force_hidden_dim)

            # 5) ì „ì—­ ê²°í•©
            combined_features = torch.cat([dist_global, force_pooled], dim=-1)  # (B, output_dim)
            return seq_out, combined_features

        else:
            # ê¸°ì¡´ ê²½ë¡œ(ì „ì—­ë§Œ)
            dist_global = self.dist_encoder(dist_data)                 # (B, output_dim - force_hidden_dim)
            f_tmp = self.force_encoder(force_data)                     # (B,T,force_hidden_dim)
            force_pooled = self.force_pool(f_tmp.transpose(1, 2)).squeeze(-1)  # (B, force_hidden_dim)
            combined_features = torch.cat([dist_global, force_pooled], dim=-1) # (B, output_dim)
            return combined_features
