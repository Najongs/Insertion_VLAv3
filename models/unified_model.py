"""
Unified Vision-Language-Action (VLA) Model - QwenVLAUnified

This file defines the main `QwenVLAUnified` model, which integrates various
sub-modules to perform Vision-Language-Action tasks.

The `QwenVLAUnified` class acts as the central orchestrator, combining:
- A Vision-Language (VL) backbone (from `vl_encoder`).
- Optional sensor and robot state encoders (from `Encoder_model`).
- An action prediction expert (from `action_decoder`).

It supports different action prediction paradigms like Flow Matching and Regression,
and handles the fusion of multimodal data.
"""

import os
import sys
from pathlib import Path
from typing import List, Optional, Literal, Tuple, Union

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from peft import LoraConfig, get_peft_model

# ìƒëŒ€ ì„í¬íŠ¸ vs ì ˆëŒ€ ì„í¬íŠ¸ ì²˜ë¦¬ (ì§ì ‘ ì‹¤í–‰ ì‹œ ì ˆëŒ€ ì„í¬íŠ¸ ì‚¬ìš©)
if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from models.Encoder_model import RobotStateEncoder, UnifiedGatedSensorEncoder, force_bn_fp32_
    from models.action_decoder import FlowMatchingActionExpert, RegressionActionExpert
    from models.vl_cache import VLACacheManager, get_cache_manager
    from models.vl_encoder import VisionLanguageEncoder
else:
    # ëª¨ë“ˆë¡œ ì„í¬íŠ¸ ì‹œ ìƒëŒ€ ì„í¬íŠ¸ ì‚¬ìš©
    from .Encoder_model import RobotStateEncoder, UnifiedGatedSensorEncoder, force_bn_fp32_
    from .action_decoder import FlowMatchingActionExpert, RegressionActionExpert
    from .vl_cache import VLACacheManager, get_cache_manager
    from .vl_encoder import VisionLanguageEncoder


class QwenVLAUnified(nn.Module):
    """
    Qwen-VL ë°±ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í†µí•© Vision-Language-Action (VLA) ëª¨ë¸ì…ë‹ˆë‹¤.
    ì„¼ì„œ ìœµí•© ë° ë¡œë´‡ ìƒíƒœ ì¸ì½”ë”ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì–‘ì‹ì˜ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ í–‰ë™ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    Flow Matching ë˜ëŠ” Regression ê¸°ë°˜ì˜ í–‰ë™ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    def __init__(
        self,
        model_type: Literal['regression', 'flow_matching'] = 'flow_matching',
        vl_model_name="Qwen/Qwen2.5-VL-3B-Instruct",
        action_dim=7,
        horizon=8,
        hidden_dim=1024,
        cache_dir="/home/najo/NAS/VLA/dataset/cache/qwen_vl_features",
        external_cache_root: Optional[str] = None,
        auto_cache_backfill: bool = True,
        # --- í†µí•©ëœ ì¸ì½”ë” íŒŒë¼ë¯¸í„° ---
        sensor_enabled=True,
        sensor_input_channels=1026, # dist_channels(1025) + force_channels(1)
        sensor_temporal_length=65,
        sensor_output_dim=3072, # UnifiedGatedSensorEncoderì˜ ê¸°ë³¸ ì¶œë ¥ ì°¨ì›
        robot_state_enabled=True,
        robot_state_temporal_length=100,
        robot_state_output_dim=1024, # ì—…ê·¸ë ˆì´ë“œëœ RobotStateEncoderì˜ ê¸°ë³¸ ì¶œë ¥ ì°¨ì›
        # --- ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„° ---
        fusion_strategy='cross_attention',
        flow_steps=10,
        flow_solver='euler',
        finetune_vl='none',
        lora_r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        image_resize_height=None,
        image_resize_width=None,
        parallel_view_encoding=False,
        view_aggregation='weighted_mean',
        view5_weight=2.0,
        device_map=None,
        cache_only_mode=False):
        super().__init__()

        if model_type not in ['regression', 'flow_matching']:
            raise ValueError(f"model_typeì€ 'regression', 'flow_matching' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬: {model_type}")

        self.model_type = model_type
        self.sensor_enabled = sensor_enabled
        self.robot_state_enabled = robot_state_enabled
        self.fusion_strategy = fusion_strategy
        self.flow_steps = flow_steps
        self.flow_solver = flow_solver
        self.action_dim = action_dim
        self.horizon = horizon
        self.auto_cache_backfill = auto_cache_backfill
        self.cache_only_mode = cache_only_mode
        self.external_cache_mgr: Optional[VLACacheManager] = None
        self.external_cache_root = None

        if external_cache_root:
            self.external_cache_root = str(external_cache_root)
            try:
                self.external_cache_mgr = get_cache_manager(cache_dir=self.external_cache_root)
                if self.auto_cache_backfill:
                    print(f"   ìë™ ìºì‹œ ë°±í•„ í™œì„±í™” â†’ {self.external_cache_root}")
            except Exception as e:
                print(f"âš ï¸ ì™¸ë¶€ ìºì‹œ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨ ({external_cache_root}): {e}")

        print(f"ğŸš€ QwenVLA í†µí•© ëª¨ë¸ V3 (Unified Encoders) ë¡œë”© ì¤‘")
        print(f"   ëª¨ë¸ íƒ€ì…: {model_type.upper()}")
        print(f"   ì„¼ì„œ í™œì„±í™”: {sensor_enabled}")
        print(f"   ë¡œë´‡ ìƒíƒœ í™œì„±í™”: {robot_state_enabled}")
        if cache_only_mode:
            print(f"   âš¡ ìºì‹œ ì „ìš© ëª¨ë“œ: VLM ëª¨ë¸ ë¡œë“œ ìŠ¤í‚µ (ë©”ëª¨ë¦¬ ì ˆì•½)")

        # VLM ë¡œë”© (cache_only_modeê°€ ì•„ë‹ ë•Œë§Œ)
        if not cache_only_mode:
            self.processor = AutoProcessor.from_pretrained(vl_model_name, use_fast=False)

            if image_resize_height and image_resize_width:
                target_pixels = image_resize_height * image_resize_width
                self.processor.image_processor.min_pixels = target_pixels
                self.processor.image_processor.max_pixels = target_pixels
                print(f"   ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: {image_resize_width}x{image_resize_height}")

            self.vl_model = self._load_qwen_with_fallback(vl_model_name, device_map)
            print(f"   VL ëª¨ë¸ hidden_size: {self.vl_model.config.hidden_size}")

            self.vl_encoder = VisionLanguageEncoder(
                vl_model=self.vl_model,
                processor=self.processor,
                cache_dir=cache_dir,
                parallel_view_encoding=parallel_view_encoding,
                view_aggregation=view_aggregation,
                view5_weight=view5_weight,
                device=next(self.vl_model.parameters()).device
            )

            if finetune_vl == 'lora':
                print(f"ğŸ”§ VL ëª¨ë¸ì— LoRA ì ìš© ì¤‘ (r={lora_r})...")
                lora_config = LoraConfig(
                    r=lora_r, lora_alpha=lora_alpha,
                    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
                    lora_dropout=lora_dropout, bias="none", task_type="CAUSAL_LM"
                )
                self.vl_model = get_peft_model(self.vl_model, lora_config)
                print("âœ… LoRA ì ìš© ì™„ë£Œ.")
            elif finetune_vl == 'none':
                print("ğŸ§Š VL ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ë™ê²° ì¤‘...")
                for p in self.vl_model.parameters():
                    p.requires_grad = False
                print("âœ… VL ëª¨ë¸ ë™ê²° ì™„ë£Œ.")
            else:
                print("ğŸ”¥ VL ëª¨ë¸ì€ ì™„ì „íˆ í•™ìŠµ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

            vl_hidden_size = self.vl_model.config.hidden_size
        else:
            # ìºì‹œ ì „ìš© ëª¨ë“œ: VLMì„ ë¡œë“œí•˜ì§€ ì•Šê³  ê¸°ë³¸ ì°¨ì›ë§Œ ì„¤ì •
            self.processor = None
            self.vl_model = None
            self.vl_encoder = None
            inferred_dim = self._infer_cached_vl_hidden_size(external_cache_root)
            if inferred_dim:
                vl_hidden_size = inferred_dim
                print(f"   ğŸ“¦ ìºì‹œ ê¸°ë°˜ hidden_size ìë™ ê°ì§€: {vl_hidden_size}")
            else:
                vl_hidden_size = 2048  # ê¸°ë³¸ê°’ (Qwen2.5-VL-3B)
                print(f"   âš ï¸ ìºì‹œì—ì„œ hidden_sizeë¥¼ ì°¾ì§€ ëª»í•´ ê¸°ë³¸ê°’ {vl_hidden_size} ì‚¬ìš©")

        if sensor_enabled:
            print("   ì„¼ì„œ ì¸ì½”ë”: UnifiedGatedSensorEncoder (bfloat16 ~53MB)")
            self.sensor_encoder = UnifiedGatedSensorEncoder(
                dist_channels=sensor_input_channels - 1,
                force_channels=1,
                temporal_length=sensor_temporal_length,
                output_dim=sensor_output_dim
            ).to(dtype=torch.bfloat16, device="cuda")
            force_bn_fp32_(self.sensor_encoder)
        else:
            self.sensor_encoder = None

        if self.robot_state_enabled:
            print("   ë¡œë´‡ ìƒíƒœ ì¸ì½”ë”: Upgraded RobotStateEncoder (bfloat16 ~41MB)")
            self.robot_state_encoder = RobotStateEncoder(
                temporal_length=robot_state_temporal_length,
                output_dim=robot_state_output_dim
            ).to(dtype=torch.bfloat16, device="cuda")
        else:
            self.robot_state_encoder = None

        combined_sensor_dim = 0
        if sensor_enabled:
            combined_sensor_dim += sensor_output_dim
        if self.robot_state_enabled:
            combined_sensor_dim += robot_state_output_dim

        ActionExpertClass = FlowMatchingActionExpert if model_type == 'flow_matching' else RegressionActionExpert
        self.action_expert = ActionExpertClass(
            image_feature_dim=vl_hidden_size,
            text_guidance_dim=vl_hidden_size,
            sensor_dim=combined_sensor_dim,
            action_dim=action_dim,
            horizon=horizon,
            hidden_dim=hidden_dim,
        ).to(dtype=torch.bfloat16, device="cuda")

        print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")

    def _load_qwen_with_fallback(self, vl_model_name: str, device_map: Optional[str]) -> Qwen2_5_VLForConditionalGeneration:
        """
        Qwen-VL ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. FlashAttention 2 ë˜ëŠ” SDPA ì–´í…ì…˜ êµ¬í˜„ì— ëŒ€í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ë©ë‹ˆë‹¤.
        GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¼ bfloat16 ë˜ëŠ” float16ì„ ì‹œë„í•©ë‹ˆë‹¤.
        """
        dtype_candidates = [torch.bfloat16, torch.float16]
        attn_candidates = ["flash_attention_2", "sdpa"]

        # FlashAttention 2 -> SDPA -> Default attention ìˆœì„œë¡œ ì‹œë„
        for impl in attn_candidates:
            for dtype in dtype_candidates:
                try:
                    print(f"ğŸ§  {impl} ì–´í…ì…˜ê³¼ {dtype} ë°ì´í„° íƒ€ì…ìœ¼ë¡œ Qwen-VL ë¡œë“œ ì‹œë„ ì¤‘...")
                    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        vl_model_name,
                        torch_dtype=dtype,
                        attn_implementation=impl,
                        device_map=device_map or "cuda",
                        low_cpu_mem_usage=True, # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPUë¡œ ì˜¤í”„ë¡œë“œ ì‹œë„
                    )
                    print(f"âœ… Qwen-VL ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {impl} ì–´í…ì…˜ ({dtype})")
                    self.attn_backend = impl
                    self.model_dtype = dtype
                    return model
                except Exception as e:
                    print(f"âš ï¸ {impl} ì–´í…ì…˜ ({dtype}) ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ëª¨ë“  íŠ¹ì • ì–´í…ì…˜ êµ¬í˜„ ì‹œë„ ì‹¤íŒ¨ ì‹œ, ê¸°ë³¸ ì–´í…ì…˜ìœ¼ë¡œ ì¬ì‹œë„
        for dtype in dtype_candidates:
            try:
                print(f"ğŸ§  ê¸°ë³¸ ì–´í…ì…˜ê³¼ {dtype} ë°ì´í„° íƒ€ì…ìœ¼ë¡œ Qwen-VL ë¡œë“œ ì‹œë„ ì¤‘...")
                model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    vl_model_name,
                    torch_dtype=dtype,
                    device_map=device_map or "cuda",
                    low_cpu_mem_usage=True,
                )
                print(f"âœ… Qwen-VL ëª¨ë¸ ë¡œë“œ ì„±ê³µ: ê¸°ë³¸ ì–´í…ì…˜ ({dtype})")
                self.attn_backend = "default"
                self.model_dtype = dtype
                return model
            except Exception as e:
                print(f"âš ï¸ ê¸°ë³¸ ì–´í…ì…˜ ({dtype}) ë¡œë“œ ì‹¤íŒ¨: {e}")

        raise RuntimeError("âŒ ëª¨ë“  Qwen-VL ëª¨ë¸ ë¡œë“œ ì‹œë„ ì‹¤íŒ¨. í˜¸í™˜ë˜ëŠ” ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")

    def set_cache_enabled(self, enabled: bool = True):
        """ë‚´ë¶€ VL íŠ¹ì§• ìºì‹± í™œì„±í™” ì—¬ë¶€ë¥¼ `vl_encoder`ì— ìœ„ì„í•©ë‹ˆë‹¤."""
        if hasattr(self, 'vl_encoder'):
            self.vl_encoder.set_cache_enabled(enabled)

    def set_strict_cache(self, enabled: bool = True):
        """ë‚´ë¶€ VL íŠ¹ì§• ìºì‹± ì‹œ ì—„ê²© ëª¨ë“œë¥¼ `vl_encoder`ì— ìœ„ì„í•©ë‹ˆë‹¤."""
        if hasattr(self, 'vl_encoder'):
            self.vl_encoder.set_strict_cache(enabled)

    def set_cache_limit_gb(self, limit_gb: float):
        """ë‚´ë¶€ VL íŠ¹ì§• ìºì‹œì˜ ìµœëŒ€ í¬ê¸°ë¥¼ `vl_encoder`ì— ìœ„ì„í•©ë‹ˆë‹¤."""
        if hasattr(self, 'vl_encoder'):
            self.vl_encoder.set_cache_limit_gb(limit_gb)

    @staticmethod
    def _infer_cached_vl_hidden_size(cache_root: Optional[str]) -> Optional[int]:
        """ì™¸ë¶€ ìºì‹œì—ì„œ VL hidden size ì¶”ë¡  (cache_only_mode ì „ìš©)."""
        if not cache_root:
            return None

        cache_root_path = Path(cache_root)
        if not cache_root_path.exists():
            return None

        try:
            prompt_dirs = [d for d in cache_root_path.iterdir() if d.is_dir()]
        except Exception:
            return None

        for prompt_dir in prompt_dirs:
            try:
                cache_files = sorted(prompt_dir.glob("*.pt"))
            except Exception:
                continue

            for cache_file in cache_files:
                try:
                    cached = torch.load(cache_file, map_location="cpu")
                except Exception:
                    continue

                if isinstance(cached, tuple) and len(cached) == 2:
                    img_tokens, txt_tokens = cached
                    if isinstance(txt_tokens, torch.Tensor) and txt_tokens.shape[-1] > 0:
                        return int(txt_tokens.shape[-1])
                    if isinstance(img_tokens, torch.Tensor) and img_tokens.dim() >= 3 and img_tokens.shape[-1] > 0:
                        return int(img_tokens.shape[-1])
                elif isinstance(cached, torch.Tensor) and cached.dim() >= 2:
                    if cached.shape[-1] > 0:
                        return int(cached.shape[-1])

        return None

    def _prepare_dataloader_cached_vl_tokens(self, cached_batch: Optional[List[Optional[Tuple[torch.Tensor, torch.Tensor]]]], device: torch.device) -> Tuple[Optional[Tuple[Union[torch.Tensor, List], Union[torch.Tensor, List]]], Optional[List[int]]]:
        """
        ë°ì´í„°ë¡œë”ì—ì„œ ì œê³µëœ ìºì‹œëœ VL íŠœí”Œ(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸)ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
        ë¶€ë¶„ì ì¸ ìºì‹œ ì»¤ë²„ë¦¬ì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        if not cached_batch:
            return None, None

        target_dtype = getattr(self, "model_dtype", torch.bfloat16)
        prepared_img_tokens: List[Optional[torch.Tensor]] = []
        prepared_txt_tokens: List[Optional[torch.Tensor]] = []
        missing_indices: List[int] = []
        has_any_valid_tensor = False

        for idx, item in enumerate(cached_batch):
            if isinstance(item, (list, tuple)) and len(item) == 2:
                img_t, txt_t = item
                img_is_tensor = isinstance(img_t, torch.Tensor) and img_t.dim() == 3
                txt_is_tensor = isinstance(txt_t, torch.Tensor) and txt_t.numel() > 0

                if img_is_tensor and txt_is_tensor:
                    prepared_img_tokens.append(img_t.to(device=device, dtype=target_dtype, non_blocking=True))
                    prepared_txt_tokens.append(txt_t.to(device=device, dtype=target_dtype, non_blocking=True))
                    has_any_valid_tensor = True
                    continue

            prepared_img_tokens.append(None)
            prepared_txt_tokens.append(None)
            missing_indices.append(idx)

        if not has_any_valid_tensor:
            return None, None

        if not missing_indices:
            # ëª¨ë“  ìƒ˜í”Œì´ ìºì‹œë¨
            return (torch.cat(prepared_img_tokens, dim=0), torch.cat(prepared_txt_tokens, dim=0)), None

        # ì¼ë¶€ ìƒ˜í”Œë§Œ ìºì‹œë¨
        return (prepared_img_tokens, prepared_txt_tokens), missing_indices

    def _encode_missing_vl_features_and_backfill(self,
                                                text_inputs: List[str],
                                                image_inputs: List[List[str]],
                                                cache_keys: List[str],
                                                indices_to_encode: List[int],
                                                device: torch.device,
                                                vl_cache_metadata: Optional[dict] = None) -> dict:
        """
        ëˆ„ë½ëœ VL íŠ¹ì§•(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ íŠœí”Œ)ë§Œì„ ì¸ì½”ë”©í•˜ê³  ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì±„ì›ë‹ˆë‹¤.
        """
        if not indices_to_encode:
            return {}

        subset_texts = [text_inputs[i] for i in indices_to_encode]
        subset_images = [image_inputs[i] for i in indices_to_encode]
        subset_keys = [cache_keys[i] for i in indices_to_encode]

        # VL íŠ¹ì§• ì¸ì½”ë”© (V2: íŠœí”Œ ë°˜í™˜)
        image_features, guidance_vectors = self.vl_encoder.encode(
            subset_texts, subset_images, subset_keys, use_cache=False
        )

        # ê²°ê³¼ë¥¼ ì¸ë±ìŠ¤ë³„ íŠœí”Œë¡œ ë¶„í• 
        img_splits = torch.split(image_features, 1, dim=0)
        txt_splits = torch.split(guidance_vectors, 1, dim=0)
        tokens_by_index = {
            idx: (img, txt) for idx, img, txt in zip(indices_to_encode, img_splits, txt_splits)
        }

        if self.auto_cache_backfill and self.external_cache_mgr and vl_cache_metadata:
            dataset_names = vl_cache_metadata.get("dataset_names")
            vlm_indices = vl_cache_metadata.get("vlm_indices")
            prompt_hashes = vl_cache_metadata.get("prompt_hashes")

            if dataset_names and vlm_indices and prompt_hashes:
                for idx, (img_tensor, txt_tensor) in tokens_by_index.items():
                    if img_tensor is None or txt_tensor is None: continue
                    try:
                        if idx >= len(dataset_names) or idx >= len(vlm_indices) or idx >= len(prompt_hashes): continue
                        dataset_name, vlm_idx, prompt_hash = dataset_names[idx], int(vlm_indices[idx]), prompt_hashes[idx]
                    except (TypeError, ValueError, IndexError):
                        continue
                    if dataset_name is None or prompt_hash is None: continue
                    
                    try:
                        if not hasattr(self, "_cache_backfill_notice"):
                            print("ğŸ§· í›ˆë ¨ ì¤‘ ëˆ„ë½ëœ VL ìºì‹œ í•­ëª©ì„ ìë™ìœ¼ë¡œ ë¹Œë“œí•©ë‹ˆë‹¤ (V2 í˜•ì‹).")
                            self._cache_backfill_notice = True
                        # V2: íŠœí”Œì„ ì €ì¥
                        self.external_cache_mgr.save_cache(
                            dataset_name=dataset_name, vlm_idx=vlm_idx, prompt_hash=prompt_hash,
                            vl_features=(img_tensor.detach(), txt_tensor.detach()),
                        )
                    except Exception as e:
                        if not hasattr(self, "_cache_backfill_warned"):
                            print(f"âš ï¸ VL ìºì‹œ í•­ëª© ë°±í•„ ì‹¤íŒ¨ ({dataset_name}_vlm{vlm_idx}): {e}")
                            self.backfill_warned = True
        return tokens_by_index

    def encode_vision(self,
                      text_inputs: List[str],
                      image_inputs: List[List[str]],
                      cache_keys: List[str],
                      use_cache: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì…ë ¥ìœ¼ë¡œë¶€í„° VL íŠ¹ì§•(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ íŠœí”Œ)ì„ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
        ì£¼ë¡œ ìºì‹œ ë¹Œë”© ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        """
        self.eval()
        return self.vl_encoder.encode(
            text_inputs, image_inputs, cache_keys, use_cache=use_cache
        )

    def forward(self,
                text_inputs: List[str],
                image_inputs: List[List[str]],
                actions: Optional[torch.Tensor] = None,
                z_chunk: Optional[torch.Tensor] = None,
                sensor_data: Optional[torch.Tensor] = None,
                robot_states: Optional[torch.Tensor] = None,
                cache_keys: Optional[List[str]] = None,
                cache: bool = True,
                vl_cache_tokens: Optional[List[Optional[Tuple[torch.Tensor, torch.Tensor]]]] = None,
                vl_cache_metadata: Optional[dict] = None) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        V2 ì•„í‚¤í…ì²˜ë¥¼ ìœ„í•œ í†µí•© í¬ì›Œë“œ íŒ¨ìŠ¤.
        ì´ë¯¸ì§€ íŠ¹ì§•ê³¼ í…ìŠ¤íŠ¸ ê°€ì´ë˜ìŠ¤ë¥¼ ë¶„ë¦¬í•˜ì—¬ í–‰ë™ ì „ë¬¸ê°€ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
        """
        device = next(self.parameters()).device
        image_features, guidance_vectors = None, None

        # 1. VL íŠ¹ì§• ì¸ì½”ë”© ë° ìºì‹± ì²˜ë¦¬ (V2)
        prepared_cached_tokens, missing_indices = self._prepare_dataloader_cached_vl_tokens(vl_cache_tokens, device)

        if prepared_cached_tokens and not missing_indices:
            image_features, guidance_vectors = prepared_cached_tokens
            if not hasattr(self, "_external_cache_confirmed"):
                print("ğŸ’¾ ë°ì´í„°ë¡œë”ì—ì„œ ì œê³µëœ VL ìºì‹œ í…ì„œ(V2) ì‚¬ìš© ì¤‘.")
                self._external_cache_confirmed = True
        elif prepared_cached_tokens and missing_indices:
            if self.cache_only_mode:
                raise RuntimeError("âš ï¸ cache_only_modeì—ì„œëŠ” ëª¨ë“  VL ìºì‹œê°€ ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ëˆ„ë½ëœ ìºì‹œê°€ ìˆìŠµë‹ˆë‹¤.")

            prepared_img_tokens, prepared_txt_tokens = prepared_cached_tokens
            new_tokens_by_idx = self._encode_missing_vl_features_and_backfill(
                text_inputs, image_inputs, cache_keys, missing_indices, device, vl_cache_metadata
            )
            for idx in missing_indices:
                if idx in new_tokens_by_idx:
                    prepared_img_tokens[idx], prepared_txt_tokens[idx] = new_tokens_by_idx[idx]

            if all(t is not None for t in prepared_img_tokens) and all(t is not None for t in prepared_txt_tokens):
                image_features = torch.cat(prepared_img_tokens, dim=0)
                guidance_vectors = torch.cat(prepared_txt_tokens, dim=0)
            else:
                raise RuntimeError("âš ï¸ ë°ì´í„°ë¡œë” ìºì‹œì™€ ì‹ ê·œ ì¸ì½”ë”© í›„ì—ë„ VL í† í°ì´ ì™„ì „íˆ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            if self.cache_only_mode:
                if actions is not None and self.training:
                    return torch.tensor(0.0, device=device, requires_grad=True), None, None
                else:
                    batch_size = len(text_inputs)
                    return torch.zeros(batch_size, self.horizon, self.action_dim, device=device), None, None

            image_features, guidance_vectors = self.vl_encoder.encode(
                text_inputs, image_inputs, cache_keys, use_cache=cache
            )

        # 2. ì„¼ì„œ ë° ë¡œë´‡ ìƒíƒœ íŠ¹ì§• ì¸ì½”ë”©
        sensor_features_encoded: Optional[torch.Tensor] = None
        if self.sensor_enabled and sensor_data is not None:
            sensor_features_encoded = self.sensor_encoder(sensor_data.to(device=device, dtype=torch.bfloat16))

        robot_state_features_encoded: Optional[torch.Tensor] = None
        if self.robot_state_enabled and robot_states is not None:
            robot_state_features_encoded = self.robot_state_encoder(robot_states.to(device=device, dtype=torch.bfloat16))

        # 3. ì„¼ì„œ íŠ¹ì§• ê²°í•©
        sensor_tensors = []
        if sensor_features_encoded is not None:
            sensor_tensors.append(sensor_features_encoded)
        if robot_state_features_encoded is not None:
            sensor_tensors.append(robot_state_features_encoded)

        sensor_features_combined: Optional[torch.Tensor] = None
        if sensor_tensors:
            if len(sensor_tensors) > 1:
                sensor_features_combined = torch.cat(sensor_tensors, dim=-1)
            else:
                sensor_features_combined = sensor_tensors[0]

        if image_features is not None and image_features.dim() == 2:
            image_features = image_features.unsqueeze(1)

        # 4. ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ í¬ì›Œë“œ íŒ¨ìŠ¤
        if self.model_type == 'flow_matching':
            if actions is not None and self.training:
                actions = actions.to(device=device, dtype=image_features.dtype)
                with torch.autocast(device.type, dtype=torch.bfloat16):
                    loss = self.action_expert.compute_loss(
                        actions, image_features, guidance_vectors,
                        sensor_features=sensor_features_combined
                    )
                return loss, None, None
            else:
                sampled_actions = self.action_expert.sample(
                    image_features, guidance_vectors,
                    sensor_features=sensor_features_combined,
                    num_steps=self.flow_steps, method=self.flow_solver
                )
                return sampled_actions, None, None
        elif self.model_type == 'regression':
            if z_chunk is None:
                raise ValueError("Regression ëª¨ë¸ì€ z_chunk ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            z_chunk = z_chunk.to(device=device, dtype=image_features.dtype)
            with torch.autocast(device.type, dtype=torch.bfloat16):
                pred_actions, delta = self.action_expert(
                    z_chunk, image_features, guidance_vectors,
                    sensor_features=sensor_features_combined
                )
            return pred_actions, delta
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")

    @torch.no_grad()
    def predict_action(self,
                       text_inputs: List[str],
                       image_inputs: List[List[str]],
                       sensor_data: Optional[torch.Tensor] = None,
                       robot_states: Optional[torch.Tensor] = None,
                       cache_keys: Optional[List[str]] = None,
                       **kwargs) -> torch.Tensor:
        """
        V2 ì•„í‚¤í…ì²˜ë¥¼ ìœ„í•œ ì¶”ë¡  ì „ìš© ë˜í¼ í•¨ìˆ˜.
        """
        self.eval()
        
        if self.model_type == 'flow_matching':
            sampled_actions, _, _ = self.forward(
                text_inputs=text_inputs, image_inputs=image_inputs,
                actions=None, sensor_data=sensor_data, robot_states=robot_states,
                cache_keys=cache_keys, **kwargs
            )
            return sampled_actions
        else:
            raise NotImplementedError(
                "Regression ëª¨ë¸ì˜ predict_actionì€ z_chunkê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                "`forward()` ë©”ì„œë“œë¥¼ `z_chunk` ë§¤ê°œë³€ìˆ˜ì™€ í•¨ê»˜ ì§ì ‘ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
            )

if __name__ == "__main__":
    print("ğŸ§ª Unified VLA ëª¨ë¸ V3 í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜: {device}")

    try:
        model_flow_matching = QwenVLAUnified(
            model_type='flow_matching', sensor_enabled=True, robot_state_enabled=True,
            finetune_vl='none', cache_dir="./test_cache",
        ).to(device)
        model_flow_matching.eval()

        batch_size = 2
        horizon = model_flow_matching.horizon
        action_dim = model_flow_matching.action_dim
        
        text_inputs_dummy = ["ë¡œë´‡ íŒ”ì„ ì›€ì§ì—¬ ì»µì„ ì¡ìœ¼ì‹œì˜¤.", "ë¹¨ê°„ ë¸”ë¡ì„ ì™¼ìª½ìœ¼ë¡œ ì˜®ê¸°ì‹œì˜¤."]
        import glob
        sample_images = glob.glob("/home/najo/NAS/VLA/dataset/New_dataset2/**/View4/*.jpg", recursive=True)[:4]
        if len(sample_images) >= 2:
            image_inputs_dummy = [[sample_images[0], sample_images[1]], [sample_images[2], sample_images[3]]]
            print(f"  í…ŒìŠ¤íŠ¸ìš© ì‹¤ì œ ì´ë¯¸ì§€ ì‚¬ìš©: {len(image_inputs_dummy)}ê°œ ìƒ˜í”Œ, ê° 2ê°œ ë·°")
        else:
            print("  âš ï¸ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
            image_inputs_dummy = [[], []]

        sensor_data_dummy = torch.randn(batch_size, 65, 1026, device=device, dtype=torch.float32)
        robot_states_dummy = torch.randn(batch_size, 100, 12, device=device, dtype=torch.float32)
        
        with torch.no_grad():
            sampled_actions_flow = model_flow_matching.predict_action(
                text_inputs=text_inputs_dummy, image_inputs=image_inputs_dummy,
                sensor_data=sensor_data_dummy, robot_states=robot_states_dummy
            )
        print(f"âœ… Flow Matching ëª¨ë¸ ì¶”ë¡  ì„±ê³µ. ì¶œë ¥ í˜•íƒœ: {sampled_actions_flow.shape}")
        assert sampled_actions_flow.shape == (batch_size, horizon, action_dim)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    test_cache_dir = Path("./test_cache")
    if test_cache_dir.exists():
        import shutil
        print(f"í…ŒìŠ¤íŠ¸ ìºì‹œ ë””ë ‰í† ë¦¬ {test_cache_dir} ì •ë¦¬ ì¤‘...")
        shutil.rmtree(test_cache_dir)
        print("ì •ë¦¬ ì™„ë£Œ.")
