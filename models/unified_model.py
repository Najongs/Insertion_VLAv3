"""
Unified Vision-Language-Action (VLA) Model - QwenVLAUnified

This file defines the main `QwenVLAUnified` model, which integrates various
sub-modules to perform Vision-Language-Action tasks.

The `QwenVLAUnified` class acts as the central orchestrator, combining:
- A Vision-Language (VL) backbone (from `vl_encoder`).
- Optional sensor and robot state encoders (from `Encoder_model`).
- An action prediction expert (from `action_decoder`).

It supports different action prediction paradigms like Flow Matching and Regression,
and handles the fusion of multimodal data.
"""

import os
import sys
from pathlib import Path
from typing import List, Optional, Literal, Tuple, Union

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from peft import LoraConfig, get_peft_model

# ìƒëŒ€ ì„í¬íŠ¸ vs ì ˆëŒ€ ì„í¬íŠ¸ ì²˜ë¦¬ (ì§ì ‘ ì‹¤í–‰ ì‹œ ì ˆëŒ€ ì„í¬íŠ¸ ì‚¬ìš©)
if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from models.Encoder_model import RobotStateEncoder, SensorEncoder, ForceAwareSensorEncoder, force_bn_fp32_
    from models.action_decoder import FlowMatchingActionExpert, RegressionActionExpert, DiffusionActionExpert
    from models.vl_cache import VLACacheManager, get_cache_manager
    from models.vl_encoder import VisionLanguageEncoder
else:
    # ëª¨ë“ˆë¡œ ì„í¬íŠ¸ ì‹œ ìƒëŒ€ ì„í¬íŠ¸ ì‚¬ìš©
    from .Encoder_model import RobotStateEncoder, SensorEncoder, ForceAwareSensorEncoder, force_bn_fp32_
    from .action_decoder import FlowMatchingActionExpert, RegressionActionExpert, DiffusionActionExpert
    from .vl_cache import VLACacheManager, get_cache_manager
    from .vl_encoder import VisionLanguageEncoder


class QwenVLAUnified(nn.Module):
    """
    Qwen-VL ë°±ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í†µí•© Vision-Language-Action (VLA) ëª¨ë¸ì…ë‹ˆë‹¤.
    ì„¼ì„œ ìœµí•© ë° ë¡œë´‡ ìƒíƒœ ì¸ì½”ë”ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì–‘ì‹ì˜ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ í–‰ë™ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    Flow Matching ë˜ëŠ” Regression ê¸°ë°˜ì˜ í–‰ë™ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    íŠ¹ì§•:
    - ìºì‹± ë° LoRA ë¯¸ì„¸ ì¡°ì •ì„ ì§€ì›í•˜ëŠ” Qwen-VL ë°±ë³¸.
    - í•™ìŠµ ê°€ëŠ¥í•œ ì„¼ì„œ ì¸ì½”ë” (ì˜µì…˜).
    - í•™ìŠµ ê°€ëŠ¥í•œ í–‰ë™ ì „ë¬¸ê°€ (Flow Matching ë˜ëŠ” Regression).
    - ë©€í‹°ë·° ì´ë¯¸ì§€ì˜ ë³‘ë ¬ ì¸ì½”ë”© ë° ìºì‹±ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”.

    Args:
        model_type (Literal['diffusion', 'regression', 'flow_matching']): ì‚¬ìš©í•  í–‰ë™ ì „ë¬¸ê°€ ëª¨ë¸ íƒ€ì….
            'diffusion'ì€ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'flow_matching' ë˜ëŠ” 'regression'ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
        vl_model_name (str, optional): Qwen-VL ëª¨ë¸ ì´ë¦„. Defaults to "Qwen/Qwen2.5-VL-3B-Instruct".
        action_dim (int, optional): í–‰ë™ ê³µê°„ì˜ ì°¨ì› (ì˜ˆ: 7 for (dx,dy,dz,droll,dpitch,dyaw,gripper)). Defaults to 7.
        horizon (int, optional): ì˜ˆì¸¡í•  í–‰ë™ ì‹œí€€ìŠ¤ì˜ ì‹œê°„ í˜¸ë¼ì´ì¦Œ. Defaults to 8.
        hidden_dim (int, optional): í–‰ë™ ì „ë¬¸ê°€ì˜ ì€ë‹‰ ì°¨ì›. Defaults to 1024.
        cache_dir (str, optional): ë‚´ë¶€ ìºì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬. Defaults to "/home/najo/NAS/VLA/dataset/cache/qwen_vl_features".
        external_cache_root (Optional[str], optional): ì™¸ë¶€(ê³µìœ ) ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ. Defaults to None.
        auto_cache_backfill (bool, optional): ì™¸ë¶€ ìºì‹œì— ì—†ëŠ” í•­ëª©ì„ ìë™ìœ¼ë¡œ ì±„ìš¸ì§€ ì—¬ë¶€. Defaults to True.

        # ì„¼ì„œ ì¸ì½”ë” ë§¤ê°œë³€ìˆ˜
        sensor_enabled (bool, optional): ì„¼ì„œ ì¸ì½”ë” í™œì„±í™” ì—¬ë¶€. Defaults to True.
        sensor_encoder_type (Literal['default', 'force_aware'], optional):
            ì„¼ì„œ ì¸ì½”ë” íƒ€ì… ('default' ë˜ëŠ” 'force_aware'). Defaults to 'default'.
        sensor_input_channels (int, optional): ì„¼ì„œ ì…ë ¥ ì±„ë„ ìˆ˜. Defaults to 1026.
        sensor_temporal_length (int, optional): ì„¼ì„œ ì‹œê³„ì—´ì˜ ê¸¸ì´ (ì˜ˆ: 650 for full, 65 for async). Defaults to 650.
        sensor_hidden_dim (int, optional): ì„¼ì„œ ì¸ì½”ë”ì˜ ì€ë‹‰ ì°¨ì›. Defaults to 512.
        sensor_output_dim (int, optional): ì„¼ì„œ ì¸ì½”ë”ì˜ ì¶œë ¥ íŠ¹ì§• ì°¨ì›. Defaults to 3072.

        # ë¡œë´‡ ìƒíƒœ ì¸ì½”ë” ë§¤ê°œë³€ìˆ˜
        robot_state_enabled (bool, optional): ë¡œë´‡ ìƒíƒœ ì…ë ¥ì„ í™œì„±í™”í• ì§€ ì—¬ë¶€ (ê´€ì ˆ + í¬ì¦ˆ). Defaults to True.
        robot_state_temporal_length (int, optional): ë¡œë´‡ ìƒíƒœ ì‹œê³„ì—´ì˜ ì‹œê°„ ìœˆë„ìš° (ì˜ˆ: 100Hzì—ì„œ 100ìƒ˜í”Œ = 1ì´ˆ). Defaults to 100.

        # íŠ¹ì§• ìœµí•© ë§¤ê°œë³€ìˆ˜
        fusion_strategy (str, optional): VL ë° ì„¼ì„œ/ë¡œë´‡ ìƒíƒœ íŠ¹ì§• ìœµí•© ì „ëµ ('concat', 'cross_attention', 'gated'). Defaults to 'concat'.

        # Flow Matching ë§¤ê°œë³€ìˆ˜ (model_type='flow_matching'ì¼ ë•Œë§Œ í•´ë‹¹)
        flow_steps (int, optional): ODE í†µí•© ìŠ¤í… ìˆ˜. Defaults to 10.
        flow_solver (str, optional): ODE ì†”ë²„ ('euler' ë˜ëŠ” 'rk4'). Defaults to 'euler'.

        # LoRA ë§¤ê°œë³€ìˆ˜ (ì„ íƒì  VL ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •ìš©)
        finetune_vl (Literal['none', 'lora', 'full']): VL ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • ì „ëµ.
            'none'ì€ ë™ê²°, 'lora'ëŠ” LoRA ì ìš©, 'full'ì€ ì „ì²´ ë¯¸ì„¸ ì¡°ì •. Defaults to 'none'.
        lora_r (int, optional): LoRA ë­í¬. Defaults to 16.
        lora_alpha (int, optional): LoRA ì•ŒíŒŒ. Defaults to 32.
        lora_dropout (float, optional): LoRA ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨. Defaults to 0.05.

        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ë§¤ê°œë³€ìˆ˜ (ë” ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•¨)
        image_resize_height (Optional[int], optional): ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ë†’ì´ (ì˜ˆ: 360). Defaults to None.
        image_resize_width (Optional[int], optional): ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ë„ˆë¹„ (ì˜ˆ: 640). Defaults to None.

        # VL ìµœì í™” ë§¤ê°œë³€ìˆ˜
        parallel_view_encoding (bool, optional): ë©€í‹°ë·° ë³‘ë ¬ ì¸ì½”ë”© í™œì„±í™” ì—¬ë¶€. Defaults to False.
            Trueì¼ ê²½ìš° ì´ë¯¸ì§€ ë·°ë³„ë¡œ ë³‘ë ¬ë¡œ VL ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        view_aggregation (Literal['mean', 'max', 'attention'], optional):
            ë©€í‹°ë·° íŠ¹ì§• ì§‘ê³„ ë°©ë²• ('mean', 'max', 'attention'). Defaults to 'mean'.
            'attention'ì€ í˜„ì¬ 'mean'ìœ¼ë¡œ í´ë°±ë©ë‹ˆë‹¤.

        device_map (Optional[str], optional): ëª¨ë¸ ë¡œë”© ì‹œ ì‚¬ìš©í•  device_map. Defaults to None.
    """
    def __init__(
        self,
        model_type: Literal['diffusion', 'regression', 'flow_matching'] = 'flow_matching',
        vl_model_name="Qwen/Qwen2.5-VL-3B-Instruct",
        action_dim=7,
        horizon=8,
        hidden_dim=1024,
        cache_dir="/home/najo/NAS/VLA/dataset/cache/qwen_vl_features",
        external_cache_root: Optional[str] = None,
        auto_cache_backfill: bool = True,
        # ì„¼ì„œ ì¸ì½”ë” ë§¤ê°œë³€ìˆ˜
        sensor_enabled=True,
        sensor_encoder_type: Literal['default', 'force_aware'] = 'force_aware', # ê¸°ë³¸ê°’ì„ force_awareë¡œ ë³€ê²½
        sensor_input_channels=1026,
        sensor_temporal_length=65, # ë¹„ë™ê¸° ë°ì´í„°ì…‹ ê¸°ì¤€ 65
        sensor_hidden_dim=512,
        sensor_output_dim=1024, # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— ëª…ì‹œëœ ê°’
        # ë¡œë´‡ ìƒíƒœ ì¸ì½”ë” ë§¤ê°œë³€ìˆ˜
        robot_state_enabled=True,
        robot_state_temporal_length=100,
        robot_state_output_dim=512, # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— ëª…ì‹œëœ ê°’
        # íŠ¹ì§• ìœµí•© ë§¤ê°œë³€ìˆ˜
        fusion_strategy='cross_attention', # 'concat', 'cross_attention', 'gated'
        # Flow Matching ë§¤ê°œë³€ìˆ˜
        flow_steps=10,
        flow_solver='euler',
        # LoRA ë§¤ê°œë³€ìˆ˜
        finetune_vl='none',
        lora_r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        # ì´ë¯¸ì§€ ë° VL ìµœì í™” ë§¤ê°œë³€ìˆ˜
        image_resize_height=None,
        image_resize_width=None,
        parallel_view_encoding=False,
        view_aggregation='weighted_mean', # V2 ì•„í‚¤í…ì²˜ ê¸°ë³¸ê°’
        view5_weight=2.0, # V2 ì•„í‚¤í…ì²˜ ê¸°ë³¸ê°’
        device_map=None,
        # ìºì‹œ ì „ìš© ëª¨ë“œ (VLM ë¡œë“œ ìŠ¤í‚µ, ë©”ëª¨ë¦¬ ì ˆì•½)
        cache_only_mode=False):
        super().__init__()

        if model_type not in ['diffusion', 'regression', 'flow_matching']:
            raise ValueError(f"model_typeì€ 'diffusion', 'regression', 'flow_matching' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬: {model_type}")

        self.model_type = model_type
        self.sensor_enabled = sensor_enabled
        self.robot_state_enabled = robot_state_enabled
        self.fusion_strategy = fusion_strategy
        self.flow_steps = flow_steps
        self.flow_solver = flow_solver
        self.action_dim = action_dim
        self.horizon = horizon
        self.auto_cache_backfill = auto_cache_backfill
        self.cache_only_mode = cache_only_mode
        self.external_cache_mgr: Optional[VLACacheManager] = None
        self.external_cache_root = None

        if external_cache_root:
            self.external_cache_root = str(external_cache_root)
            try:
                self.external_cache_mgr = get_cache_manager(cache_dir=self.external_cache_root)
                if self.auto_cache_backfill:
                    print(f"   ìë™ ìºì‹œ ë°±í•„ í™œì„±í™” â†’ {self.external_cache_root}")
            except Exception as e:
                print(f"âš ï¸ ì™¸ë¶€ ìºì‹œ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨ ({external_cache_root}): {e}")

        print(f"ğŸš€ QwenVLA í†µí•© ëª¨ë¸ V2 (Cross-Attention) ë¡œë”© ì¤‘")
        print(f"   ëª¨ë¸ íƒ€ì…: {model_type.upper()}")
        print(f"   ì„¼ì„œ í™œì„±í™”: {sensor_enabled}")
        print(f"   ë¡œë´‡ ìƒíƒœ í™œì„±í™”: {robot_state_enabled}")
        if model_type == 'flow_matching':
            print(f"   Flow ìŠ¤í…: {flow_steps}, ì†”ë²„: {flow_solver}")
        if cache_only_mode:
            print(f"   âš¡ ìºì‹œ ì „ìš© ëª¨ë“œ: VLM ëª¨ë¸ ë¡œë“œ ìŠ¤í‚µ (ë©”ëª¨ë¦¬ ì ˆì•½)")

        # VLM ë¡œë”© (cache_only_modeê°€ ì•„ë‹ ë•Œë§Œ)
        if not cache_only_mode:
            self.processor = AutoProcessor.from_pretrained(vl_model_name, use_fast=False)

            if image_resize_height and image_resize_width:
                target_pixels = image_resize_height * image_resize_width
                self.processor.image_processor.min_pixels = target_pixels
                self.processor.image_processor.max_pixels = target_pixels
                print(f"   ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: {image_resize_width}x{image_resize_height}")

            self.vl_model = self._load_qwen_with_fallback(vl_model_name, device_map)
            print(f"   VL ëª¨ë¸ hidden_size: {self.vl_model.config.hidden_size}")

            self.vl_encoder = VisionLanguageEncoder(
                vl_model=self.vl_model,
                processor=self.processor,
                cache_dir=cache_dir,
                parallel_view_encoding=parallel_view_encoding,
                view_aggregation=view_aggregation,
                view5_weight=view5_weight,
                device=next(self.vl_model.parameters()).device
            )

            if finetune_vl == 'lora':
                print(f"ğŸ”§ VL ëª¨ë¸ì— LoRA ì ìš© ì¤‘ (r={lora_r})...")
                lora_config = LoraConfig(
                    r=lora_r, lora_alpha=lora_alpha,
                    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
                    lora_dropout=lora_dropout, bias="none", task_type="CAUSAL_LM"
                )
                self.vl_model = get_peft_model(self.vl_model, lora_config)
                print("âœ… LoRA ì ìš© ì™„ë£Œ.")
            elif finetune_vl == 'none':
                print("ğŸ§Š VL ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ë™ê²° ì¤‘...")
                for p in self.vl_model.parameters():
                    p.requires_grad = False
                print("âœ… VL ëª¨ë¸ ë™ê²° ì™„ë£Œ.")
            else:
                print("ğŸ”¥ VL ëª¨ë¸ì€ ì™„ì „íˆ í•™ìŠµ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

            vl_hidden_size = self.vl_model.config.hidden_size
        else:
            # ìºì‹œ ì „ìš© ëª¨ë“œ: VLMì„ ë¡œë“œí•˜ì§€ ì•Šê³  ê¸°ë³¸ ì°¨ì›ë§Œ ì„¤ì •
            self.processor = None
            self.vl_model = None
            self.vl_encoder = None
            vl_hidden_size = 2048  # Qwen2.5-VL-3B ê¸°ë³¸ hidden_size

        if sensor_enabled:
            if sensor_encoder_type == 'force_aware':
                print("   ì„¼ì„œ ì¸ì½”ë” íƒ€ì…: Force-Aware")
                self.sensor_encoder = ForceAwareSensorEncoder(
                    dist_channels=sensor_input_channels - 1, force_channels=1,
                    temporal_length=sensor_temporal_length, dist_hidden_dim=sensor_hidden_dim,
                    force_hidden_dim=128, output_dim=sensor_output_dim,
                    use_transformer=True, num_transformer_layers=2
                ).to(dtype=torch.bfloat16, device="cuda")
            else:
                print("   ì„¼ì„œ ì¸ì½”ë” íƒ€ì…: ê¸°ë³¸ê°’")
                self.sensor_encoder = SensorEncoder(
                    input_channels=sensor_input_channels, temporal_length=sensor_temporal_length,
                    hidden_dim=sensor_hidden_dim, output_dim=sensor_output_dim,
                    use_transformer=True, num_transformer_layers=2
                ).to(dtype=torch.bfloat16, device="cuda")
            force_bn_fp32_(self.sensor_encoder)
        else:
            self.sensor_encoder = None

        if self.robot_state_enabled:
            self.robot_state_encoder = RobotStateEncoder(
                input_dim=12, temporal_length=robot_state_temporal_length,
                model_dim=256, output_dim=robot_state_output_dim, # ìˆ˜ì •: robot_state_output_dim ì‚¬ìš©
                num_layers=3, num_heads=8, dropout=0.1
            ).to(dtype=torch.bfloat16, device="cuda")
        else:
            self.robot_state_encoder = None

        combined_sensor_dim = 0
        if sensor_enabled:
            combined_sensor_dim += sensor_output_dim
        if self.robot_state_enabled:
            combined_sensor_dim += robot_state_output_dim # ìˆ˜ì •: robot_state_output_dim ë”í•˜ê¸°

        # --- íŠ¹ì§• í”„ë¡œì ì…˜ ë ˆì´ì–´ (ì°¨ì› í†µì¼) ---
        self.vl_proj = nn.Linear(vl_hidden_size, hidden_dim)
        if self.sensor_enabled:
            self.sensor_proj = nn.Linear(sensor_output_dim, hidden_dim)
        if self.robot_state_enabled:
            self.robot_state_proj = nn.Linear(robot_state_output_dim, hidden_dim)

        # ìƒˆë¡œ ì¶”ê°€ëœ í”„ë¡œì ì…˜ ë ˆì´ì–´ë“¤ì„ ëª¨ë¸ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ ë° dtypeìœ¼ë¡œ ì´ë™
        if not cache_only_mode:
            device = next(self.vl_model.parameters()).device
            dtype = next(self.vl_model.parameters()).dtype
        else:
            device = torch.device("cuda")
            dtype = torch.bfloat16

        self.vl_proj.to(device=device, dtype=dtype)
        if hasattr(self, 'sensor_proj'):
            self.sensor_proj.to(device=device, dtype=dtype)
        if hasattr(self, 'robot_state_proj'):
            self.robot_state_proj.to(device=device, dtype=dtype)

        if model_type == 'diffusion':
            raise ValueError("Diffusion ëª¨ë¸ì€ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'flow_matching' ë˜ëŠ” 'regression'ì„ ì‚¬ìš©í•´ì£¼ì‹­ì‹œì˜¤.")
        elif model_type == 'flow_matching':
            self.action_expert = FlowMatchingActionExpert(
                image_feature_dim=hidden_dim, # ì´ì œ í”„ë¡œì ì…˜ëœ ì°¨ì›ì„ ì‚¬ìš©
                text_guidance_dim=vl_hidden_size,
                sensor_dim=0, # action_expert ë‚´ë¶€ì—ì„œëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                action_dim=action_dim,
                horizon=horizon,
                hidden_dim=hidden_dim,
            ).to(dtype=torch.bfloat16, device="cuda")
        else:  # regression
            self.action_expert = RegressionActionExpert(
                image_feature_dim=hidden_dim, # ì´ì œ í”„ë¡œì ì…˜ëœ ì°¨ì›ì„ ì‚¬ìš©
                text_guidance_dim=vl_hidden_size,
                sensor_dim=0, # action_expert ë‚´ë¶€ì—ì„œëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                action_dim=action_dim,
                horizon=horizon,
                hidden_dim=hidden_dim,
            ).to(dtype=torch.bfloat16, device="cuda")

        print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")

    def _load_qwen_with_fallback(self, vl_model_name: str, device_map: Optional[str]) -> Qwen2_5_VLForConditionalGeneration:
        """
        Qwen-VL ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. FlashAttention 2 ë˜ëŠ” SDPA ì–´í…ì…˜ êµ¬í˜„ì— ëŒ€í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ë©ë‹ˆë‹¤.
        GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¼ bfloat16 ë˜ëŠ” float16ì„ ì‹œë„í•©ë‹ˆë‹¤.
        """
        dtype_candidates = [torch.bfloat16, torch.float16]
        attn_candidates = ["flash_attention_2", "sdpa"]

        # FlashAttention 2 -> SDPA -> Default attention ìˆœì„œë¡œ ì‹œë„
        for impl in attn_candidates:
            for dtype in dtype_candidates:
                try:
                    print(f"ğŸ§  {impl} ì–´í…ì…˜ê³¼ {dtype} ë°ì´í„° íƒ€ì…ìœ¼ë¡œ Qwen-VL ë¡œë“œ ì‹œë„ ì¤‘...")
                    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                        vl_model_name,
                        torch_dtype=dtype,
                        attn_implementation=impl,
                        device_map=device_map or "cuda",
                        low_cpu_mem_usage=True, # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPUë¡œ ì˜¤í”„ë¡œë“œ ì‹œë„
                    )
                    print(f"âœ… Qwen-VL ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {impl} ì–´í…ì…˜ ({dtype})")
                    self.attn_backend = impl
                    self.model_dtype = dtype
                    return model
                except Exception as e:
                    print(f"âš ï¸ {impl} ì–´í…ì…˜ ({dtype}) ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ëª¨ë“  íŠ¹ì • ì–´í…ì…˜ êµ¬í˜„ ì‹œë„ ì‹¤íŒ¨ ì‹œ, ê¸°ë³¸ ì–´í…ì…˜ìœ¼ë¡œ ì¬ì‹œë„
        for dtype in dtype_candidates:
            try:
                print(f"ğŸ§  ê¸°ë³¸ ì–´í…ì…˜ê³¼ {dtype} ë°ì´í„° íƒ€ì…ìœ¼ë¡œ Qwen-VL ë¡œë“œ ì‹œë„ ì¤‘...")
                model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                    vl_model_name,
                    torch_dtype=dtype,
                    device_map=device_map or "cuda",
                    low_cpu_mem_usage=True,
                )
                print(f"âœ… Qwen-VL ëª¨ë¸ ë¡œë“œ ì„±ê³µ: ê¸°ë³¸ ì–´í…ì…˜ ({dtype})")
                self.attn_backend = "default"
                self.model_dtype = dtype
                return model
            except Exception as e:
                print(f"âš ï¸ ê¸°ë³¸ ì–´í…ì…˜ ({dtype}) ë¡œë“œ ì‹¤íŒ¨: {e}")

        raise RuntimeError("âŒ ëª¨ë“  Qwen-VL ëª¨ë¸ ë¡œë“œ ì‹œë„ ì‹¤íŒ¨. í˜¸í™˜ë˜ëŠ” ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")

    def set_cache_enabled(self, enabled: bool = True):
        """ë‚´ë¶€ VL íŠ¹ì§• ìºì‹± í™œì„±í™” ì—¬ë¶€ë¥¼ `vl_encoder`ì— ìœ„ì„í•©ë‹ˆë‹¤."""
        if hasattr(self, 'vl_encoder'):
            self.vl_encoder.set_cache_enabled(enabled)

    def set_strict_cache(self, enabled: bool = True):
        """ë‚´ë¶€ VL íŠ¹ì§• ìºì‹± ì‹œ ì—„ê²© ëª¨ë“œë¥¼ `vl_encoder`ì— ìœ„ì„í•©ë‹ˆë‹¤."""
        if hasattr(self, 'vl_encoder'):
            self.vl_encoder.set_strict_cache(enabled)

    def set_cache_limit_gb(self, limit_gb: float):
        """ë‚´ë¶€ VL íŠ¹ì§• ìºì‹œì˜ ìµœëŒ€ í¬ê¸°ë¥¼ `vl_encoder`ì— ìœ„ì„í•©ë‹ˆë‹¤."""
        if hasattr(self, 'vl_encoder'):
            self.vl_encoder.set_cache_limit_gb(limit_gb)

    def _prepare_dataloader_cached_vl_tokens(self, cached_batch: Optional[List[Optional[Tuple[torch.Tensor, torch.Tensor]]]], device: torch.device) -> Tuple[Optional[Tuple[Union[torch.Tensor, List], Union[torch.Tensor, List]]], Optional[List[int]]]:
        """
        ë°ì´í„°ë¡œë”ì—ì„œ ì œê³µëœ ìºì‹œëœ VL íŠœí”Œ(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸)ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
        ë¶€ë¶„ì ì¸ ìºì‹œ ì»¤ë²„ë¦¬ì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        if not cached_batch:
            return None, None

        target_dtype = getattr(self, "model_dtype", torch.bfloat16)
        prepared_img_tokens: List[Optional[torch.Tensor]] = []
        prepared_txt_tokens: List[Optional[torch.Tensor]] = []
        missing_indices: List[int] = []
        has_any_valid_tensor = False

        for idx, item in enumerate(cached_batch):
            if isinstance(item, (list, tuple)) and len(item) == 2 and all(isinstance(t, torch.Tensor) and t.numel() > 0 for t in item):
                img_t, txt_t = item
                prepared_img_tokens.append(img_t.to(device=device, dtype=target_dtype, non_blocking=True))
                prepared_txt_tokens.append(txt_t.to(device=device, dtype=target_dtype, non_blocking=True))
                has_any_valid_tensor = True
            else:
                prepared_img_tokens.append(None)
                prepared_txt_tokens.append(None)
                missing_indices.append(idx)

        if not has_any_valid_tensor:
            return None, None

        if not missing_indices:
            # ëª¨ë“  ìƒ˜í”Œì´ ìºì‹œë¨
            return (torch.cat(prepared_img_tokens, dim=0), torch.cat(prepared_txt_tokens, dim=0)), None

        # ì¼ë¶€ ìƒ˜í”Œë§Œ ìºì‹œë¨
        return (prepared_img_tokens, prepared_txt_tokens), missing_indices

    def _encode_missing_vl_features_and_backfill(self,
                                                text_inputs: List[str],
                                                image_inputs: List[List[str]],
                                                cache_keys: List[str],
                                                indices_to_encode: List[int],
                                                device: torch.device,
                                                vl_cache_metadata: Optional[dict] = None) -> dict:
        """
        ëˆ„ë½ëœ VL íŠ¹ì§•(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ íŠœí”Œ)ë§Œì„ ì¸ì½”ë”©í•˜ê³  ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì±„ì›ë‹ˆë‹¤.
        """
        if not indices_to_encode:
            return {}

        subset_texts = [text_inputs[i] for i in indices_to_encode]
        subset_images = [image_inputs[i] for i in indices_to_encode]
        subset_keys = [cache_keys[i] for i in indices_to_encode]

        # VL íŠ¹ì§• ì¸ì½”ë”© (V2: íŠœí”Œ ë°˜í™˜)
        image_features, guidance_vectors = self.vl_encoder.encode(
            subset_texts, subset_images, subset_keys, use_cache=False
        )

        # ê²°ê³¼ë¥¼ ì¸ë±ìŠ¤ë³„ íŠœí”Œë¡œ ë¶„í• 
        img_splits = torch.split(image_features, 1, dim=0)
        txt_splits = torch.split(guidance_vectors, 1, dim=0)
        tokens_by_index = {
            idx: (img, txt) for idx, img, txt in zip(indices_to_encode, img_splits, txt_splits)
        }

        if self.auto_cache_backfill and self.external_cache_mgr and vl_cache_metadata:
            dataset_names = vl_cache_metadata.get("dataset_names")
            vlm_indices = vl_cache_metadata.get("vlm_indices")
            prompt_hashes = vl_cache_metadata.get("prompt_hashes")

            if dataset_names and vlm_indices and prompt_hashes:
                for idx, (img_tensor, txt_tensor) in tokens_by_index.items():
                    if img_tensor is None or txt_tensor is None: continue
                    try:
                        if idx >= len(dataset_names) or idx >= len(vlm_indices) or idx >= len(prompt_hashes): continue
                        dataset_name, vlm_idx, prompt_hash = dataset_names[idx], int(vlm_indices[idx]), prompt_hashes[idx]
                    except (TypeError, ValueError, IndexError):
                        continue
                    if dataset_name is None or prompt_hash is None: continue
                    
                    try:
                        if not hasattr(self, "_cache_backfill_notice"):
                            print("ğŸ§· í›ˆë ¨ ì¤‘ ëˆ„ë½ëœ VL ìºì‹œ í•­ëª©ì„ ìë™ìœ¼ë¡œ ë¹Œë“œí•©ë‹ˆë‹¤ (V2 í˜•ì‹).")
                            self._cache_backfill_notice = True
                        # V2: íŠœí”Œì„ ì €ì¥
                        self.external_cache_mgr.save_cache(
                            dataset_name=dataset_name, vlm_idx=vlm_idx, prompt_hash=prompt_hash,
                            vl_features=(img_tensor.detach(), txt_tensor.detach()),
                        )
                    except Exception as e:
                        if not hasattr(self, "_cache_backfill_warned"):
                            print(f"âš ï¸ VL ìºì‹œ í•­ëª© ë°±í•„ ì‹¤íŒ¨ ({dataset_name}_vlm{vlm_idx}): {e}")
                            self.backfill_warned = True
        return tokens_by_index

    def encode_vision(self,
                      text_inputs: List[str],
                      image_inputs: List[List[str]],
                      cache_keys: List[str],
                      use_cache: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì…ë ¥ìœ¼ë¡œë¶€í„° VL íŠ¹ì§•(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ íŠœí”Œ)ì„ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
        ì£¼ë¡œ ìºì‹œ ë¹Œë”© ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        """
        self.eval()
        return self.vl_encoder.encode(
            text_inputs, image_inputs, cache_keys, use_cache=use_cache
        )

    def forward(self,
                text_inputs: List[str],
                image_inputs: List[List[str]],
                actions: Optional[torch.Tensor] = None,
                z_chunk: Optional[torch.Tensor] = None,
                sensor_data: Optional[torch.Tensor] = None,
                robot_states: Optional[torch.Tensor] = None,
                cache_keys: Optional[List[str]] = None,
                cache: bool = True,
                vl_cache_tokens: Optional[List[Optional[Tuple[torch.Tensor, torch.Tensor]]]] = None,
                vl_cache_metadata: Optional[dict] = None) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        V2 ì•„í‚¤í…ì²˜ë¥¼ ìœ„í•œ í†µí•© í¬ì›Œë“œ íŒ¨ìŠ¤.
        ì´ë¯¸ì§€ íŠ¹ì§•ê³¼ í…ìŠ¤íŠ¸ ê°€ì´ë˜ìŠ¤ë¥¼ ë¶„ë¦¬í•˜ì—¬ í–‰ë™ ì „ë¬¸ê°€ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
        """
        device = next(self.parameters()).device
        image_features, guidance_vectors = None, None

        # 1. VL íŠ¹ì§• ì¸ì½”ë”© ë° ìºì‹± ì²˜ë¦¬ (V2)
        prepared_cached_tokens, missing_indices = self._prepare_dataloader_cached_vl_tokens(vl_cache_tokens, device)

        if prepared_cached_tokens and not missing_indices:
            image_features, guidance_vectors = prepared_cached_tokens
            if not hasattr(self, "_external_cache_confirmed"):
                print("ğŸ’¾ ë°ì´í„°ë¡œë”ì—ì„œ ì œê³µëœ VL ìºì‹œ í…ì„œ(V2) ì‚¬ìš© ì¤‘.")
                self._external_cache_confirmed = True
        elif prepared_cached_tokens and missing_indices:
            if self.cache_only_mode:
                raise RuntimeError("âš ï¸ cache_only_modeì—ì„œëŠ” ëª¨ë“  VL ìºì‹œê°€ ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ëˆ„ë½ëœ ìºì‹œê°€ ìˆìŠµë‹ˆë‹¤.")

            prepared_img_tokens, prepared_txt_tokens = prepared_cached_tokens
            new_tokens_by_idx = self._encode_missing_vl_features_and_backfill(
                text_inputs, image_inputs, cache_keys, missing_indices, device, vl_cache_metadata
            )
            for idx in missing_indices:
                if idx in new_tokens_by_idx:
                    prepared_img_tokens[idx], prepared_txt_tokens[idx] = new_tokens_by_idx[idx]

            if all(t is not None for t in prepared_img_tokens) and all(t is not None for t in prepared_txt_tokens):
                image_features = torch.cat(prepared_img_tokens, dim=0)
                guidance_vectors = torch.cat(prepared_txt_tokens, dim=0)
            else:
                raise RuntimeError("âš ï¸ ë°ì´í„°ë¡œë” ìºì‹œì™€ ì‹ ê·œ ì¸ì½”ë”© í›„ì—ë„ VL í† í°ì´ ì™„ì „íˆ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            if self.cache_only_mode:
                raise RuntimeError("âš ï¸ cache_only_modeì—ì„œëŠ” VL ìºì‹œê°€ í•„ìˆ˜ì…ë‹ˆë‹¤. vl_cache_tokensë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.")

            image_features, guidance_vectors = self.vl_encoder.encode(
                text_inputs, image_inputs, cache_keys, use_cache=cache
            )

        # 2. ì„¼ì„œ ë° ë¡œë´‡ ìƒíƒœ íŠ¹ì§• ì¸ì½”ë”©
        sensor_features_encoded: Optional[torch.Tensor] = None
        if self.sensor_enabled and sensor_data is not None:
            sensor_features_encoded = self.sensor_encoder(sensor_data.to(device=device, dtype=torch.bfloat16))

        robot_state_features_encoded: Optional[torch.Tensor] = None
        if self.robot_state_enabled and robot_states is not None:
            robot_state_features_encoded = self.robot_state_encoder(robot_states.to(device=device, dtype=torch.bfloat16))

        # 3. ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ íŠ¹ì§•ì„ í”„ë¡œì ì…˜í•˜ê³  ë‹¨ì¼ í…ì„œë¡œ ê²°í•©
        context_tensors = [self.vl_proj(image_features)]
        if sensor_features_encoded is not None:
            context_tensors.append(self.sensor_proj(sensor_features_encoded).unsqueeze(1))
        if robot_state_features_encoded is not None:
            context_tensors.append(self.robot_state_proj(robot_state_features_encoded).unsqueeze(1))
        
        context_features = torch.cat(context_tensors, dim=1)

        # 4. ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ í¬ì›Œë“œ íŒ¨ìŠ¤ (V2, Cross-Attention)
        if self.model_type == 'flow_matching':
            if actions is not None and self.training:
                actions = actions.to(device=device, dtype=image_features.dtype)
                with torch.autocast(device.type, dtype=torch.bfloat16):
                    loss = self.action_expert.compute_loss(
                        actions, context_features, guidance_vectors
                    )
                return loss, None, None
            else:
                sampled_actions = self.action_expert.sample(
                    context_features, guidance_vectors,
                    num_steps=self.flow_steps, method=self.flow_solver
                )
                return sampled_actions, None, None
        elif self.model_type == 'regression':
            if z_chunk is None:
                raise ValueError("Regression ëª¨ë¸ì€ z_chunk ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            z_chunk = z_chunk.to(device=device, dtype=image_features.dtype)
            with torch.autocast(device.type, dtype=torch.bfloat16):
                pred_actions, delta = self.action_expert(
                    z_chunk, context_features, guidance_vectors
                )
            return pred_actions, delta
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")

    @torch.no_grad()
    def predict_action(self,
                       text_inputs: List[str],
                       image_inputs: List[List[str]],
                       sensor_data: Optional[torch.Tensor] = None,
                       robot_states: Optional[torch.Tensor] = None,
                       cache_keys: Optional[List[str]] = None,
                       **kwargs) -> torch.Tensor:
        """
        V2 ì•„í‚¤í…ì²˜ë¥¼ ìœ„í•œ ì¶”ë¡  ì „ìš© ë˜í¼ í•¨ìˆ˜.
        """
        self.eval()
        
        if self.model_type == 'flow_matching':
            sampled_actions, _, _ = self.forward(
                text_inputs=text_inputs, image_inputs=image_inputs,
                actions=None, sensor_data=sensor_data, robot_states=robot_states,
                cache_keys=cache_keys, **kwargs
            )
            return sampled_actions
        else:
            raise NotImplementedError(
                "Regression ëª¨ë¸ì˜ predict_actionì€ z_chunkê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                "`forward()` ë©”ì„œë“œë¥¼ `z_chunk` ë§¤ê°œë³€ìˆ˜ì™€ í•¨ê»˜ ì§ì ‘ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
            )

if __name__ == "__main__":
    print("ğŸ§ª Unified VLA ëª¨ë¸ V2 í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜: {device}")

    # Flow Matching ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n=== Flow Matching ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    try:
        model_flow_matching = QwenVLAUnified(
            model_type='flow_matching', sensor_enabled=True, robot_state_enabled=True,
            finetune_vl='none', cache_dir="./test_cache",
        ).to(device)
        model_flow_matching.eval()

        batch_size = 2
        horizon = model_flow_matching.horizon
        action_dim = model_flow_matching.action_dim
        
        text_inputs_dummy = ["ë¡œë´‡ íŒ”ì„ ì›€ì§ì—¬ ì»µì„ ì¡ìœ¼ì‹œì˜¤.", "ë¹¨ê°„ ë¸”ë¡ì„ ì™¼ìª½ìœ¼ë¡œ ì˜®ê¸°ì‹œì˜¤."]
        import glob
        sample_images = glob.glob("/home/najo/NAS/VLA/dataset/New_dataset2/**/View4/*.jpg", recursive=True)[:4]
        if len(sample_images) >= 2:
            image_inputs_dummy = [[sample_images[0], sample_images[1]], [sample_images[2], sample_images[3]]]
            print(f"  í…ŒìŠ¤íŠ¸ìš© ì‹¤ì œ ì´ë¯¸ì§€ ì‚¬ìš©: {len(image_inputs_dummy)}ê°œ ìƒ˜í”Œ, ê° 2ê°œ ë·°")
        else:
            print("  âš ï¸ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
            image_inputs_dummy = [[], []]

        sensor_data_dummy = torch.randn(batch_size, 650, 1026, device=device, dtype=torch.float32)
        robot_states_dummy = torch.randn(batch_size, 100, 12, device=device, dtype=torch.float32)
        
        with torch.no_grad():
            sampled_actions_flow = model_flow_matching.predict_action(
                text_inputs=text_inputs_dummy, image_inputs=image_inputs_dummy,
                sensor_data=sensor_data_dummy, robot_states=robot_states_dummy
            )
        print(f"âœ… Flow Matching ëª¨ë¸ ì¶”ë¡  ì„±ê³µ. ì¶œë ¥ í˜•íƒœ: {sampled_actions_flow.shape}")
        assert sampled_actions_flow.shape == (batch_size, horizon, action_dim)
    except Exception as e:
        print(f"âŒ Flow Matching ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

    # Regression ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n=== Regression ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    try:
        model_regression = QwenVLAUnified(
            model_type='regression', sensor_enabled=True, robot_state_enabled=True,
            finetune_vl='none', cache_dir="./test_cache",
        ).to(device)
        model_regression.eval()

        z_chunk_dummy = torch.randn(batch_size, horizon, action_dim, device=device, dtype=torch.float32)

        with torch.no_grad():
            pred_actions_reg, delta_reg = model_regression.forward(
                text_inputs=text_inputs_dummy, image_inputs=image_inputs_dummy,
                z_chunk=z_chunk_dummy, sensor_data=sensor_data_dummy, robot_states=robot_states_dummy,
            )
        print(f"âœ… Regression ëª¨ë¸ ì¶”ë¡  ì„±ê³µ. ì˜ˆì¸¡ í–‰ë™ í˜•íƒœ: {pred_actions_reg.shape}, ë¸íƒ€ í˜•íƒœ: {delta_reg.shape}")
        assert pred_actions_reg.shape == (batch_size, horizon, action_dim)
        assert delta_reg.shape == (batch_size, horizon, action_dim)
    except Exception as e:
        print(f"âŒ Regression ëª¨ë¸ ì¶”ë¡  ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    test_cache_dir = Path("./test_cache")
    if test_cache_dir.exists():
        import shutil
        print(f"í…ŒìŠ¤íŠ¸ ìºì‹œ ë””ë ‰í† ë¦¬ {test_cache_dir} ì •ë¦¬ ì¤‘...")
        shutil.rmtree(test_cache_dir)
        print("ì •ë¦¬ ì™„ë£Œ.")